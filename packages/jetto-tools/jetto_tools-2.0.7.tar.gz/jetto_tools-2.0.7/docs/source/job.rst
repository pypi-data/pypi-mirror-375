.. _job-label:

==========
Job module
==========

Introduction
============

The ``job`` module provides a means of automatically submitting a JETTO job to be run. Jobs can be submitted to:

* Your HPC cluster's batch system
* Cloud HPC systems (via PROMINENCE)

To the extent possible, the ``job`` module attempts to mimic the corresponding functionality provided by the JAMS
application.

Creating a configuration
========================

To submit a job, it is first necessary to create a run configuration. This is done by first loading a JETTO template,
using the :ref:`template <template-label>` module, followed by creating and applying any desired modifications to the
run configuration, using the :ref:`config <config-label>` module.

As an starting point for the subsequent job management examples, the code snippet below demonstrates a simple case
where we load a JETTO template from the catalogue, followed by creating a run configuration.

.. highlight:: python

>>> import jetto_tools.template
>>> template = jetto_tools.template.from_catalogue('fcasson', 'jet', 92398, 'dec1318', 1, './lookup.json', retrieve_dir='./jetto_retrieve')
>>> import jetto_tools.config
>>> config = jetto_tools.config.RunConfig(template)

See the documentation for the :ref:`template <template-label>` and :ref:`config <config-label>` modules for more
information and examples of how templates can be created and configurations defined and modified.

Creating a job manager
======================

All jobs are submitted via the interface provided by the ``JobManager`` class. Creation of a job manager is
straightforward:

.. highlight:: python

>>> import jetto_tools.job
>>> manager = jetto_tools.job.JobManager()

Job submission
==============

Batch system
------------

Prerequisites
~~~~~~~~~~~~~

The batch job submission functionality can only be easily used from within an existing JINTRAC environment (e.g. via
the loaded JINTRAC environment module on your cluster), as it depends on various environment variables and scripts used
for configuring a JETTO run, which are only present there.

Submission
~~~~~~~~~~

For the local batch system, the user has two options: create the job files without running a job, or create the job files
**and** run the job.

To just create all of the necessary job files on disk, without submitting a job to be run, call the
``submit_job_to_batch`` function of the ``JobManager`` class, as follows. This function takes the run configuration
and run directory name as arguments e.g.:

.. highlight:: python

>>> jobs = manager.submit_job_to_batch(config, 'my_run')

The above call is equivalent to the ``Set Up Job But Don't Run`` option provided by JAMS in the ``Job Process`` panel.

The manager will create the job files in the directory formed by concatenating ``rundir`` with the JETTO run root,
which is assumed to be ``$RUNS_HOME/jetto/runs``. So in this case, all of the configuration files and batch scripts will
be created in ``$RUNS_HOME/jetto/runs/my_run``. Note that ``submit_job_to_batch`` automatically calls the ``export``
function of the ``config`` object: there is no need for the user to do this explicitly.

If the configuration contains scans over one or more parameters, then the job submission will create a separate directory
for each point in the configuration, in line with the description given of the ``export`` method in the
:ref:`config <config-label>` module documentation e.g. ``$RUNS_HOME/jetto/runs/my_run/point_000``,
``$RUNS_HOME/jetto/runs/my_run/point_001`` etc.

In addition to the JETTO configuration files generated by the ``config`` module, the job manager creates a number of
additional files in each run directory. These are the batchfile (``.llcmd``) and JETTO run scripts ``rjettov`` and
``utils``). The JETTO run scripts are copied into the run directory from the source distribution of JETTO corresponding
to the configured load module version. They are copied unmodified. If the source JETTO distribution does not contain a
`utils` script, the copy of this file is skipped, for backwards compatibility.

To run a job in addition to creating the files, modify the call to ``submit_job_to_batch`` to include the ``run`` flag:

.. highlight:: python

>>> jobs = manager.submit_job_to_batch(config, 'my_run', run=True)

The above call will cause the manager to create all of the necessary JETTO files in the indicated directory, **and**
will submit a job to the batch system. It is equivalent to the ``Run Job Now`` option provided by JAMS.

Example
~~~~~~~

As a complete example of generating and submitting a job, the example below illustrates loading the catalogue case
``88888/fkochl/jetto/step/dec1520/seq.4``, setting the JETTO version to run, and configuring a parameter scan over the
'IPRAUX' parameter in the ``INESCO`` namelist. The jobs are then submitted to the batch system.

.. code-block:: python

    import jetto_tools.template
    import jetto_tools.config
    import jetto_tools.job

    template = jetto_tools.template.from_catalogue('fkochl', 'step', 88888, 'dec1520', 4, lookup='lookup.json')

    config = jetto_tools.config.RunConfig(template)
    config.binary = 'v111120'
    config.userid = 'sim'
    config['ipraux'] = jetto_tools.config.Scan(range(10))

    manager = jetto_tools.job.JobManager()
    jobs = manager.submit_job_to_batch(config, 'testdata', run=True)

Running ``llq`` (on Heimdall) then shows the submitted jobs being queued:

::

    5320146                  user       27/01 11:04  R  0  std          heimdall037
    5320147                  user       27/01 11:04  R  0  std          heimdall006
    5320148                  user       27/01 11:04  R  0  std          heimdall041
    5320149                  user       27/01 11:04  R  0  std          heimdall037
    5320150                  user       27/01 11:04  R  0  std          heimdall006
    5320151                  user       27/01 11:04  R  0  std          heimdall024
    5320152                  user       27/01 11:04  R  0  std          heimdall041
    5320153                  user       27/01 11:04  R  0  std          heimdall016
    5320154                  user       27/01 11:04  R  0  std          heimdall037
    5320155                  user       27/01 11:04  R  0  std          heimdall006

Once the jobs complete, the results can be found in the run directory for each point.

PROMINENCE
----------

Prerequisites
~~~~~~~~~~~~~

In order to use the ``job`` module's PROMINENCE functionality, the environment needs to be configured for PROMINENCE
usage. The PROMINENCE `Quick Start <https://prominence-eosc.github.io/docs/quick-start#>`_ describes how to do this.
To summarise:

1. The PROMINENCE Python package must be installed
2. The PROMINENCE_URL and PROMINENCE_OIDC_URL environment variables must be set
3. The client device must be registered

The above requirements are met by default if working from the UKAEA clusters (Freia or JDC).

Once these requirements are met, the user must also authenticate to PROMINENCE via the CLI by running
``prominence login``. The authentication is valid for a period of time (varies by CLIENT_ID).

More general information about the use of JINTRAC and JETTO with cloud resources can be found on the JINTRAC
`cloud computing <https://users.euro-fusion.org/pages/data-cmg/wiki/JINTRAC_cloud.html>`_ page.

Submission
~~~~~~~~~~

To submit a job to PROMINENCE, call the ``submit_job_to_prominence`` function of the ``JobManager`` class, as follows.
This function takes the run configuration and run directory name as arguments e.g.:

.. highlight:: python

>>> id = manager.submit_job_to_prominence(config, 'my_run')

The manager will create the job files in the directory formed by concatenating ``rundir`` with the JETTO run root,
which is assumed to be ``$RUNS_HOME/jetto/runs``. So in this case, all of the configuration files will
be created in ``$RUNS_HOME/jetto/runs/my_run``. Note that ``submit_job_to_batch`` automatically calls the ``export``
function of the ``config`` object: there is no need for the user to do this explicitly. The manager will also create a
tarball from the run directory's contents and upload it to PROMINENCE as part of submitting the job.

**Note:** To reduce the size of the uploaded tarball, the JSET files (``jetto.jset``) are omitted by default. They are
retained in the local copy of the run directories, however.

If the configuration contains parameter scans, then the run will be submitted to PROMINENCE as a *workflow*, where each
individual job in the workflow is generated via  PROMINENCE *job factory*. The job factory iterates over the work
directories corresponding to each point in the scan. The workflow will thne consist of as many jobs as there are points
in the scan. If the configuration does not contain any parameter scans, then the run be be submitted to PROMINENCE as a
single job, in the same manner as currently used by JAMS.

The returned ``id`` is the job or workflow id. This can be used with the prominence CLI to get information
about the status of the run, and to retrieve the results when the run is completed. See the
`Checking job status <https://prominence-eosc.github.io/docs/job-status>`_ page in the PROMINENCE documentation for
more information. In addition to the CLI, there is also a `Python API <https://prominence-eosc.github.io/docs/python>`_
which can be used to interact with the submitted run, but note that (at the time of writing) its documentation is more
limited than that of the CLI.

Example
~~~~~~~

As a complete example of generating and submitting a job, the example below illustrates loading the catalogue case
``88888/fkochl/jetto/step/dec1520/seq.4``, setting the JETTO version to run, and configuring a parameter scan over the
'IPRAUX' parameter in the ``INESCO`` namelist. The jobs are then submitted to PROMINENCE.

.. code-block:: python

    import jetto_tools.template
    import jetto_tools.config
    import jetto_tools.job

    template = jetto_tools.template.from_catalogue('fkochl', 'step', 88888, 'dec1520', 4, lookup='lookup.json')

    config = jetto_tools.config.RunConfig(template)
    config.binary = 'v111120'
    config.userid = 'sim'
    config.walltime = 1
    config['ipraux'] = jetto_tools.config.Scan(range(10))

    manager = jetto_tools.job.JobManager()
    id = manager.submit_job_to_prominence(config, 'ipraux_scan')

Running ``prominence list`` (on Heimdall) then shows the submitted jobs being deployed:

::

    ID      NAME                        CREATED               STATUS      ELAPSED      IMAGE                          CMD
    44318   ipraux_scan/ipraux_scan/0   2021-01-27 11:57:28   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44319   ipraux_scan/ipraux_scan/1   2021-01-27 11:57:28   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44320   ipraux_scan/ipraux_scan/2   2021-01-27 11:57:30   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44321   ipraux_scan/ipraux_scan/3   2021-01-27 11:57:30   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44322   ipraux_scan/ipraux_scan/4   2021-01-27 11:57:32   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44323   ipraux_scan/ipraux_scan/5   2021-01-27 11:57:32   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44324   ipraux_scan/ipraux_scan/6   2021-01-27 11:57:34   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44325   ipraux_scan/ipraux_scan/7   2021-01-27 11:57:35   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44326   ipraux_scan/ipraux_scan/8   2021-01-27 11:57:37   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir
    44327   ipraux_scan/ipraux_scan/9   2021-01-27 11:57:37   idle                     CCFE/JINTRAC/sim:v111120.tgz   rjettov -x64 $workdir

There are a number of important differences between running a scan on PROMINENCE versus running locally on the batch
system.

* Many template cases do not have the walltime configured in the template JSET. This must be added prior to
  submission, via the ``walltime`` property of the ``RunConfig`` class.
* The id returned by ``submit_job_to_prominence`` can be either a job id (if the configuration does not contain any
  scans), or a workflow id (if there are scans). The workflow id usually immediately precedes the collection of job ids
  associated with the workflow. In the example above, the workflow id returned was 44317, which then spawned jobs
  44318-44327. The association between jobs and workflows can also be seen in the ``NAME`` column, where the name is
  given by ``<run name>/<run name>/<n>`` where n is an increasing integer. <n> will be the same integer as the corresponding
  run directory e.g. ``point_002`` will correspond to ``ipraux_scan/ipraux_scan/2``. The workflow/job id is also recorded
  in the file ``remote.jobid`` in the top-level run directory.

When a job completes on PROMINENCE, the contents of the run directory can be downloaded via the ``prominence download <job id>``
command. For configurations containing only a single job, the download will contain the run directory for that job. For
configurations containing multiple jobs, the download will contain *all* of the run directories for the workflow, irrespective
of the job id that is used to perform the download. This is because (at the time of writing), only a single output
directory can be specified in an uploaded workflow description, even if the workflow generates multiple jobs. This may
be addressed in a future version.

Job status
==========

Batch system
------------

It's possible to use the ``Job`` objects returned by the ``submit_job_to_batch`` to monitor the status of individual
JETTO jobs. As noted above, when calling ``submit_job_to_batch``, the function returns a list of one or more ``Job``
objects.

.. highlight:: python

>>> jobs = manager.submit_job_to_batch(config, 'my_run')
>>> myjob = jobs[0]

(If there is only a single point within the run configuration, the list returned by the call above will only contain
a single job).

Each ``Job`` object records the run directory of the job:

.. highlight:: python

>>> myjob.rundir
/home/user/jetto/runs/my_run/point_000

Each job object also records the LoadLeveler Id associated with the job's submission on the batch system (if the job
was not run as part of the submission, or if the job was not run on the batch system, the ``id`` will be ``None``):

.. highlight:: python

>>> myjob.id
5273791

The configuration of the job can be retrieved by accessing the ``serialisation`` property of the job. This contains the
same information stored in the ``serialisation.json`` created when the job was exported to the filesystem, and can be
used to query specific aspects of the job's configuration (such as the value of a particular parameter). If no serialisation
exists (e.g. because the job was launched by JAMS), then the ``serialisation`` property returns ``None``:

.. highlight:: python

>>> job.serialisation
    {
        "loadmodule": {
            "binary": "v060619",
            "userid": "sim"
        },
        "parameters": {
            "bound_ellip": 1.5,
            "bcintrhon": 0.7,
            "ipraux": 2,
            "rcntren": [10.1, 10.2]
        },
...

Finally, the job has a ``status`` method, which returns the currrent status of the job e.g.

.. highlight:: python

>>> myjob.status()
0

The possible statuses are:

 * 0: The job has completed successfully
 * 1: The job has completed with a failure
 * 2: The job status is unknown

For convenience, the possible statuses are expressed in terms of the ``Status`` class provided by the ``job`` module:

.. highlight:: python

>>> from jetto_tools.job import Status
>>> myjob.status() == Status.SUCCESSFUL
True

The other statuses are ``Status.Failed`` and ``Status.UNKNOWN``. The ``Status`` class also provides a convenience
method which converts the job status into a printable string:

.. highlight:: python

>>> Status.to_string(myjob.status())
'Successful'

Determination of the status of the job is done based on the contents of the job's run directory:

1. If the ``jetto.out`` file exists, the job's status is either ``Status.SUCCESSFUL``, or ``Status.FAILED``, depending
   on the contents of the file
2. If ``jetto.out`` does not exist, the job's status is ``Status.UNKNOWN``

An example of a script which launches and continually monitors the status of a single job submitted to the batch system
is provided below:

.. code-block:: python

    import jetto_tools.template
    import jetto_tools.config
    import jetto_tools.job
    from jetto_tools.job import Status
    import time

    template = jetto_tools.template.from_directory('/home/user/jetto/runs/001')

    config = jetto_tools.config.RunConfig(template)

    manager = jetto_tools.job.JobManager()
    myjob = manager.submit_job_to_batch(config, 'test', run=True)[0]

    status = Status.UNKNOWN
    while status not in (Status.SUCCESSFUL, Status.FAILED):
        status = myjob.status()

        print(f'Job {myjob.id} (run directory {myjob.rundir}) is {Status.to_string(status)}...')

        time.sleep(1)

The above script will produce output along the lines of:

::

    Job 7568351 (run directory /home/user/jetto/runs/test) is Unknown...
    Job 7568351 (run directory /home/user/jetto/runs/test) is Unknown...
    Job 7568351 (run directory /home/user/jetto/runs/test) is Unknown...
    Job 7568351 (run directory /home/user/jetto/runs/test) is Successful...

For convenience, each ``Job`` object has a printable representation which comprises similar information to what we
manually extracted above

.. highlight:: python

>>> print(myjob)
JETTO job (ID: 7568351, Run directory: /home/user/jetto/runs/test, Status: Successful)

For long running jobs, it may not be desirable to run a Python script continually from the point of job submission
in order to monitor the job's status. In that case, it is also possible to manually create a ``Job`` object by
providing the path to the run directory directly to the constructor e.g.

.. highlight:: python

>>> myjob = jetto_tools.job.Job('/home/user/jetto/runs/test')
>>> print(myjob)
JETTO job (ID: 7568351, Run directory: /home/user/jetto/runs/test, Status: Successful)

For scans, where many jobs are submitted at once, the equivalent can be achieved by using the function ``retrieve_jobs``,
which returns a list of all jobs in the scan
e.g.

.. highlight:: python

>>> myjobs = jetto_tools.job.retrieve_jobs('/home/user/jetto/runs/test')
>>> for j in myjobs:
    print(j)
JETTO job (ID: 7568351, Run directory: /home/user/jetto/runs/test/point_000, Status: Successful)
JETTO job (ID: 7568352, Run directory: /home/user/jetto/runs/test/point_001, Status: Successful)
JETTO job (ID: 7568353, Run directory: /home/user/jetto/runs/test/point_002, Status: Successful)
...

PROMINENCE
----------

At the present time, monitoring and checking of the status of PROMINENCE jobs running in the cloud is **not** supported
by this API.

Job results
===========

Batch system
------------


PROMINENCE
----------

.. _prominence-download-label:

Download
~~~~~~~~

To examine the results of a scan which was run on PROMINENCE, it is first necessary to download the results. The ``job``
module provides a function to automatically download all point directories in a scan.   It is also possible to download
all the results from a workflow using the prominence CLI which may be faster since it benefits from parallel
downloads.

.. highlight:: bash

>>> prominence download workflow #


The function in the job module takes the workflow identifier
for the scan (which is returned by ``submit_job_to_prominence``, and can also be found in the file ``remote.jobid`` in
the scan's run directory), and a directory in which to store the downloaded point directories:

.. highlight:: python

>>> jetto_tools.job.prominence_download_scan_results(workflow_id=60751, outdir='scan')

(Use of this function requires that the user has logged in to PROMINENCE via the usual ``prominence login``.)

Note that for large scans, the download can take a long time (> 1 hour for 500 points is typical). In order to monitor
the progress of the downloads, pass the ``verbose=True`` option to the function:

.. highlight:: python

>>> jetto_tools.job.prominence_download_scan_results(workflow_id=60751, outdir='scan', verbose=True)
Downloading point_000...
Downloading point_001...
Downloading point_002...
Downloading point_003...
Downloading point_004...
Downloading point_005...

Only points whose PROMINENCE status is ``'completed'`` will be downloaded; any still running or deleted will be skipped:

::

    ...
    Downloading point_364...
    Downloading point_365...
    Skipping point_365 - not completed
    ...

Since not all points may be completed at a given time, it can be useful to specify a particular set or range of points
to download. To do this, pass the optional argument ``points`` to the function:

.. highlight:: python

>>> jetto_tools.job.prominence_download_scan_results(workflow_id=60751, outdir='scan', verbose=True, points=range(361, 401))
Downloading point_361...
Downloading point_362...
Downloading point_363...
...

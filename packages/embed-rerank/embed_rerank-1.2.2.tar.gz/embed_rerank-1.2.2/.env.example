############################################################
# Example Environment (.env example)
############################################################

# Backend
BACKEND=auto
MODEL_NAME=mlx-community/Qwen3-Embedding-4B-4bit-DWQ
MODEL_PATH=
CROSS_ENCODER_MODEL=

# Model Cache & Storage
# MODEL_PATH: Custom path for MLX models (overrides auto cache detection)
# If empty, uses Hugging Face cache or environment variables below:
# TRANSFORMERS_CACHE: Override HF transformers cache location
# HF_HOME: Hugging Face cache home directory  
# Default cache location: ~/.cache/huggingface/hub/
# 
# Examples:
# MODEL_PATH=/path/to/local/models/Qwen3-Embedding-4B-4bit-DWQ
# TRANSFORMERS_CACHE=/custom/cache/transformers
# HF_HOME=/custom/huggingface

# Server
HOST=0.0.0.0
PORT=9000
RELOAD=false

# Performance
BATCH_SIZE=32
MAX_BATCH_SIZE=128
MAX_TEXTS_PER_REQUEST=100
MAX_PASSAGES_PER_RERANK=1000
MAX_SEQUENCE_LENGTH=512
DEVICE_MEMORY_FRACTION=0.8
REQUEST_TIMEOUT=300

# ðŸš€ Text Processing Configuration (NEW!)
# Default text processing options for the service
DEFAULT_AUTO_TRUNCATE=true
DEFAULT_TRUNCATION_STRATEGY=smart_truncate
# DEFAULT_MAX_TOKENS_OVERRIDE=2048
DEFAULT_RETURN_PROCESSING_INFO=false

# Text processing strategies:
# - smart_truncate: Preserve sentence boundaries while truncating (recommended)
# - truncate: Simple token-based truncation
# - extract: Extract key sentences only
# - error: Raise error when token limit is exceeded
#
# Token limits (automatically detected from model metadata):
# - Recommended max tokens: 2048 (auto-truncation trigger)
# - Absolute max tokens: 8192 (hard limit, will raise error)
# - Users can override recommended limit via max_tokens_override (up to absolute max)

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# Security (optional)
# ALLOWED_HOSTS=["example.com","api.example.com"]
# ALLOWED_ORIGINS=["https://example.com","https://app.example.com"]

# Copy to .env and adjust as needed.

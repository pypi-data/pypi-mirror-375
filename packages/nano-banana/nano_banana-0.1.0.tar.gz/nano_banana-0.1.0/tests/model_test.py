#!/usr/bin/env python3
"""
æµ‹è¯• gpt-4o-mini è§†è§‰åŠŸèƒ½
"""

import os
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

def test_gpt4o_mini_vision():
    """æµ‹è¯• gpt-4o-mini è§†è§‰åŠŸèƒ½"""
    print("ğŸ‘ï¸ æµ‹è¯• gpt-4o-mini è§†è§‰åŠŸèƒ½...")
    
    api_key = os.environ.get("SIMEN_AI_API_KEY")
    base_url = os.environ.get("SIMEN_BASEURL")
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # æµ‹è¯•å›¾ç‰‡URL
    test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"
    
    try:
        print("ğŸ” å‘é€å›¾ç‰‡åˆ†æè¯·æ±‚...")
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆåŠ¨ç‰©ï¼Ÿè¯·ç”¨ä¸­æ–‡ç®€çŸ­å›ç­”ã€‚"},
                        {"type": "image_url", "image_url": {"url": test_image_url}},
                    ],
                }
            ],
            max_tokens=200,
        )
        
        result = response.choices[0].message.content
        print(f"âœ… gpt-4o-mini è§†è§‰åˆ†ææˆåŠŸ:")
        print(f"ğŸ“ åˆ†æç»“æœ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ gpt-4o-mini è§†è§‰åˆ†æå¤±è´¥:")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
        return False

def test_other_models():
    """æµ‹è¯•å…¶ä»–å¯èƒ½çš„æ¨¡å‹"""
    print("\nğŸ”„ æµ‹è¯•å…¶ä»–æ¨¡å‹çš„è§†è§‰åŠŸèƒ½...")
    
    api_key = os.environ.get("SIMEN_AI_API_KEY")
    base_url = os.environ.get("SIMEN_BASEURL")
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
    models = [
        "gpt-4o",
        "gemini-1.5-flash",
        "gemini-2.0-flash-exp",
        "claude-3-haiku"
    ]
    
    test_image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/2023_06_08_Raccoon1.jpg/1599px-2023_06_08_Raccoon1.jpg"
    
    for model in models:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model}")
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
                            {"type": "image_url", "image_url": {"url": test_image_url}},
                        ],
                    }
                ],
                max_tokens=100,
            )
            result = response.choices[0].message.content
            print(f"âœ… {model}: {result}")
            
        except Exception as e:
            print(f"âŒ {model}: {str(e)}")

def main():
    print("ğŸ‘ï¸ æµ‹è¯•è§†è§‰åŠŸèƒ½")
    print("=" * 30)
    
    # å…ˆæµ‹è¯• gpt-4o-mini
    success = test_gpt4o_mini_vision()
    
    if success:
        print("\nğŸ‰ gpt-4o-mini è§†è§‰åŠŸèƒ½æ­£å¸¸ï¼")
        print("å¯ä»¥å°† nano-banana åŒ…çš„æ¨¡å‹æ”¹ä¸º gpt-4o-mini")
    else:
        print("\nğŸ’¥ gpt-4o-mini è§†è§‰åŠŸèƒ½å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ¨¡å‹...")
        test_other_models()

if __name__ == "__main__":
    main()

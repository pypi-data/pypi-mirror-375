# Example .env file for MCP Client Multi-Provider (Version 2.x)
# ---------------------------------------------------------------
# Copy this file to .env in the project root and fill in your details.
# Lines starting with # are comments and are ignored.
# Values are read by the application on startup. Changes require restart.

# --- Provider API Keys (Required for desired providers) ---
# Obtain keys from the respective provider websites.
# The client will use these keys to authenticate API calls.
# If DEFAULT_PROVIDER is set below, its key will be checked on startup in interactive mode.

ANTHROPIC_API_KEY=""
OPENAI_API_KEY=""
# For Google Gemini, use the API key generated in Google AI Studio or GCP.
GEMINI_API_KEY=""
# For xAI Grok
GROK_API_KEY=""
# For DeepSeek
DEEPSEEK_API_KEY=""
# For Mistral AI
MISTRAL_API_KEY=""
# For GroqCloud
GROQ_API_KEY=""
# For Cerebras
CEREBRAS_API_KEY=""
# For OpenRouter.ai
OPENROUTER_API_KEY=""


# --- Provider Base URLs (Optional - Override Defaults) ---
# Change these only if you use a proxy, self-hosted model (like LocalAI),
# or a different API endpoint than the provider's default.

# Default: Handled by OpenAI SDK (usually https://api.openai.com/v1)
OPENAI_BASE_URL=""
# Default: "https://generativelanguage.googleapis.com/v1beta/openai/" (OpenAI compatible API)
GEMINI_BASE_URL=""
# Default: "https://api.x.ai/v1"
GROK_BASE_URL=""
# Default: "https://api.deepseek.com/v1"
DEEPSEEK_BASE_URL=""
# Default: "https://api.mistral.ai/v1"
MISTRAL_BASE_URL=""
# Default: "https://api.groq.com/openai/v1"
GROQ_BASE_URL=""
# Default: "https://api.cerebras.ai/v1"
CEREBRAS_BASE_URL=""
# Default: "https://openrouter.ai/api/v1"
OPENROUTER_BASE_URL=""


# --- Core Client Settings ---

# Default provider to use if a model name doesn't specify one (e.g., "gpt-4o" vs "openai/gpt-4o").
# Must be one of the Provider enum values: openai, anthropic, deepseek, gemini, openrouter, grok, mistral, groq, cerebras
# Code Default: Not explicitly set, logic relies on model name inference first.
# Setting this helps if the API key prompt is needed on startup.
DEFAULT_PROVIDER="openai" # Example: Set your most frequently used provider

# Default model name to use if not specified in the query or via /model command.
# Should match a known model identifier for the chosen DEFAULT_PROVIDER or include a provider prefix.
# See DEFAULT_MODELS in the code for provider-specific defaults if this is unset.
# Code Default: "gpt-4.1-mini" (if OpenAI is inferred/default)
DEFAULT_MODEL="gpt-4o-mini" # Example

# Default maximum number of tokens for the model's response generation.
# Code Default: 8000
DEFAULT_MAX_TOKENS="8000"

# Default temperature for generation (0.0-2.0). Higher values = more creative/random.
# Code Default: 0.7
TEMPERATURE="0.7"

# Maximum number of query/response pairs to keep in the history view (/history).
# Code Default: 300
HISTORY_SIZE="300"

# --- MCP Server Discovery & Connection Settings ---

# Enable automatic discovery of MCP servers in local filesystem paths (DISCOVERY_PATHS).
# Code Default: true
AUTO_DISCOVER="true"

# Comma-separated list of absolute or relative paths to search for MCP servers.
# Overrides the default paths defined in the code (usually includes ~/.mcp-servers, project dirs).
# Example: DISCOVERY_PATHS="/path/to/my/servers,/another/path"
DISCOVERY_PATHS=""

# Enable Zeroconf/mDNS local network discovery for MCP servers. Requires 'zeroconf' library.
# Code Default: true
ENABLE_LOCAL_DISCOVERY="true"


# --- Port Scanning Settings ---

# Enable actively scanning local ports for MCP SSE servers.
# Code Default: true
ENABLE_PORT_SCANNING="true"
# Start port for scanning range.
# Code Default: 8000
PORT_SCAN_RANGE_START="8000"
# End port for scanning range (inclusive).
# Code Default: 9000
PORT_SCAN_RANGE_END="9000"
# Maximum number of concurrent port probes.
# Code Default: 50
PORT_SCAN_CONCURRENCY="50"
# Timeout per port probe in seconds (float).
# Code Default: 4.5
PORT_SCAN_TIMEOUT="4.5"
# Comma-separated list of target IP addresses for port scanning.
# Code Default: "127.0.0.1"
PORT_SCAN_TARGETS="127.0.0.1"


# --- Caching Settings ---

# Enable caching of MCP tool results to disk and memory.
# Code Default: true
ENABLE_CACHING="true"
# Note: Custom Cache TTLs (Time-To-Live) per tool category/name are configured
# via the 'cache_ttl_mapping' section in the config.yaml file, NOT here in .env.


# --- Registry Settings ---

# Enable connecting to remote MCP server registries for discovery.
# Code Default: true
ENABLE_REGISTRY="true"
# Comma-separated list of registry URLs to query for discovering public/shared MCP servers.
# Code Default: Empty list []
REGISTRY_URLS=""


# --- Conversation Optimization Settings ---

# Default model used for summarizing long conversations (/optimize or auto-summarize).
# Should be a fast and capable model (e.g., Claude Haiku, GPT-4o-mini).
# Code Default: "claude-3-haiku-20240307" (or Anthropic default if costs change)
SUMMARIZATION_MODEL="claude-3-5-haiku-20241022" # Example using latest known fast model

# Enable automatic summarization when conversation token count exceeds the threshold.
# Code Default: false
USE_AUTO_SUMMARIZATION="false"
# Token threshold to trigger auto-summarization if USE_AUTO_SUMMARIZATION is true.
# Code Default: 100000
AUTO_SUMMARIZE_THRESHOLD="100000"
# Target token count for the summary generated by optimization.
# Code Default: 1500
MAX_SUMMARIZED_TOKENS="1500"


# --- Other Settings ---

# Enable sending OpenTelemetry metrics (if exporter configured - currently console/disabled).
# Code Default: true
ENABLE_METRICS="true"
# Directory to store conversation graph files (JSON).
# Code Default: ./.mcpclient_multi_config/conversations
CONVERSATION_GRAPHS_DIR=""
# Refresh rate in seconds for the live dashboard (/dashboard).
# Code Default: 2.0
DASHBOARD_REFRESH_RATE="2.0"


# --- Debugging (Optional) ---
# These are read via os.environ.get directly, not part of the Config class parsing.

# Set to 1 to enable extremely verbose logging, including raw MCP JSON-RPC data.
# USE_VERBOSE_SESSION_LOGGING="0"
# Set to 1 (default) to enable checks for accidental stdout writes when STDIO servers run. Set to 0 to disable.
# MCP_VERIFY_STDOUT="1"
# Set to any value (e.g., "1") to show full Python tracebacks on unexpected errors in the console.
# MCP_CLIENT_DEBUG=""

import torch

from ...core import Chainable, Module


# class GradientAccumulation(Module):
#     """Uses :code:`n` steps to accumulate gradients, after :code:`n` gradients have been accumulated, they are passed to :code:`modules` and parameters are updates.

#     Accumulating gradients for :code:`n` steps is equivalent to increasing batch size by :code:`n`. Increasing the batch size
#     is more computationally efficient, but sometimes it is not feasible due to memory constraints.

#     .. note::
#         Technically this can accumulate any inputs, including updates generated by previous modules. As long as this module is first, it will accumulate the gradients.

#     Args:
#         modules (Chainable): modules that perform a step every :code:`n` steps using the accumulated gradients.
#         n (int): number of gradients to accumulate.
#         mean (bool, optional): if True, uses mean of accumulated gradients, otherwise uses sum. Defaults to True.
#         stop (bool, optional):
#             this module prevents next modules from stepping unless :code:`n` gradients have been accumulate. Setting this argument to False disables that. Defaults to True.

#     Examples:
#         Adam with gradients accumulated for 16 batches.

#         .. code-block:: python

#             opt = tz.Modular(
#                 model.parameters(),
#                 tz.m.GradientAccumulation(
#                     [tz.m.Adam(), tz.m.LR(1e-2)],
#                     n=16
#                 )
#             )

#     """
#     def __init__(self, modules: Chainable, n: int, mean=True, stop=True):
#         defaults = dict(n=n, mean=mean, stop=stop)
#         super().__init__(defaults)
#         self.set_child('modules', modules)


#     @torch.no_grad
#     def step(self, var):
#         accumulator = self.get_state(var.params, 'accumulator')
#         settings = self.defaults
#         n = settings['n']; mean = settings['mean']; stop = settings['stop']
#         step = self.global_state['step'] = self.global_state.get('step', 0) + 1

#         # add update to accumulator
#         torch._foreach_add_(accumulator, var.get_update())

#         # step with accumulated updates
#         if step % n == 0:
#             if mean:
#                 torch._foreach_div_(accumulator, n)

#             var.update = [a.clone() for a in accumulator]
#             var = self.children['modules'].step(var)

#             # zero accumulator
#             torch._foreach_zero_(accumulator)

#         else:
#             # prevent update
#             if stop:
#                 var.update = None
#                 var.stop=True
#                 var.skip_update=True

#         return var




class GradientAccumulation(Module):
    """Uses ``n`` steps to accumulate gradients, after ``n`` gradients have been accumulated, they are passed to :code:`modules` and parameters are updates.

    Accumulating gradients for ``n`` steps is equivalent to increasing batch size by ``n``. Increasing the batch size
    is more computationally efficient, but sometimes it is not feasible due to memory constraints.

    Note:
        Technically this can accumulate any inputs, including updates generated by previous modules. As long as this module is first, it will accumulate the gradients.

    Args:
        n (int): number of gradients to accumulate.
        mean (bool, optional): if True, uses mean of accumulated gradients, otherwise uses sum. Defaults to True.
        stop (bool, optional):
            this module prevents next modules from stepping unless ``n`` gradients have been accumulate. Setting this argument to False disables that. Defaults to True.

    ## Examples:

    Adam with gradients accumulated for 16 batches.

    ```python
    opt = tz.Modular(
        model.parameters(),
        tz.m.GradientAccumulation(),
        tz.m.Adam(),
        tz.m.LR(1e-2),
    )
    ```
    """
    def __init__(self, n: int, mean=True, stop=True):
        defaults = dict(n=n, mean=mean, stop=stop)
        super().__init__(defaults)


    @torch.no_grad
    def step(self, var):
        accumulator = self.get_state(var.params, 'accumulator')
        settings = self.defaults
        n = settings['n']; mean = settings['mean']; stop = settings['stop']
        step = self.global_state['step'] = self.global_state.get('step', 0) + 1

        # add update to accumulator
        torch._foreach_add_(accumulator, var.get_update())

        # step with accumulated updates
        if step % n == 0:
            if mean:
                torch._foreach_div_(accumulator, n)

            var.update = accumulator

            # zero accumulator
            self.clear_state_keys('accumulator')

        else:
            # prevent update
            if stop:
                var.update = None
                var.stop=True
                var.skip_update=True

        return var


import pandas as pd
import pickle
import json
import numpy as np
import ast
import sys
from collections import OrderedDict
import base64
from importlib import import_module
import sys

DELIMITER = "\t"

def convert_to_type(val, typee):
    if typee == 'int':
        return int(val) if val != "" else np.nan
    if typee == 'float':
        if isinstance(val, str):
            val = val.replace(' ', '')
        return float(val) if val != "" else np.nan
    if typee == 'bool':
        return eval(val) if val != "" else None
    return str(val) if val != "" else None

def splitter(strr, delim=",", convert_to="str"):
    """
    Split the string based on delimiter and convert to the type specified.
    """
    if strr == "None":
        return []
    return [convert_to_type(i, convert_to) for i in strr.split(delim)]


is_lake_system = eval(sys.argv[2])
model_file_prefix = sys.argv[1]
if not is_lake_system:
    db = sys.argv[0].split("/")[1]

### Start of data related arguments processing
data_partition_column_values = []
data_present = False
partition_join = ""
model = None

# Data related arguments information of indices and types.
data_args_indices_types = OrderedDict()

func_name = <func_name>
module_name = <module_name>
class_name = <class_name>
all_col_names = <all_col_names>
all_col_types = <types_of_data_cols>
data_partition_column_indices = <partition_cols_indices>
data_partition_column_types = [all_col_types[idx] for idx in data_partition_column_indices]

# Data related arguments values - prepare dictionary and populate data later.
data_args_values = {}

data_args_info_str = <data_args_info_str>

for data_arg in data_args_info_str.split("--"):
    _arg_name, _indices, _types = data_arg.split("-")
    _indices = splitter(_indices, convert_to="int")
    types = [type_ for idx, type_ in enumerate(all_col_types) if idx in _indices]

    data_args_indices_types[_arg_name] = {"indices": _indices, "types": types}
    data_args_values[_arg_name] = [] # Keeping empty for each data arg name and populate data later.

### End of data related arguments processing


### Start of other arguments processing
params = json.loads('<params>')
### End of other arguments processing


# Read data - columns information is passed as command line argument and stored in
# data_args_indices_types dictionary. 
while 1:
    try:
        line = input()
        if line == '':  # Exit if user provides blank line
            break
        else:
            data_present = True
            values = line.split(DELIMITER)
            if not data_partition_column_values:
                # Partition column values is same for all rows. Hence, only read once.
                for i, val in enumerate(data_partition_column_indices):
                    data_partition_column_values.append(
                        convert_to_type(values[val], typee=data_partition_column_types[i])
                        )

                # Prepare the corresponding model file name and extract model.
                partition_join = "_".join([str(x) for x in data_partition_column_values])
                # Replace '-' with '_' as '-' because partition_columns can be negative.
                partition_join = partition_join.replace("-", "_")

                model_file_path = f"{model_file_prefix}_{partition_join}"\
                    if is_lake_system else \
                    f"./{db}/{model_file_prefix}_{partition_join}"

            # Prepare data dictionary containing only arguments related to data.
            for arg_name in data_args_values:
                data_indices = data_args_indices_types[arg_name]["indices"]
                types = data_args_indices_types[arg_name]["types"]
                cur_row = []
                for idx, data_idx in enumerate(data_indices):
                    cur_row.append(convert_to_type(values[data_idx], types[idx]))
                data_args_values[arg_name].append(cur_row)
    except EOFError:  # Exit if reached EOF or CTRL-D
        break

if not data_present:
    sys.exit(0)

for key, value in data_args_values.items():
    col_names = [all_col_names[idx] for idx in data_args_indices_types[key]["indices"]]
    data_args_values[key] = pd.DataFrame(value, columns=col_names)

# If reference argument (is a Dataset object) present in params, then it contains
# the prefix of the file path which contains the reference Dataset object.
if "reference" in params.keys() and params["reference"] is not None:
    reference_dataset_file_prefix = params["reference"]
    reference_arg_file_path = f"{reference_dataset_file_prefix}_{partition_join}"\
                    if is_lake_system else \
                    f"./{db}/{reference_dataset_file_prefix}_{partition_join}"
    with open(reference_arg_file_path, "rb") as f:
        params["reference"] = pickle.load(f)

if not func_name:
    # Create DataSet object if no function of Dataset class is called.
    lib = import_module(module_name)
    class_instance = getattr(lib, class_name)
    obj = class_instance(**{**data_args_values, **params})
else:
    # If function of Dataset object is called, then call the function on model object.
    with open(model_file_path, "rb") as fp:
        model = pickle.loads(fp.read())

    if not model:
        sys.exit("Model file is not installed in Vantage.")

    obj = getattr(model, func_name)(**{**data_args_values, **params})

model_str = pickle.dumps(obj)

if is_lake_system:
    model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"

# Save DataSet object to binary file
with open(model_file_path, "wb") as f:
    f.write(model_str)

model_data = model_file_path if is_lake_system else base64.b64encode(model_str)

print(*(data_partition_column_values + [model_data]), sep=DELIMITER)
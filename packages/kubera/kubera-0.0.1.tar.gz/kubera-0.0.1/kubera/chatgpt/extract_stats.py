#!/usr/bin/env python3
"""
Extract statistics from ChatGPT conversation trace data.

Analyzes the CSV generated by extract_trace.py to provide insights on:
- Token usage patterns
- Session statistics
- Conversation flow analysis
- Time-based patterns
- Role distribution
"""

import argparse
import csv
import json
import os
from collections import Counter, defaultdict
from datetime import datetime
from typing import Any, Dict


def parse_timestamp(timestamp_str: str) -> datetime:
    """Parse ISO format timestamp."""
    if not timestamp_str:
        return None
    try:
        return datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
    except:
        try:
            return datetime.strptime(timestamp_str.split(".")[0], "%Y-%m-%dT%H:%M:%S")
        except:
            return None


def extract_statistics(csv_file: str) -> Dict[str, Any]:
    """Extract comprehensive statistics from the ChatGPT trace CSV file."""

    stats = {
        "overall": {
            "total_messages": 0,
            "total_tokens": 0,
            "total_conversations": 0,
            "role_distribution": Counter(),
        },
        "conversations": defaultdict(
            lambda: {
                "messages": 0,
                "total_tokens": 0,
                "duration_minutes": 0,
                "timestamps": [],
                "roles": Counter(),
                "tokens_by_role": defaultdict(int),
                "message_chain_length": 0,
            }
        ),
        "token_analysis": {
            "avg_tokens_per_message": 0,
            "avg_tokens_per_conversation": 0,
            "token_distribution_by_role": defaultdict(list),
        },
        "conversation_patterns": {
            "avg_messages_per_conversation": 0,
            "avg_conversation_duration_minutes": 0,
            "conversations_by_length": Counter(),
        },
    }

    rows = []

    # Read CSV file
    with open(csv_file, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)

            # Update overall stats
            stats["overall"]["total_messages"] += 1
            tokens = int(row["tokens"]) if row["tokens"] else 0
            stats["overall"]["total_tokens"] += tokens
            stats["overall"]["role_distribution"][row["role"]] += 1

            # Conversation stats
            session_uuid = row["session_uuid"]
            stats["conversations"][session_uuid]["messages"] += 1
            stats["conversations"][session_uuid]["total_tokens"] += tokens
            stats["conversations"][session_uuid]["roles"][row["role"]] += 1
            stats["conversations"][session_uuid]["tokens_by_role"][
                row["role"]
            ] += tokens

            # Track timestamps for conversation duration
            timestamp = parse_timestamp(row["timestamp"])
            if timestamp:
                stats["conversations"][session_uuid]["timestamps"].append(timestamp)

            # Token distribution by role
            stats["token_analysis"]["token_distribution_by_role"][row["role"]].append(
                tokens
            )

    # Calculate conversation durations and chain lengths
    conversation_durations = []
    conversation_lengths = []

    for session_uuid, conv_data in stats["conversations"].items():
        # Duration calculation
        if len(conv_data["timestamps"]) > 1:
            duration = max(conv_data["timestamps"]) - min(conv_data["timestamps"])
            conv_data["duration_minutes"] = duration.total_seconds() / 60
            conversation_durations.append(conv_data["duration_minutes"])
        else:
            conv_data["duration_minutes"] = 0

        # Message chain length
        conv_data["message_chain_length"] = conv_data["messages"]
        conversation_lengths.append(conv_data["messages"])

        # Remove timestamps from final output
        del conv_data["timestamps"]

    # Overall conversation count
    stats["overall"]["total_conversations"] = len(stats["conversations"])

    # Token analysis
    if stats["overall"]["total_messages"] > 0:
        stats["token_analysis"]["avg_tokens_per_message"] = (
            stats["overall"]["total_tokens"] / stats["overall"]["total_messages"]
        )

    if stats["overall"]["total_conversations"] > 0:
        stats["token_analysis"]["avg_tokens_per_conversation"] = (
            stats["overall"]["total_tokens"] / stats["overall"]["total_conversations"]
        )
        stats["conversation_patterns"]["avg_messages_per_conversation"] = (
            stats["overall"]["total_messages"] / stats["overall"]["total_conversations"]
        )

        if conversation_durations:
            stats["conversation_patterns"]["avg_conversation_duration_minutes"] = sum(
                conversation_durations
            ) / len(conversation_durations)

    # Conversation length distribution
    for length in conversation_lengths:
        if length <= 5:
            stats["conversation_patterns"]["conversations_by_length"][
                "1-5 messages"
            ] += 1
        elif length <= 10:
            stats["conversation_patterns"]["conversations_by_length"][
                "6-10 messages"
            ] += 1
        elif length <= 20:
            stats["conversation_patterns"]["conversations_by_length"][
                "11-20 messages"
            ] += 1
        elif length <= 50:
            stats["conversation_patterns"]["conversations_by_length"][
                "21-50 messages"
            ] += 1
        else:
            stats["conversation_patterns"]["conversations_by_length"][
                "50+ messages"
            ] += 1

    # Calculate average tokens by role
    for role, token_list in stats["token_analysis"][
        "token_distribution_by_role"
    ].items():
        if token_list:
            stats["token_analysis"]["token_distribution_by_role"][role] = {
                "total": sum(token_list),
                "average": sum(token_list) / len(token_list),
                "count": len(token_list),
            }

    # Convert defaultdicts and Counters to regular dicts for JSON serialization
    stats["conversations"] = dict(stats["conversations"])
    stats["overall"]["role_distribution"] = dict(stats["overall"]["role_distribution"])
    stats["token_analysis"]["token_distribution_by_role"] = dict(
        stats["token_analysis"]["token_distribution_by_role"]
    )
    stats["conversation_patterns"]["conversations_by_length"] = dict(
        stats["conversation_patterns"]["conversations_by_length"]
    )

    # Convert Counter objects in conversations
    for conv_uuid, conv_data in stats["conversations"].items():
        conv_data["roles"] = dict(conv_data["roles"])
        conv_data["tokens_by_role"] = dict(conv_data["tokens_by_role"])

    return stats


def print_statistics(stats: Dict[str, Any]):
    """Print statistics in a readable format."""

    print("\n" + "=" * 60)
    print("CHATGPT CONVERSATION ANALYSIS")
    print("=" * 60)

    print("\nðŸ“Š OVERALL STATISTICS")
    print("-" * 40)
    print(f"Total Messages: {stats['overall']['total_messages']:,}")
    print(f"Total Conversations: {stats['overall']['total_conversations']:,}")
    print(f"Total Tokens: {stats['overall']['total_tokens']:,}")

    print("\nðŸ‘¥ ROLE DISTRIBUTION")
    print("-" * 40)
    for role, count in stats["overall"]["role_distribution"].items():
        percentage = (count / stats["overall"]["total_messages"]) * 100
        print(f"{role.capitalize()}: {count:,} ({percentage:.1f}%)")

    print("\nðŸ”¢ TOKEN ANALYSIS")
    print("-" * 40)
    print(
        f"Average Tokens per Message: {stats['token_analysis']['avg_tokens_per_message']:.1f}"
    )
    print(
        f"Average Tokens per Conversation: {stats['token_analysis']['avg_tokens_per_conversation']:.1f}"
    )

    print("\nTokens by Role:")
    for role, data in stats["token_analysis"]["token_distribution_by_role"].items():
        if isinstance(data, dict):
            print(
                f"  {role.capitalize()}: {data['average']:.1f} avg, {data['total']:,} total ({data['count']:,} messages)"
            )

    print("\nðŸ’¬ CONVERSATION PATTERNS")
    print("-" * 40)
    print(
        f"Average Messages per Conversation: {stats['conversation_patterns']['avg_messages_per_conversation']:.1f}"
    )
    print(
        f"Average Conversation Duration: {stats['conversation_patterns']['avg_conversation_duration_minutes']:.1f} minutes"
    )

    print("\nConversation Length Distribution:")
    for length_range, count in stats["conversation_patterns"][
        "conversations_by_length"
    ].items():
        percentage = (count / stats["overall"]["total_conversations"]) * 100
        print(f"  {length_range}: {count:,} ({percentage:.1f}%)")

    print("\nðŸ“ˆ CONVERSATION TOKEN BREAKDOWN")
    print("-" * 40)
    if stats["conversations"]:
        # Calculate total tokens by role across all conversations
        total_tokens_by_role = defaultdict(int)
        for conv in stats["conversations"].values():
            for role, tokens in conv["tokens_by_role"].items():
                total_tokens_by_role[role] += tokens

        print("Token Distribution Across All Conversations:")
        for role, tokens in total_tokens_by_role.items():
            if stats["overall"]["total_tokens"] > 0:
                percentage = (tokens / stats["overall"]["total_tokens"]) * 100
                print(f"  {role.capitalize()}: {tokens:,} ({percentage:.1f}%)")

    print("\n" + "=" * 60)


def main():
    """Main function to process CSV and generate statistics."""
    parser = argparse.ArgumentParser(
        description="Extract statistics from ChatGPT conversation trace data."
    )
    parser.add_argument(
        "--input-file",
        type=str,
        default="./data/chatgpt_trace.csv",
        help="Input CSV trace file path.",
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="./data/stats/chatgpt_stats.json",
        help="Output file for statistics (optional, prints to stdout by default).",
    )

    args = parser.parse_args()

    # Check if input file exists
    if not os.path.exists(args.input_file):
        print(f"Error: Input file not found: {args.input_file}")
        return

    # Extract statistics
    print(f"Extracting statistics from ChatGPT trace data: {args.input_file}")

    stats = extract_statistics(args.input_file)

    # Output statistics
    if args.output_file:
        # Make sure the output directory exists
        output_dir = os.path.dirname(args.output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
        # Save to file
        with open(args.output_file, "w") as f:
            json.dump(stats, f, indent=2, default=str)
        print(f"\nStatistics saved to: {args.output_file}")

    print_statistics(stats)


if __name__ == "__main__":
    main()

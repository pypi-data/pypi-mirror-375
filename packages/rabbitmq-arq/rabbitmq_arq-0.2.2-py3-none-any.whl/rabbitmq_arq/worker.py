# -*- coding: utf-8 -*-
# @version        : 1.0
# @Create Time    : 2025/5/9 15:02
# @File           : worker
# @IDE            : PyCharm
# @desc           : Worker æ ¸å¿ƒå®ç° - ä½¿ç”¨ Python 3.12 ç°ä»£è¯­æ³•

from __future__ import annotations

import asyncio
import json
import logging
import signal
import traceback
import uuid
from collections.abc import Callable
from datetime import datetime, timedelta, timezone
from functools import partial
import inspect
from typing import get_type_hints
from signal import Signals
from typing import Any

from aio_pika import connect_robust, IncomingMessage, Message
from aio_pika.abc import AbstractConnection, AbstractChannel
from pydantic import TypeAdapter

from .connections import WorkerSettings
from .exceptions import Retry, JobTimeout, MaxRetriesExceeded, RabbitMQConnectionError
from .models import JobModel, JobContext, JobStatus, WorkerInfo
from .protocols import WorkerCoroutine
from .result_storage import ResultStore
from .result_storage.factory import create_result_store_from_settings
from .result_storage.models import JobResult
from .result_storage.url_parser import parse_store_type_from_url

logger = logging.getLogger('rabbitmq-arq.worker')

# TypeAdapter ç®€æ˜“ç¼“å­˜ï¼Œå‡å°‘é‡å¤æ„å»ºå¼€é”€
_TYPE_ADAPTER_CACHE: dict[str, TypeAdapter] = {}


# é”™è¯¯åˆ†ç±»å®šä¹‰
class ErrorClassification:
    """é”™è¯¯åˆ†ç±»é…ç½®ï¼Œç”¨äºæ™ºèƒ½é‡è¯•ç­–ç•¥"""

    # å¯é‡è¯•çš„é”™è¯¯ç±»å‹ï¼ˆä»…é™ç³»ç»Ÿçº§é”™è¯¯å’Œæ˜¾å¼é‡è¯•ï¼‰
    RETRIABLE_ERRORS = (
        RabbitMQConnectionError,  # RabbitMQ è¿æ¥é”™è¯¯
        TimeoutError,  # è¶…æ—¶é”™è¯¯
        OSError,  # æ“ä½œç³»ç»Ÿé”™è¯¯
        IOError,  # IOé”™è¯¯
        Retry,  # æ˜¾å¼é‡è¯•è¯·æ±‚ï¼ˆARQ é£æ ¼ï¼‰
    )

    # ä¸å¯é‡è¯•çš„é”™è¯¯ç±»å‹ï¼ˆåŒ…æ‹¬æ‰€æœ‰å…¶ä»–å¼‚å¸¸ï¼‰
    NON_RETRIABLE_ERRORS = (
        TypeError,  # å‡½æ•°ç­¾åé”™è¯¯ã€å‚æ•°ç±»å‹é”™è¯¯
        ValueError,  # å‚æ•°å€¼é”™è¯¯
        AttributeError,  # å±æ€§é”™è¯¯
        ImportError,  # å¯¼å…¥é”™è¯¯
        ModuleNotFoundError,  # æ¨¡å—æœªæ‰¾åˆ°
        SyntaxError,  # è¯­æ³•é”™è¯¯
        NameError,  # åç§°é”™è¯¯
        KeyError,  # å­—å…¸é”®é”™è¯¯ï¼ˆé…ç½®ç›¸å…³ï¼‰
        MaxRetriesExceeded,  # å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
        Exception,  # æ‰€æœ‰å…¶ä»–å¼‚å¸¸éƒ½ä¸è‡ªåŠ¨é‡è¯•ï¼ˆARQ é£æ ¼ï¼‰
    )

    @classmethod
    def is_retriable_error(cls, error: Exception) -> bool:
        """
        åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•ï¼ˆARQ é£æ ¼ï¼‰
        
        Args:
            error: å¼‚å¸¸å¯¹è±¡
            
        Returns:
            True å¦‚æœé”™è¯¯å¯é‡è¯•ï¼ŒFalse å¦‚æœåº”ç«‹å³å¤±è´¥
        """
        # æ˜¾å¼å¯é‡è¯•çš„é”™è¯¯ï¼ˆä¸»è¦æ˜¯ç³»ç»Ÿçº§é”™è¯¯å’Œ Retry å¼‚å¸¸ï¼‰
        if isinstance(error, cls.RETRIABLE_ERRORS):
            return True

        # æ‰€æœ‰å…¶ä»–å¼‚å¸¸éƒ½ä¸å¯é‡è¯•ï¼ˆARQ é£æ ¼ï¼‰
        return False

    @classmethod
    def get_error_category(cls, error: Exception) -> str:
        """
        è·å–é”™è¯¯åˆ†ç±»ï¼ˆARQ é£æ ¼ï¼‰
        
        Args:
            error: å¼‚å¸¸å¯¹è±¡
            
        Returns:
            é”™è¯¯åˆ†ç±»å­—ç¬¦ä¸²
        """
        if isinstance(error, cls.RETRIABLE_ERRORS):
            return "retriable"
        else:
            return "non_retriable"


class WorkerUtils:
    """
    æ¶ˆè´¹è€…å·¥å…·ç±» - åŸºç¡€å±æ€§å’Œä¿¡å·å¤„ç†
    """

    def __init__(self, worker_settings: WorkerSettings | None = None):
        self.allow_pick_jobs = True
        self.tasks: dict[str, asyncio.Task] = {}
        self.main_task: asyncio.Task | None = None
        self.on_stop: Callable | None = None

        # åŸºç¡€é…ç½® - å¦‚æœæœ‰worker_settingså°±ä½¿ç”¨ï¼Œå¦åˆ™åˆ›å»ºä¸´æ—¶å±æ€§
        self.worker_settings = worker_settings
        self.shutdown_event = asyncio.Event()

        # ä»»åŠ¡ç»Ÿè®¡
        self.jobs_complete = 0
        self.jobs_failed = 0
        self.jobs_retried = 0

        # Worker åŸºç¡€ä¿¡æ¯ - å¯èƒ½ä¼šè¢«å­ç±»è¦†ç›–
        self.worker_id = uuid.uuid4().hex
        self.worker_info = WorkerInfo(
            worker_id=self.worker_id,
            start_time=datetime.now(timezone.utc)
        )

        # ä¿¡å·å¤„ç†ç›¸å…³å±æ€§
        self._job_completion_wait = 30  # é»˜è®¤ç­‰å¾…æ—¶é—´30ç§’

        # å­ç±»å¯èƒ½éœ€è¦çš„burstæ¨¡å¼ç›¸å…³å±æ€§çš„é»˜è®¤å€¼
        self._burst_mode = False
        self._burst_should_exit = False

        # è¿æ¥ç›¸å…³å±æ€§ - å­ç±»ä¼šè¦†ç›–è¿™äº›é»˜è®¤å€¼
        self.connection: AbstractConnection | None = None
        self.channel: AbstractChannel | None = None
        self.dlq_channel: AbstractChannel | None = None

        # è®¾ç½®ä¿¡å·å¤„ç†å™¨çš„æ ‡å¿—ï¼Œå­ç±»å¯ä»¥æ§åˆ¶æ˜¯å¦å¯ç”¨
        self._signal_handlers_enabled = False

    def handle_sig_wait_for_completion(self, signum: Signals) -> None:
        """
        å…è®¸ä»»åŠ¡åœ¨ç»™å®šæ—¶é—´å†…å®Œæˆåå†å…³é—­ worker çš„ä¿¡å·å¤„ç†å™¨ã€‚
        å¯é€šè¿‡ `wait_for_job_completion_on_signal_second` é…ç½®æ—¶é—´ã€‚
        æ”¶åˆ°ä¿¡å·å worker å°†åœæ­¢è·å–æ–°ä»»åŠ¡ã€‚
        """
        sig = Signals(signum)

        # è®°å½•å½“å‰çŠ¶æ€
        running_tasks = len(self.tasks)
        logger.info(
            f'ğŸ›‘ æ”¶åˆ° {sig.name} ä¿¡å· - ç»Ÿè®¡ä¿¡æ¯: âœ…å®Œæˆ:{self.jobs_complete} âŒå¤±è´¥:{self.jobs_failed} '
            f'ğŸ”„é‡è¯•:{self.jobs_retried} â³è¿è¡Œä¸­:{running_tasks}'
        )

        if self._burst_mode:
            logger.info(f'ğŸ›‘ Burst æ¨¡å¼æ”¶åˆ°ä¿¡å· {sig.name}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­')
            self.allow_pick_jobs = False
            self._burst_should_exit = True

            # ç«‹å³å–æ¶ˆæ¶ˆè´¹è€…ï¼Œåœæ­¢æ¥æ”¶æ–°æ¶ˆæ¯
            asyncio.create_task(self._cancel_consumer())

            # åœ¨ burst æ¨¡å¼ä¸‹ï¼Œå¯ä»¥é€‰æ‹©ç«‹å³é€€å‡ºæˆ–ç­‰å¾…ä»»åŠ¡å®Œæˆ
            if self.worker_settings.burst_wait_for_tasks and running_tasks > 0:
                logger.info(f'â³ Burst æ¨¡å¼ï¼šç­‰å¾… {running_tasks} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆ...')
                asyncio.create_task(self._wait_for_tasks_to_complete(signum=sig))
            else:
                if running_tasks > 0:
                    logger.info(f'ğŸš« Burst æ¨¡å¼ï¼šä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œå–æ¶ˆ {running_tasks} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡')
                    # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
                    for t in self.tasks.values():
                        if not t.done():
                            t.cancel()
                else:
                    logger.info('âœ… Burst æ¨¡å¼ï¼šæ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œç«‹å³é€€å‡º')
                self.main_task and self.main_task.cancel()
        else:
            logger.info(f'ğŸ”„ å¸¸è§„æ¨¡å¼ï¼šå¼€å§‹ä¼˜é›…å…³é—­ï¼Œåœæ­¢æ¥æ”¶æ–°ä»»åŠ¡')
            self.allow_pick_jobs = False

            # ç«‹å³å–æ¶ˆæ¶ˆè´¹è€…ï¼Œåœæ­¢æ¥æ”¶æ–°æ¶ˆæ¯
            asyncio.create_task(self._cancel_consumer())

            if running_tasks > 0:
                # è·å–ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
                timeout = (getattr(self.worker_settings, 'wait_for_job_completion_on_signal_second', None)
                           if self.worker_settings else None) or 30
                logger.info(
                    f'â³ ç­‰å¾… {running_tasks} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰')
                asyncio.create_task(self._wait_for_tasks_to_complete(signum=sig))
            else:
                logger.info('âœ… æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¯ä»¥ç«‹å³å…³é—­')
                self.shutdown_event.set()
                # å–æ¶ˆä¸»ä»»åŠ¡ä»¥ç«‹å³é€€å‡º
                if self.main_task and not self.main_task.done():
                    self.main_task.cancel()

    async def _wait_for_tasks_to_complete(self, signum: Signals) -> None:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œç›´åˆ°è¾¾åˆ° `wait_for_job_completion_on_signal_second`ã€‚
        """
        start_time = datetime.now(timezone.utc)
        initial_tasks = len(self.tasks)
        # ä½¿ç”¨worker_settingsä¸­çš„é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        timeout = (getattr(self.worker_settings, 'wait_for_job_completion_on_signal_second', None)
                   if self.worker_settings else None) or self._job_completion_wait

        logger.info(f'â³ å¼€å§‹ç­‰å¾…ä»»åŠ¡å®Œæˆï¼šåˆå§‹ä»»åŠ¡æ•° {initial_tasks}ï¼Œè¶…æ—¶æ—¶é—´ {timeout} ç§’')

        try:
            await asyncio.wait_for(
                self._sleep_until_tasks_complete(),
                timeout,
            )
            elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.info(f'âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’')
        except asyncio.TimeoutError:
            elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
            remaining_tasks = len(self.tasks)
            completed_tasks = initial_tasks - remaining_tasks

            logger.warning(
                f'â° ç­‰å¾…è¶…æ—¶ï¼ˆ{elapsed:.2f}ç§’ï¼‰ï¼š{completed_tasks} ä¸ªä»»åŠ¡å·²å®Œæˆï¼Œ'
                f'{remaining_tasks} ä¸ªä»»åŠ¡å°†è¢«å¼ºåˆ¶å–æ¶ˆ'
            )

        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€ç»Ÿè®¡
        cancelled_count = sum(not t.done() for t in self.tasks.values())
        logger.info(
            f'ğŸ”š å…³é—­ä¿¡å· {signum.name} å¤„ç†å®Œæˆ - ç»Ÿè®¡ä¿¡æ¯: âœ…å®Œæˆ:{self.jobs_complete} '
            f'âŒå¤±è´¥:{self.jobs_failed} ğŸ”„é‡è¯•:{self.jobs_retried} ğŸš«å–æ¶ˆ:{cancelled_count}'
        )

        # å–æ¶ˆå‰©ä½™çš„ä»»åŠ¡
        for t in self.tasks.values():
            if not t.done():
                t.cancel()

        # è®¾ç½®å…³é—­äº‹ä»¶
        self.shutdown_event.set()

        # å–æ¶ˆä¸»ä»»åŠ¡
        self.main_task and self.main_task.cancel()

        # æ‰§è¡Œå…³é—­å›è°ƒ
        self.on_stop and self.on_stop(signum)

    async def _sleep_until_tasks_complete(self) -> None:
        """
        ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚ä¸ asyncio.wait_for() ä¸€èµ·ä½¿ç”¨ã€‚
        """
        while len(self.tasks):
            await asyncio.sleep(0.1)

    def _add_signal_handler(self, signum: Signals, handler: Callable[[Signals], None]) -> None:
        try:
            # ä½¿ç”¨å½“å‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œè€Œä¸æ˜¯ä¿å­˜çš„å¾ªç¯å¼•ç”¨
            loop = asyncio.get_running_loop()
            loop.add_signal_handler(signum, partial(handler, signum))
            logger.debug(f"âœ… å·²è®¾ç½® {signum.name} ä¿¡å·å¤„ç†å™¨")
        except NotImplementedError:  # pragma: no cover
            logger.debug('Windows ä¸æ”¯æŒå‘äº‹ä»¶å¾ªç¯æ·»åŠ ä¿¡å·å¤„ç†å™¨')
        except RuntimeError as e:
            logger.warning(f"âš ï¸ æ— æ³•è®¾ç½® {signum.name} ä¿¡å·å¤„ç†å™¨: {e}")

    async def _cancel_consumer(self) -> None:
        """
        å–æ¶ˆæ¶ˆè´¹è€…ï¼Œåœæ­¢æ¥æ”¶æ–°æ¶ˆæ¯
        """
        if hasattr(self, '_consumer_tag') and self._consumer_tag and hasattr(self, '_queue') and self._queue:
            if not self.channel or self.channel.is_closed:
                logger.warning("âš ï¸ æ¶ˆæ¯é€šé“å·²å…³é—­ï¼Œæ— æ³•å–æ¶ˆæ¶ˆè´¹è€…")
                return

            try:
                logger.info("ğŸ›‘ ç«‹å³åœæ­¢æ¶ˆæ¯æ¶ˆè´¹è€…ï¼Œé˜»æ­¢æ¥æ”¶æ–°æ¶ˆæ¯")
                await self._safe_operation_with_timeout(
                    self._queue.cancel(self._consumer_tag),
                    "å–æ¶ˆæ¶ˆæ¯æ¶ˆè´¹è€…",
                    timeout=5.0
                )
                logger.info("âœ… æ¶ˆæ¯æ¶ˆè´¹è€…å·²åœæ­¢")
                self._consumer_tag = None
            except Exception as e:
                logger.warning(f"âš ï¸ å–æ¶ˆæ¶ˆè´¹è€…æ—¶å‡ºç°é”™è¯¯: {e}")
        else:
            logger.debug("ğŸ” æ¶ˆè´¹è€…æœªå¯åŠ¨æˆ–å·²å–æ¶ˆ")

    async def _safe_operation_with_timeout(self, operation, operation_name: str, timeout: float = 30.0):
        """
        å®‰å…¨æ‰§è¡Œæ“ä½œï¼Œå¸¦è¶…æ—¶ä¿æŠ¤å’Œå¼‚å¸¸å¤„ç†
        
        Args:
            operation: è¦æ‰§è¡Œçš„åç¨‹æ“ä½œ
            operation_name: æ“ä½œåç§°ï¼Œç”¨äºæ—¥å¿—
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        try:
            logger.debug(f"ğŸ”§ å¼€å§‹æ‰§è¡Œ {operation_name}...")
            await asyncio.wait_for(operation, timeout=timeout)
            logger.debug(f"âœ… {operation_name} æ‰§è¡ŒæˆåŠŸ")
        except asyncio.TimeoutError:
            logger.warning(f"â° {operation_name} æ‰§è¡Œè¶…æ—¶ ({timeout}ç§’)")
        except Exception as e:
            logger.error(f"âŒ {operation_name} æ‰§è¡Œå¤±è´¥: {e}")
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    async def graceful_shutdown(self, reason: str = "ç”¨æˆ·è¯·æ±‚") -> None:
        """
        åŸºç¡€ä¼˜é›…å…³é—­æ–¹æ³• - ä¸åŒ…å«ç‰¹å®šçš„ç»“æœå­˜å‚¨é€»è¾‘
        
        Args:
            reason: å…³é—­åŸå› ï¼Œç”¨äºæ—¥å¿—è®°å½•
        """
        running_tasks = len(self.tasks)
        logger.info(
            f'ğŸ”„ å¼€å§‹ä¼˜é›…å…³é—­ Worker - åŸå› : {reason}'
            f' - ç»Ÿè®¡ä¿¡æ¯: âœ…å®Œæˆ:{self.jobs_complete} âŒå¤±è´¥:{self.jobs_failed} '
            f'ğŸ”„é‡è¯•:{self.jobs_retried} â³è¿è¡Œä¸­:{running_tasks}'
        )

        # åœæ­¢æ¥æ”¶æ–°ä»»åŠ¡
        self.allow_pick_jobs = False

        # ç«‹å³å–æ¶ˆæ¶ˆè´¹è€…ï¼Œåœæ­¢æ¥æ”¶æ–°æ¶ˆæ¯
        await self._cancel_consumer()

        # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œç­‰å¾…å®ƒä»¬å®Œæˆ
        if running_tasks > 0:
            # ç»Ÿä¸€çš„è¶…æ—¶æ—¶é—´è·å–é€»è¾‘
            timeout = (getattr(self.worker_settings, 'wait_for_job_completion_on_signal_second', None)
                       if self.worker_settings else None) or 30
            logger.info(f'â³ ç­‰å¾… {running_tasks} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰')

            try:
                await asyncio.wait_for(
                    self._sleep_until_tasks_complete(),
                    timeout=timeout
                )
                logger.info('âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œå¼€å§‹å…³é—­è¿æ¥')
            except asyncio.TimeoutError:
                remaining = len(self.tasks)
                logger.warning(f'â° ç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶å–æ¶ˆ {remaining} ä¸ªæœªå®Œæˆçš„ä»»åŠ¡')
                for t in self.tasks.values():
                    if not t.done():
                        t.cancel()

        # å…³é—­è¿æ¥ - ä½¿ç”¨è¶…æ—¶ä¿æŠ¤
        if self.connection and not self.connection.is_closed:
            await self._safe_operation_with_timeout(
                self.connection.close(),
                "RabbitMQ è¿æ¥å…³é—­ (graceful_shutdown)",
                timeout=10.0
            )

        # è®¾ç½®å…³é—­äº‹ä»¶
        self.shutdown_event.set()
        logger.info('âœ… Worker åŸºç¡€å…³é—­å®Œæˆ')


class Worker(WorkerUtils):
    """
    æ¶ˆè´¹è€…åŸºç±»ã€‚
    è¯¥ç±»ç”¨äºå®ç° RabbitMQ æ¶ˆè´¹è€…çš„æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒè‡ªå®šä¹‰å¯åŠ¨ã€å…³é—­ã€ä»»åŠ¡å¼€å§‹å’Œç»“æŸçš„é’©å­å‡½æ•°ï¼Œ
    å¹¶å¯é€šè¿‡ ctx ä¼ é€’ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
    """

    def __init__(self, worker_settings: WorkerSettings) -> None:
        """
        åˆå§‹åŒ– Worker
        
        Args:
            worker_settings: Worker é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„é…ç½®å‚æ•°
        """
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä¼ é€’worker_settings
        super().__init__(worker_settings)

        # Workerç‰¹æœ‰çš„å±æ€§
        self.functions = {fn.__name__: fn for fn in worker_settings.functions}
        self.consuming = False

        # ç”Ÿå‘½å‘¨æœŸé’©å­
        self.on_startup = worker_settings.on_startup
        self.on_shutdown = worker_settings.on_shutdown
        self.on_job_start = worker_settings.on_job_start
        self.on_job_end = worker_settings.on_job_end

        # ä¸Šä¸‹æ–‡
        self.ctx = {}

        # å…¼å®¹æ€§å±æ€§
        self.functions_map = self.functions  # å…¼å®¹æ€§åˆ«å
        self.after_job_end = None  # å…¼å®¹æ€§é’©å­

        # è¦†ç›–çˆ¶ç±»çš„worker_idï¼Œä½¿ç”¨é…ç½®ä¸­çš„åç§°
        self.worker_id = worker_settings.worker_name or f"worker_{uuid.uuid4().hex[:8]}"
        self.worker_info = WorkerInfo(
            worker_id=self.worker_id,
            start_time=datetime.now(timezone.utc)
        )

        # Burst æ¨¡å¼ç›¸å…³ï¼ˆè¦†ç›–çˆ¶ç±»é»˜è®¤å€¼ï¼‰
        self._burst_mode = worker_settings.burst_mode
        self._burst_start_time: datetime | None = None
        self._burst_check_task: asyncio.Task | None = None
        self._health_check_task: asyncio.Task | None = None

        # å»¶è¿Ÿä»»åŠ¡æœºåˆ¶é…ç½®ï¼ˆåˆå§‹åŒ–æ—¶æš‚ä¸è®¾ç½®ï¼Œåœ¨è¿æ¥åæŒ‰éœ€è®¾ç½®ï¼‰
        self._use_delayed_exchange = False
        self._delayed_exchange_name = None
        self._delay_queue_name = None
        self._delay_mechanism_detected = False

        # æ¶ˆè´¹è€…æ ‡ç­¾ç®¡ç† - ç”¨äºå–æ¶ˆæ¶ˆè´¹è€…
        self._consumer_tag: str | None = None
        self._queue = None

        # ç»“æœå­˜å‚¨åˆå§‹åŒ–
        self.result_store: ResultStore | None = None
        self._init_result_store()

        # å¹¶å‘æ§åˆ¶ï¼ˆä¸ prefetch ååŒï¼‰ï¼šæœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
        self._job_semaphore: asyncio.Semaphore | None = None
        try:
            mcj = getattr(self.worker_settings, 'max_concurrent_jobs', None)
            if isinstance(mcj, int) and mcj > 0:
                self._job_semaphore = asyncio.Semaphore(mcj)
        except Exception:
            # è‹¥é…ç½®å¼‚å¸¸åˆ™ä¸å¯ç”¨å¹¶å‘é™åˆ¶
            self._job_semaphore = None

        # ä¿¡å·å¤„ç†å™¨å°†åœ¨ main() æ–¹æ³•ä¸­è®¾ç½®ï¼Œå› ä¸ºæ­¤æ—¶äº‹ä»¶å¾ªç¯è¿˜æ²¡æœ‰è¿è¡Œ

    @staticmethod
    def _ensure_aware_utc(dt: datetime | None) -> datetime | None:
        """å°† datetime ç»Ÿä¸€ä¸ºå¸¦æ—¶åŒº(UTC)ã€‚è‹¥ä¼ å…¥ä¸º naiveï¼Œåˆ™å‡å®šä¸º UTCã€‚"""
        if dt is None:
            return None
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)

    def _init_result_store(self) -> None:
        """åˆå§‹åŒ–ç»“æœå­˜å‚¨"""
        try:
            self.result_store = create_result_store_from_settings(
                store_url=self.worker_settings.job_result_store_url,
                ttl=self.worker_settings.job_result_ttl
            )

            if self.result_store:
                store_type = parse_store_type_from_url(self.worker_settings.job_result_store_url)
                logger.info(f"ä»»åŠ¡ç»“æœå­˜å‚¨å·²åˆå§‹åŒ–: {store_type} ({self.worker_settings.job_result_store_url})")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–ç»“æœå­˜å‚¨å¤±è´¥: {e}")
            logger.info("å°†åœ¨ä¸å­˜å‚¨ç»“æœçš„æƒ…å†µä¸‹ç»§ç»­è¿è¡Œ")
    
    async def _validate_result_store(self) -> None:
        """éªŒè¯ç»“æœå­˜å‚¨è¿æ¥"""
        if not self.result_store:
            logger.error("âš ï¸ æœªé…ç½®ç»“æœå­˜å‚¨ï¼Œä»»åŠ¡ç»“æœå°†ä¸ä¼šè¢«ä¿å­˜")
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å…è®¸é™çº§
            if getattr(self.worker_settings, 'job_result_store_degrade_on_failure', True):
                logger.warning("âš ï¸ ç»“æœå­˜å‚¨æœªé…ç½®ï¼šå¯ç”¨é™çº§æ¨¡å¼ï¼ˆä¸ä¿å­˜ç»“æœï¼‰")
                return
            raise RuntimeError("ç»“æœå­˜å‚¨æœªé…ç½®")
        
        try:
            # è°ƒç”¨å­˜å‚¨å¯¹è±¡çš„è¿æ¥éªŒè¯æ–¹æ³•
            is_valid = await self.result_store.validate_connection()
            if is_valid:
                store_type = parse_store_type_from_url(self.worker_settings.job_result_store_url)
                logger.info(f"âœ… ç»“æœå­˜å‚¨è¿æ¥éªŒè¯æˆåŠŸ: {store_type}")
            else:
                store_type = parse_store_type_from_url(self.worker_settings.job_result_store_url)
                raise ValueError(f"ç»“æœå­˜å‚¨è¿æ¥éªŒè¯å¤±è´¥: {store_type}")
                
        except Exception as e:
            store_type = parse_store_type_from_url(self.worker_settings.job_result_store_url)
            logger.error(f"âŒ ç»“æœå­˜å‚¨è¿æ¥éªŒè¯å¤±è´¥ ({store_type}): {e}")
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦é™çº§
            if getattr(self.worker_settings, 'job_result_store_degrade_on_failure', True):
                logger.warning("âš ï¸ å¯ç”¨é™çº§æ¨¡å¼ï¼šç¦ç”¨ç»“æœå­˜å‚¨å¹¶ç»§ç»­è¿è¡Œ")
                self.result_store = None
                return
            # å¦åˆ™æŠ›å‡ºå¼‚å¸¸é˜»æ­¢å¯åŠ¨
            raise RuntimeError(f"ç»“æœå­˜å‚¨ ({store_type}) è¿æ¥å¤±è´¥ï¼ŒWorker æ— æ³•å¯åŠ¨") from e

    async def _store_job_result(self, job: JobModel) -> None:
        """å­˜å‚¨ä»»åŠ¡ç»“æœ
        
        Args:
            job: ä»»åŠ¡æ¨¡å‹ï¼ŒåŒ…å«æ‰§è¡Œç»“æœå’Œå…ƒæ•°æ®
        """
        # æ£€æŸ¥ç»“æœå­˜å‚¨æ˜¯å¦å¯ç”¨
        if not self.result_store:
            return  # ç»“æœå­˜å‚¨æœªå¯ç”¨ã€åˆå§‹åŒ–å¤±è´¥æˆ–è¿æ¥ä¸å¯ç”¨

        try:

            # æ„å»ºç»“æœå¯¹è±¡
            job_result = JobResult(
                job_id=job.job_id,
                status=job.status,
                result=job.result,
                error=job.error,
                start_time=job.start_time,
                end_time=job.end_time,
                duration=(job.end_time - job.start_time).total_seconds() if job.end_time else None,
                worker_id=self.worker_id,
                queue_name=job.queue_name,
                retry_count=job.job_try - 1,  # job_try ä»1å¼€å§‹
                function_name=job.function,
                args=job.args,
                kwargs=job.kwargs
            )

            # å¼‚æ­¥å­˜å‚¨ç»“æœ
            await self.result_store.store_result(job_result)
            logger.debug(f"ä»»åŠ¡ç»“æœå·²å­˜å‚¨: {job.job_id} - {job.status}")

        except Exception as e:
            # å­˜å‚¨å¤±è´¥ä¸åº”å½±å“ä»»åŠ¡å¤„ç†æµç¨‹ï¼Œä½†æ ‡è®°å­˜å‚¨ä¸ºä¸å¯ç”¨é¿å…é‡å¤æŠ¥é”™
            logger.warning(f"å­˜å‚¨ä»»åŠ¡ç»“æœå¤±è´¥ {job.job_id}: {e}")

    def _setup_signal_handlers(self) -> None:
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        if not self._signal_handlers_enabled:
            logger.info("ğŸ”§ æ­£åœ¨è®¾ç½®ä¿¡å·å¤„ç†å™¨...")

            # è®¾ç½®ä¸»è¦çš„ç»ˆæ­¢ä¿¡å·å¤„ç†å™¨
            signals_to_handle = [signal.SIGINT, signal.SIGTERM]

            # åœ¨éWindowsç³»ç»Ÿä¸Šæ·»åŠ SIGHUPæ”¯æŒ
            if hasattr(signal, 'SIGHUP'):
                signals_to_handle.append(signal.SIGHUP)

            for sig in signals_to_handle:
                self._add_signal_handler(sig, self.handle_sig_wait_for_completion)

            self._signal_handlers_enabled = True
            signal_names = [sig.name for sig in signals_to_handle]
            logger.info(f"âœ… ä¿¡å·å¤„ç†å™¨è®¾ç½®å®Œæˆ ({', '.join(signal_names)})")
            logger.info("ğŸ’¡ æç¤º: è¯·ä½¿ç”¨ Ctrl+Cã€kill -TERM æˆ– kill -HUP ä¼˜é›…åœæ­¢ Worker")

    async def _init(self) -> None:
        """åˆå§‹åŒ–è¿æ¥"""
        if not self.worker_settings.rabbitmq_settings:
            raise ValueError("å¿…é¡»æä¾› RabbitMQ è¿æ¥é…ç½®")

        logger.info(f"æ­£åœ¨è¿æ¥åˆ° RabbitMQ: {self.worker_settings.rabbitmq_settings.rabbitmq_url}")
        self.connection = await connect_robust(self.worker_settings.rabbitmq_settings.rabbitmq_url)
        self.channel = await self.connection.channel()
        self.dlq_channel = await self.connection.channel()

        # è®¾ç½® QoS é™åˆ¶é¢„å–æ¶ˆæ¯æ•°é‡
        await self.channel.set_qos(prefetch_count=self.worker_settings.rabbitmq_settings.prefetch_count)

        # é˜Ÿåˆ—åç§°è®¾ç½®
        self.rabbitmq_queue = self.worker_settings.queue_name
        self.rabbitmq_dlq = self.worker_settings.dlq_name

        # æ„å»ºå»¶è¿Ÿæœºåˆ¶ç›¸å…³åç§°ï¼ˆä¸ Client ä¿æŒä¸€è‡´ï¼‰
        self._delayed_exchange_name = f"delayed.{self.worker_settings.queue_name}"
        self._delay_queue_name = f"delay.{self.worker_settings.queue_name}"

        # å£°æ˜ä¸»é˜Ÿåˆ—
        await self.channel.declare_queue(self.rabbitmq_queue, durable=True)

        # å£°æ˜æ­»ä¿¡é˜Ÿåˆ—
        await self.dlq_channel.declare_queue(self.rabbitmq_dlq, durable=True)

        # æ£€æµ‹å»¶è¿Ÿæœºåˆ¶
        await self._setup_delay_mechanism()

        logger.info(f"æˆåŠŸè¿æ¥åˆ° RabbitMQï¼Œé˜Ÿåˆ—: {self.rabbitmq_queue}")
        
        # è¾“å‡ºè®¢é˜…çš„é˜Ÿåˆ—ä¿¡æ¯
        logger.info(f"ğŸ“‹ è®¢é˜…é˜Ÿåˆ—: {self.rabbitmq_queue}")
        logger.info(f"ğŸ’€ æ­»ä¿¡é˜Ÿåˆ—: {self.rabbitmq_dlq}")
        if hasattr(self, '_delay_queue_name'):
            logger.info(f"â° å»¶è¿Ÿé˜Ÿåˆ—: {self._delay_queue_name}")
        
        # è¾“å‡ºæ³¨å†Œçš„å‡½æ•°åˆ—è¡¨
        if self.worker_settings.functions:
            function_names = [func.__name__ for func in self.worker_settings.functions]
            logger.info(f"ğŸ”§ æ³¨å†Œå‡½æ•°: {', '.join(function_names)}")
        else:
            logger.info("âš ï¸ æœªæ³¨å†Œä»»ä½•å‡½æ•°")

    async def on_message(self, message: IncomingMessage) -> None:
        """
        å¤„ç† RabbitMQ æ¶ˆæ¯çš„å›è°ƒæ–¹æ³•ï¼ŒåŒ…å«é‡è¯•å’Œå¤±è´¥è½¬æ­»ä¿¡é˜Ÿåˆ—é€»è¾‘ã€‚
        """
        job_id = None
        
        # æå‰æ£€æŸ¥æ˜¯å¦å…è®¸æ¥æ”¶æ–°ä»»åŠ¡ï¼Œé¿å…æ¶ˆæ¯è¢«processåå†reject
        if not self.allow_pick_jobs:
            # ç›´æ¥rejectï¼Œä¸è¿›å…¥processä¸Šä¸‹æ–‡
            await message.reject(requeue=True)
            logger.warning(f"Worker æ­£åœ¨å…³é—­ï¼Œæ‹’ç»æ¶ˆæ¯å¤„ç†")
            return
            
        async with message.process(requeue=False):  # ç¦ç”¨è‡ªåŠ¨é‡å…¥é˜Ÿï¼Œé˜²æ­¢é‡å¤æ¶ˆè´¹
            headers = message.headers or {}
            retry_count = headers.get("x-retry-count", 0)

            try:
                # è§£ææ¶ˆæ¯
                job_data = json.loads(message.body.decode())
                job = JobModel(**job_data)
                job_id = job.job_id

                # æ£€æŸ¥æ˜¯å¦æ˜¯å®¢æˆ·ç«¯å·²å¤„ç†çš„å»¶è¿Ÿä»»åŠ¡
                client_delayed = headers.get("x-client-delayed") == "true"

                # åªæœ‰éå®¢æˆ·ç«¯å»¶è¿Ÿä»»åŠ¡æ‰éœ€è¦æ£€æŸ¥å»¶è¿Ÿæ‰§è¡Œæ—¶é—´
                if not client_delayed and job.defer_until:
                    now_utc = datetime.now(timezone.utc)
                    defer_dt = self._ensure_aware_utc(job.defer_until)
                    if defer_dt and defer_dt > now_utc:
                        delay_seconds = (defer_dt - now_utc).total_seconds()
                        logger.info(f"ä»»åŠ¡ {job_id} éœ€è¦å»¶è¿Ÿ {delay_seconds:.1f} ç§’æ‰§è¡Œï¼Œå‘é€åˆ°å»¶è¿Ÿé˜Ÿåˆ—")
                        # å‘é€åˆ°å»¶è¿Ÿé˜Ÿåˆ—ï¼Œä¸é˜»å¡å½“å‰å¤„ç†
                        await self._send_to_delay_queue(job, delay_seconds)
                        return
                elif client_delayed:
                    logger.debug(f"ä»»åŠ¡ {job_id} å·²ç”±å®¢æˆ·ç«¯å¤„ç†å»¶è¿Ÿï¼Œç›´æ¥æ‰§è¡Œ")

                # åˆ›å»ºä»»åŠ¡å¹¶æ‰§è¡Œï¼ˆå—å¹¶å‘é™åˆ¶ï¼‰
                acquired = False
                try:
                    if self._job_semaphore is not None:
                        await self._job_semaphore.acquire()
                        acquired = True

                    task = asyncio.create_task(self._execute_job(job))
                    self.tasks[job_id] = task

                    # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                    await task
                finally:
                    if acquired:
                        try:
                            self._job_semaphore.release()
                        except Exception:
                            pass

            except json.JSONDecodeError as e:
                logger.error(f"æ¶ˆæ¯è§£æå¤±è´¥: {e}\n{message.body}")
                # æ— æ³•è§£æçš„æ¶ˆæ¯ç›´æ¥å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                await self._send_to_dlq_with_error(message.body, headers, e, job_id="parse_failed")

            except Exception as e:
                logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}\n{traceback.format_exc()}")

                # ARQ é£æ ¼é”™è¯¯å¤„ç†
                error_category = ErrorClassification.get_error_category(e)

                # ä¸å¯é‡è¯•çš„é”™è¯¯ï¼šç«‹å³å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                if not ErrorClassification.is_retriable_error(e):
                    logger.error(f"ä»»åŠ¡ {job_id} é‡åˆ°ä¸å¯é‡è¯•é”™è¯¯ ({error_category}): {type(e).__name__}: {e}")
                    await self._send_to_dlq_with_error(message.body, headers, e, job_id)
                    return

                # å¯é‡è¯•çš„é”™è¯¯ï¼šæ£€æŸ¥é‡è¯•æ¬¡æ•°é™åˆ¶
                if retry_count >= self.worker_settings.max_retries:
                    logger.error(f"ä»»åŠ¡ {job_id} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.worker_settings.max_retries}")
                    await self._send_to_dlq_with_error(message.body, headers, e, job_id)
                    return

                # å‡†å¤‡é‡è¯•ï¼šé€’å¢é‡è¯•è®¡æ•°
                new_retry_count = retry_count + 1
                if job_id and job:
                    # æ›´æ–°ä»»åŠ¡çš„å°è¯•æ¬¡æ•°ï¼ˆç”¨äºä¸‹æ¬¡æ‰§è¡Œï¼‰
                    job.job_try = new_retry_count + 1  # job_try ä»1å¼€å§‹ï¼Œè¡¨ç¤ºæ‰§è¡Œæ¬¡æ•°

                # è®¡ç®—é€€é¿æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                delay_seconds = self.worker_settings.retry_backoff * (2 ** retry_count)

                # å‘é€åˆ°å»¶è¿Ÿé˜Ÿåˆ—è¿›è¡Œé‡è¯•
                if job_id and job:
                    await self._send_to_delay_queue(job, delay_seconds, new_retry_count)
                    logger.debug(
                        f"ä»»åŠ¡ {job_id} ç¬¬ {new_retry_count} æ¬¡é‡è¯•ï¼Œå»¶è¿Ÿ {delay_seconds:.1f} ç§’ (é”™è¯¯ç±»å‹: {type(e).__name__}, åˆ†ç±»: {error_category})")

            finally:
                # ä»ä»»åŠ¡åˆ—è¡¨ä¸­ç§»é™¤
                if job_id and job_id in self.tasks:
                    del self.tasks[job_id]

    async def _execute_job(self, job: JobModel) -> Any:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        """
        job.start_time = datetime.now(timezone.utc)
        job.status = JobStatus.IN_PROGRESS
        self.worker_info.jobs_ongoing = len(self.tasks)

        # æ„å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
        job_ctx = JobContext(
            job_id=job.job_id,
            job_try=job.job_try,
            enqueue_time=job.enqueue_time,
            start_time=job.start_time,
            queue_name=job.queue_name,
            worker_id=self.worker_id,
            extra=self.ctx
        )

        try:
            # è°ƒç”¨ on_job_start é’©å­
            if self.on_job_start:
                # ä¼ é€’ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œ Worker ç»Ÿè®¡ä¿¡æ¯
                hook_ctx = job_ctx.model_dump()
                hook_ctx['worker_stats'] = {
                    'jobs_complete': self.jobs_complete,
                    'jobs_failed': self.jobs_failed,
                    'jobs_retried': self.jobs_retried,
                    'jobs_ongoing': len(self.tasks)
                }
                await self.on_job_start(hook_ctx)

            # è·å–è¦æ‰§è¡Œçš„å‡½æ•°
            func = self.functions.get(job.function)  # type: WorkerCoroutine
            if not func:
                logger.error(f"æœªæ‰¾åˆ°å‡½æ•°: {job.function}")
                logger.error(f"å¯ç”¨å‡½æ•°åˆ—è¡¨: {list(self.functions.keys())}")
                raise ValueError(f"æœªæ‰¾åˆ°å‡½æ•°: {job.function}")

            # åœ¨è°ƒç”¨ä»»åŠ¡å‡½æ•°å‰ï¼ŒåŸºäºå‡½æ•°çš„ç±»å‹æ³¨è§£å°† JSON ååºåˆ—åŒ–åçš„ dict/list
            # è‡ªåŠ¨é‡å»ºä¸º Pydantic æ¨¡å‹æˆ–ç›¸åº”å®¹å™¨ç±»å‹ï¼ˆä¸ä¿®æ”¹åŸå§‹ job.args/kwargsï¼‰
            coerced_args, coerced_kwargs = self._coerce_task_args(func, job.args, job.kwargs)

            # æ‰§è¡Œå‡½æ•°ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
            logger.debug(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡ {job.job_id} - {job.function}")

            if asyncio.iscoroutinefunction(func):
                result = await asyncio.wait_for(
                    func(job_ctx, *coerced_args, **coerced_kwargs),
                    timeout=self.worker_settings.job_timeout
                )
            else:
                # åŒæ­¥å‡½æ•°åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
                loop = asyncio.get_running_loop()
                result = await asyncio.wait_for(
                    loop.run_in_executor(None, partial(func, job_ctx, *coerced_args, **coerced_kwargs)),
                    timeout=self.worker_settings.job_timeout
                )

            # ä»»åŠ¡æˆåŠŸå®Œæˆ
            job.status = JobStatus.COMPLETED
            job.result = result
            job.end_time = datetime.now(timezone.utc)
            self.jobs_complete += 1

            logger.info(f"ä»»åŠ¡ {job.job_id} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶ {(job.end_time - job.start_time).total_seconds():.2f} ç§’")

            # å­˜å‚¨ä»»åŠ¡ç»“æœ
            await self._store_job_result(job)

            # æ— éœ€æ›´æ–°å…¨å±€ç»Ÿè®¡ï¼Œå°†é€šè¿‡é’©å­ä¼ é€’

        except asyncio.TimeoutError:
            job.status = JobStatus.FAILED
            job.error = f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ ({self.worker_settings.job_timeout}ç§’)"
            job.end_time = datetime.now(timezone.utc)
            self.jobs_failed += 1
            logger.error(f"ä»»åŠ¡ {job.job_id} æ‰§è¡Œè¶…æ—¶")
            # å­˜å‚¨å¤±è´¥ç»“æœ
            await self._store_job_result(job)
            raise JobTimeout(job.error)

        except Retry as e:
            job.status = JobStatus.RETRYING
            job.error = str(e)
            self.jobs_retried += 1
            logger.warning(f"ä»»åŠ¡ {job.job_id} è¯·æ±‚é‡è¯•: {e}")

            # æ— éœ€æ›´æ–°å…¨å±€ç»Ÿè®¡ï¼Œå°†é€šè¿‡é’©å­ä¼ é€’

            # åœ¨é‡è¯•å‰æ£€æŸ¥æ¬¡æ•°ï¼ˆé¿å…åœ¨ _enqueue_job_retry ä¸­æŠ›å‡ºå¼‚å¸¸ï¼‰
            current_retry_count = job.job_try - 1  # job_try ä»1å¼€å§‹ï¼Œè¡¨ç¤ºå½“å‰æ˜¯ç¬¬å‡ æ¬¡æ‰§è¡Œ
            if current_retry_count >= self.worker_settings.max_retries:
                logger.error(f"ä»»åŠ¡ {job.job_id} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {self.worker_settings.max_retries}ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—")
                job.status = JobStatus.FAILED
                job.error = f"ä»»åŠ¡è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° {self.worker_settings.max_retries}"
                job.end_time = datetime.now(timezone.utc)
                # å­˜å‚¨æœ€ç»ˆå¤±è´¥ç»“æœ
                await self._store_job_result(job)
                return  # ç›´æ¥è¿”å›ï¼Œä¸å†é‡è¯•

            # è®¡ç®—é‡è¯•å»¶è¿Ÿ
            if e.defer:
                if isinstance(e.defer, timedelta):
                    defer_seconds = e.defer.total_seconds()
                else:
                    defer_seconds = float(e.defer)
            else:
                # æŒ‡æ•°é€€é¿ï¼ˆåŸºäºå½“å‰é‡è¯•æ¬¡æ•°ï¼‰
                defer_seconds = self.worker_settings.retry_backoff * (2 ** current_retry_count)

            # é‡æ–°å…¥é˜Ÿå‰é€’å¢é‡è¯•è®¡æ•°
            job.job_try += 1
            await self._enqueue_job_retry(job, defer_seconds)

        except Exception as e:
            job.status = JobStatus.FAILED
            job.error = f"{type(e).__name__}: {str(e)}"
            job.end_time = datetime.now(timezone.utc)
            self.jobs_failed += 1
            logger.error(f"ä»»åŠ¡ {job.job_id} æ‰§è¡Œå¤±è´¥: {job.error}\n{traceback.format_exc()}")

            # å­˜å‚¨å¤±è´¥ç»“æœ
            await self._store_job_result(job)

            # æ— éœ€æ›´æ–°å…¨å±€ç»Ÿè®¡ï¼Œå°†é€šè¿‡é’©å­ä¼ é€’

            raise

        finally:
            job.end_time = datetime.now(timezone.utc)

            # è°ƒç”¨ on_job_end é’©å­
            if self.on_job_end:
                # ä¼ é€’ä»»åŠ¡ä¸Šä¸‹æ–‡å’Œæ›´æ–°åçš„ Worker ç»Ÿè®¡ä¿¡æ¯
                hook_ctx = job_ctx.model_dump()
                hook_ctx['worker_stats'] = {
                    'jobs_complete': self.jobs_complete,
                    'jobs_failed': self.jobs_failed,
                    'jobs_retried': self.jobs_retried,
                    'jobs_ongoing': len(self.tasks)
                }
                # åŒæ­¥ç»Ÿè®¡æ•°æ®åˆ°å…¨å±€ ctxï¼ˆç”¨äºå…³é—­é’©å­ï¼‰
                self.ctx['jobs_complete'] = self.jobs_complete
                self.ctx['jobs_failed'] = self.jobs_failed
                self.ctx['jobs_retried'] = self.jobs_retried
                self.ctx['jobs_ongoing'] = len(self.tasks)

                await self.on_job_end(hook_ctx)

            # è°ƒç”¨ after_job_end é’©å­
            if self.after_job_end:
                hook_ctx = job_ctx.model_dump()
                hook_ctx['worker_stats'] = {
                    'jobs_complete': self.jobs_complete,
                    'jobs_failed': self.jobs_failed,
                    'jobs_retried': self.jobs_retried,
                    'jobs_ongoing': len(self.tasks)
                }
                await self.after_job_end(hook_ctx)

            # æ›´æ–° Worker ä¿¡æ¯
            self.worker_info.jobs_complete = self.jobs_complete
            self.worker_info.jobs_failed = self.jobs_failed
            self.worker_info.jobs_retried = self.jobs_retried
            self.worker_info.jobs_ongoing = len(self.tasks)

    def _coerce_task_args(self, func: WorkerCoroutine, args: list[Any], kwargs: dict[str, Any]) -> tuple[list[Any], dict[str, Any]]:
        """
        åŸºäºä»»åŠ¡å‡½æ•°çš„ç±»å‹æ³¨è§£ï¼Œå°† JSON ååºåˆ—åŒ–åçš„å‚æ•°æ¢å¤ä¸º Pydantic æ¨¡å‹æˆ–å®¹å™¨ç±»å‹ã€‚
        ä»…ç”¨äºè°ƒç”¨æ—¶çš„å‚æ•°è½¬æ¢ï¼Œä¸ä¿®æ”¹åŸå§‹ job.args/kwargsã€‚

        Args:
            func: ä»»åŠ¡å‡½æ•°
            args: ä½ç½®å‚æ•°åˆ—è¡¨ï¼ˆä¸åŒ…å« ctxï¼‰
            kwargs: å…³é”®å­—å‚æ•°å­—å…¸

        Returns:
            (coerced_args, coerced_kwargs): è½¬æ¢åçš„å‚æ•°
        """
        try:
            # æå–ç±»å‹æ³¨è§£ï¼ˆè§£æå‰å‘å¼•ç”¨ã€è·¨æ¨¡å—ï¼‰
            type_hints = get_type_hints(func, globalns=getattr(func, "__globals__", None))
        except Exception:
            type_hints = {}

        try:
            params = list(inspect.signature(func).parameters.values())
        except Exception:
            params = []

        # è·³è¿‡ç¬¬ä¸€ä¸ª ctx å‚æ•°ï¼Œæ„é€ ä½ç½®å‚æ•°ååºåˆ—
        positional_param_names: list[str] = [p.name for p in params[1:]] if params else []

        # å¤„ç†ä½ç½®å‚æ•°
        coerced_args: list[Any] = []
        for idx, value in enumerate(args):
            if idx < len(positional_param_names):
                name = positional_param_names[idx]
                annotation = type_hints.get(name, params[idx + 1].annotation if params else inspect._empty)
            else:
                annotation = inspect._empty

            coerced_args.append(self._coerce_single_value(value, annotation))

        # å¤„ç†å…³é”®å­—å‚æ•°
        coerced_kwargs: dict[str, Any] = {}
        for k, v in kwargs.items():
            annotation = type_hints.get(k, inspect._empty)
            coerced_kwargs[k] = self._coerce_single_value(v, annotation)

        return coerced_args, coerced_kwargs

    @staticmethod
    def _get_type_adapter(annotation: Any) -> TypeAdapter | None:
        """è·å–æˆ–åˆ›å»ºæ³¨è§£å¯¹åº”çš„ TypeAdapterï¼Œå¸¦æœ¬åœ°ç¼“å­˜ã€‚"""
        try:
            key = repr(annotation)
            adapter = _TYPE_ADAPTER_CACHE.get(key)
            if adapter is None:
                adapter = TypeAdapter(annotation)
                _TYPE_ADAPTER_CACHE[key] = adapter
            return adapter
        except Exception:
            return None

    @staticmethod
    def _coerce_single_value(value: Any, annotation: Any) -> Any:
        """
        ä½¿ç”¨ Pydantic TypeAdapter æŒ‰æ³¨è§£å°†å€¼è½¬æ¢ä¸ºç›®æ ‡ç±»å‹ï¼›å¤±è´¥åˆ™åŸæ ·è¿”å›ã€‚
        æ”¯æŒ BaseModel ä»¥åŠ list[Model]ã€dict[str, Model]ã€Optional[Model] ç­‰å®¹å™¨/è”åˆç±»å‹ã€‚
        """
        if annotation in (None, inspect._empty) or annotation is Any:
            return value
        try:
            adapter = Worker._get_type_adapter(annotation)
            return adapter.validate_python(value) if adapter else value
        except Exception:
            return value

    async def _enqueue_job_retry(self, job: JobModel, defer_seconds: float) -> None:
        """
        é‡æ–°å…¥é˜Ÿä»»åŠ¡è¿›è¡Œé‡è¯•ï¼Œä½¿ç”¨å»¶è¿Ÿé˜Ÿåˆ—
        
        Args:
            job: ä»»åŠ¡æ¨¡å‹ï¼ˆåŒ…å«æ­£ç¡®çš„ job_try è®¡æ•°ï¼‰
            defer_seconds: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        # æ£€æŸ¥é‡è¯•æ¬¡æ•°ï¼ˆjob_try å·²ç»åœ¨è°ƒç”¨å‰æ­£ç¡®è®¾ç½®ï¼‰
        retry_count = job.job_try - 1
        if retry_count >= self.worker_settings.max_retries:
            logger.error(f"ä»»åŠ¡ {job.job_id} é‡è¯•æ¬¡æ•° {retry_count} å·²è¶…è¿‡æœ€å¤§é™åˆ¶ {self.worker_settings.max_retries}")
            raise MaxRetriesExceeded(max_retries=self.worker_settings.max_retries, job_id=job.job_id)

        # ä½¿ç”¨å»¶è¿Ÿé˜Ÿåˆ—è¿›è¡Œé‡è¯•
        await self._send_to_delay_queue(job, defer_seconds)

        logger.debug(f"ä»»åŠ¡ {job.job_id} å·²å‘é€åˆ°å»¶è¿Ÿé˜Ÿåˆ—è¿›è¡Œé‡è¯•ï¼Œå°†åœ¨ {defer_seconds:.1f} ç§’åæ‰§è¡Œ (é‡è¯•æ¬¡æ•°: {retry_count})")

    async def _send_to_dlq(self, body: bytes, headers: dict[str, Any]) -> None:
        """
        å°†æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
        """
        await self.dlq_channel.default_exchange.publish(
            Message(body=body, headers=headers),
            routing_key=self.rabbitmq_dlq
        )

    async def _send_to_dlq_with_error(self, body: bytes, headers: dict[str, Any], error: Exception, job_id: str | None = None) -> None:
        """
        å°†æ¶ˆæ¯è¿åŒé”™è¯¯ä¿¡æ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
        
        Args:
            body: æ¶ˆæ¯ä½“
            headers: æ¶ˆæ¯å¤´
            error: å¼‚å¸¸å¯¹è±¡
            job_id: ä»»åŠ¡ID
        """
        # å¢å¼ºé”™è¯¯ä¿¡æ¯
        error_headers = headers.copy()
        error_headers.update({
            'x-error-type': type(error).__name__,
            'x-error-message': str(error),
            'x-error-category': ErrorClassification.get_error_category(error),
            'x-failed-at': datetime.now(timezone.utc).isoformat(),
            'x-job-id': job_id or 'unknown'
        })

        logger.error(f"ä»»åŠ¡ {job_id} å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: {type(error).__name__}: {error}")

        await self.dlq_channel.default_exchange.publish(
            Message(body=body, headers=error_headers),
            routing_key=self.rabbitmq_dlq
        )

    async def _send_to_delay_queue(self, job: JobModel, delay_seconds: float, retry_count: int | None = None) -> None:
        """
        å°†ä»»åŠ¡å‘é€åˆ°å»¶è¿Ÿé˜Ÿåˆ—ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³å»¶è¿Ÿæœºåˆ¶
        
        Args:
            job: ä»»åŠ¡æ¨¡å‹
            delay_seconds: å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            retry_count: é‡è¯•è®¡æ•°ï¼ˆå¦‚æœæä¾›ï¼Œåˆ™ä½¿ç”¨æ­¤å€¼ï¼›å¦åˆ™ä» job.job_try è®¡ç®—ï¼‰
        """
        # æ¸…é™¤å»¶è¿Ÿæ—¶é—´ï¼Œé¿å…å¾ªç¯å»¶è¿Ÿ
        job.defer_until = None

        # åºåˆ—åŒ–ä»»åŠ¡
        message_body = json.dumps(job.model_dump(), ensure_ascii=False, default=str).encode()

        # ç¡®å®šé‡è¯•è®¡æ•°
        if retry_count is not None:
            # ä½¿ç”¨æä¾›çš„é‡è¯•è®¡æ•°
            actual_retry_count = retry_count
        else:
            # ä» job_try è®¡ç®—é‡è¯•è®¡æ•°
            actual_retry_count = job.job_try - 1 if job.job_try > 0 else 0
            
        headers = {"x-retry-count": actual_retry_count}

        if self._use_delayed_exchange:
            # ä½¿ç”¨å»¶è¿Ÿæ’ä»¶ï¼ˆæ›´ä¼˜é›…çš„æ–¹æ¡ˆï¼‰
            # å»¶è¿Ÿæ—¶é—´é€šè¿‡ x-delay å¤´è®¾ç½®ï¼ˆæ¯«ç§’ï¼‰
            delay_ms = int(delay_seconds * 1000)
            headers['x-delay'] = delay_ms

            # è·å–å»¶è¿Ÿäº¤æ¢æœº
            delayed_exchange = await self.channel.get_exchange(self._delayed_exchange_name)

            # å‘é€åˆ°å»¶è¿Ÿäº¤æ¢æœº
            await delayed_exchange.publish(
                Message(
                    body=message_body,
                    headers=headers
                ),
                routing_key=self.rabbitmq_queue
            )

            logger.debug(f"ä»»åŠ¡ {job.job_id} å·²é€šè¿‡å»¶è¿Ÿäº¤æ¢æœºå‘é€ï¼Œå°†åœ¨ {delay_seconds:.1f} ç§’åå¤„ç† (é‡è¯•æ¬¡æ•°: {actual_retry_count})")

        else:
            # ä½¿ç”¨ TTL + DLX æ–¹æ¡ˆï¼ˆé™çº§æ–¹æ¡ˆï¼‰
            expiration = timedelta(seconds=delay_seconds)

            # å‘é€åˆ° TTL å»¶è¿Ÿé˜Ÿåˆ—
            await self.channel.default_exchange.publish(
                Message(
                    body=message_body,
                    headers=headers,
                    expiration=expiration  # TTL è®¾ç½®
                ),
                routing_key=self._delay_queue_name
            )

            logger.debug(f"ä»»åŠ¡ {job.job_id} å·²é€šè¿‡ TTL é˜Ÿåˆ—å‘é€ï¼Œå°†åœ¨ {delay_seconds:.1f} ç§’åå¤„ç† (é‡è¯•æ¬¡æ•°: {actual_retry_count})")

    async def _setup_delay_mechanism(self) -> None:
        """
        æ£€æµ‹å¹¶è®¾ç½®å»¶è¿Ÿæœºåˆ¶ï¼šä¼˜å…ˆä½¿ç”¨å»¶è¿Ÿæ’ä»¶ï¼Œå…¶æ¬¡ä½¿ç”¨ TTL + DLX
        ä¸ Client çš„æ£€æµ‹é€»è¾‘ä¿æŒä¸€è‡´
        """
        if self._delay_mechanism_detected:
            return  # å·²æ£€æµ‹è¿‡

        logger.info(f"ğŸ” æ­£åœ¨ä¸ºé˜Ÿåˆ— {self.worker_settings.queue_name} æ£€æµ‹å»¶è¿Ÿæœºåˆ¶...")

        try:
            # å°è¯•å£°æ˜å»¶è¿Ÿäº¤æ¢æœºï¼ˆéœ€è¦ rabbitmq_delayed_message_exchange æ’ä»¶ï¼‰
            delayed_exchange = await self.channel.declare_exchange(
                self._delayed_exchange_name,
                type='x-delayed-message',  # ç‰¹æ®Šçš„å»¶è¿Ÿæ¶ˆæ¯ç±»å‹
                durable=True,
                arguments={
                    'x-delayed-type': 'direct'  # å®é™…çš„è·¯ç”±ç±»å‹
                }
            )

            # ç»‘å®šå»¶è¿Ÿäº¤æ¢æœºåˆ°ä¸»é˜Ÿåˆ—
            queue = await self.channel.get_queue(self.rabbitmq_queue)
            await queue.bind(delayed_exchange, routing_key=self.rabbitmq_queue)

            self._use_delayed_exchange = True
            self._delay_mechanism_detected = True
            logger.info(f"âœ… é˜Ÿåˆ— {self.worker_settings.queue_name} æ£€æµ‹åˆ° RabbitMQ å»¶è¿Ÿæ’ä»¶ï¼Œä½¿ç”¨å»¶è¿Ÿäº¤æ¢æœºæ¨¡å¼")

        except Exception as e:
            # æ’ä»¶æœªå®‰è£…æˆ–å£°æ˜å¤±è´¥ï¼Œé™çº§åˆ° TTL + DLX æ–¹æ¡ˆ
            logger.warning(f"âš ï¸ é˜Ÿåˆ— {self.worker_settings.queue_name} æœªæ£€æµ‹åˆ° RabbitMQ å»¶è¿Ÿæ’ä»¶: {e}")
            logger.warning("ğŸ’¡ æ¨èå®‰è£… rabbitmq_delayed_message_exchange æ’ä»¶ä»¥è·å¾—æ›´å¥½çš„å»¶è¿Ÿé˜Ÿåˆ—æ€§èƒ½")
            logger.warning("   å®‰è£…å‘½ä»¤: rabbitmq-plugins enable rabbitmq_delayed_message_exchange")
            logger.info(f"ğŸ“Œ é˜Ÿåˆ— {self.worker_settings.queue_name} é™çº§ä½¿ç”¨ TTL + Dead Letter Exchange æ–¹æ¡ˆ")

            try:
                # é‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„ Channelï¼ˆå¦‚æœå½“å‰ Channel æœ‰é—®é¢˜ï¼‰
                if self.channel.is_closed:
                    logger.warning("ğŸ”„ å½“å‰ Channel å·²å…³é—­ï¼Œé‡æ–°åˆ›å»º...")
                    self.channel = await self.connection.channel()
                    await self.channel.set_qos(prefetch_count=self.worker_settings.rabbitmq_settings.prefetch_count)

                # å£°æ˜ TTL å»¶è¿Ÿé˜Ÿåˆ—
                await self.channel.declare_queue(
                    self._delay_queue_name,
                    durable=True,
                    arguments={
                        'x-dead-letter-exchange': '',  # é»˜è®¤äº¤æ¢æœº
                        'x-dead-letter-routing-key': self.rabbitmq_queue  # è·¯ç”±åˆ°ä¸»é˜Ÿåˆ—
                    }
                )

                self._use_delayed_exchange = False
                self._delay_mechanism_detected = True
                logger.info(f"âœ… é˜Ÿåˆ— {self.worker_settings.queue_name} TTL + DLX å»¶è¿Ÿæœºåˆ¶è®¾ç½®å®Œæˆ")

            except Exception as dlx_error:
                logger.error(f"âŒ TTL + DLX å»¶è¿Ÿæœºåˆ¶è®¾ç½®å¤±è´¥: {dlx_error}")
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…å¤„ç†
                raise

    async def _health_check_loop(self) -> None:
        """
        å¥åº·æ£€æŸ¥å¾ªç¯
        """
        while self.allow_pick_jobs:
            try:
                self.worker_info.last_health_check = datetime.now(timezone.utc)
                # å¥åº·æ£€æŸ¥ï¼šå¯ä»¥æ‰©å±•æ·»åŠ æ›´å¤šæ£€æŸ¥é€»è¾‘ï¼ˆå¦‚ Redis å¿ƒè·³ç­‰ï¼‰
                logger.debug(f"å¥åº·æ£€æŸ¥ - Worker {self.worker_id} æ­£å¸¸è¿è¡Œ")
                await asyncio.sleep(self.worker_settings.health_check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")

    async def _get_queue_message_count(self) -> int:
        """
        è·å–é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯æ•°é‡
        
        Returns:
            é˜Ÿåˆ—ä¸­å¾…å¤„ç†çš„æ¶ˆæ¯æ•°é‡
        """
        try:
            queue = await self.channel.declare_queue(self.rabbitmq_queue, durable=True, passive=True)
            return queue.declaration_result.message_count
        except Exception as e:
            logger.warning(f"è·å–é˜Ÿåˆ—æ¶ˆæ¯æ•°é‡å¤±è´¥: {e}")
            return 0

    async def _should_exit_burst_mode(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥é€€å‡º burst æ¨¡å¼
        
        Returns:
            True å¦‚æœåº”è¯¥é€€å‡º burst æ¨¡å¼
        """
        if not self._burst_mode:
            return False

        # æ£€æŸ¥æ˜¯å¦å·²æ ‡è®°ä¸ºåº”è¯¥é€€å‡º
        if self._burst_should_exit:
            return True

        # æ£€æŸ¥è¶…æ—¶
        if self._burst_start_time:
            elapsed = (datetime.now(timezone.utc) - self._burst_start_time).total_seconds()
            if elapsed >= self.worker_settings.burst_timeout:
                logger.info(f"ğŸ• Burst æ¨¡å¼è¶…æ—¶ ({elapsed:.1f}s >= {self.worker_settings.burst_timeout}s)ï¼Œå‡†å¤‡é€€å‡º")
                return True

        # æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©ºä¸”æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡
        queue_count = await self._get_queue_message_count()
        running_tasks = len(self.tasks)

        if queue_count == 0 and running_tasks == 0:
            logger.info("ğŸ¯ Burst æ¨¡å¼: é˜Ÿåˆ—ä¸ºç©ºä¸”æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå‡†å¤‡é€€å‡º")
            return True

        # å¦‚æœé…ç½®äº†ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œåªæ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
        if not self.worker_settings.burst_wait_for_tasks and queue_count == 0:
            logger.info("ğŸ¯ Burst æ¨¡å¼: é˜Ÿåˆ—ä¸ºç©ºï¼Œç«‹å³é€€å‡ºï¼ˆä¸ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼‰")
            return True

        logger.debug(f"Burst æ£€æŸ¥: é˜Ÿåˆ—={queue_count}æ¡æ¶ˆæ¯, è¿è¡Œä¸­={running_tasks}ä¸ªä»»åŠ¡")
        return False

    async def _burst_monitor_loop(self) -> None:
        """
        Burst æ¨¡å¼ç›‘æ§å¾ªç¯
        """
        if not self._burst_mode:
            return

        logger.info(f"ğŸš€ å¯åŠ¨ Burst æ¨¡å¼ç›‘æ§ (è¶…æ—¶: {self.worker_settings.burst_timeout}s)")
        self._burst_start_time = datetime.now(timezone.utc)

        while self.allow_pick_jobs and not self._burst_should_exit:
            try:
                if await self._should_exit_burst_mode():
                    logger.info("ğŸ“¤ Burst æ¨¡å¼é€€å‡ºæ¡ä»¶æ»¡è¶³ï¼Œåœæ­¢æ¥æ”¶æ–°ä»»åŠ¡")
                    self.allow_pick_jobs = False
                    self._burst_should_exit = True

                    # å¦‚æœéœ€è¦ç­‰å¾…ä»»åŠ¡å®Œæˆ
                    if self.worker_settings.burst_wait_for_tasks and self.tasks:
                        logger.info(f"â³ ç­‰å¾… {len(self.tasks)} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆ...")
                        await self._sleep_until_tasks_complete()

                    # å–æ¶ˆä¸»ä»»åŠ¡ä»¥è§¦å‘é€€å‡º
                    if self.main_task:
                        self.main_task.cancel()
                    break

                await asyncio.sleep(self.worker_settings.burst_check_interval)

            except asyncio.CancelledError:
                logger.debug("Burst ç›‘æ§å¾ªç¯è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"Burst ç›‘æ§å‡ºé”™: {e}")
                await asyncio.sleep(1)

    async def consume(self) -> None:
        """
        å¼€å§‹æ¶ˆè´¹æ¶ˆæ¯
        """
        # å£°æ˜é˜Ÿåˆ—ï¼ˆBurst å’Œå¸¸è§„æ¨¡å¼éƒ½éœ€è¦ï¼‰
        queue = await self.channel.declare_queue(self.rabbitmq_queue, durable=True)
        self._queue = queue  # ä¿å­˜é˜Ÿåˆ—å¼•ç”¨

        if self._burst_mode:
            # Burst æ¨¡å¼ï¼šæ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
            initial_queue_count = await self._get_queue_message_count()
            if initial_queue_count == 0:
                logger.info("ğŸ¯ Burst æ¨¡å¼: é˜Ÿåˆ—ä¸ºç©ºï¼Œç«‹å³é€€å‡º")
                return

            logger.info(f"ğŸš€ Burst æ¨¡å¼å¯åŠ¨: é˜Ÿåˆ—ä¸­æœ‰ {initial_queue_count} æ¡æ¶ˆæ¯å¾…å¤„ç†")
            # å¯åŠ¨ burst ç›‘æ§
            self._burst_check_task = asyncio.create_task(self._burst_monitor_loop())
        else:
            logger.info(f"[*] ç­‰å¾…é˜Ÿåˆ— {self.rabbitmq_queue} ä¸­çš„æ¶ˆæ¯ã€‚æŒ‰ CTRL+C é€€å‡º")

        # å¼€å§‹å¥åº·æ£€æŸ¥ï¼ˆé burst æ¨¡å¼æˆ–éœ€è¦å¥åº·æ£€æŸ¥çš„ burst æ¨¡å¼ï¼‰
        if not self._burst_mode or self.worker_settings.health_check_interval > 0:
            self._health_check_task = asyncio.create_task(self._health_check_loop())

        # å¼€å§‹æ¶ˆè´¹æ¶ˆæ¯ï¼ˆBurst å’Œå¸¸è§„æ¨¡å¼éƒ½éœ€è¦ï¼‰
        consumer_tag = await queue.consume(lambda message: asyncio.create_task(self.on_message(message)))
        self._consumer_tag = consumer_tag  # ä¿å­˜æ¶ˆè´¹è€…æ ‡ç­¾
        logger.debug(f"ğŸ”§ æ¶ˆæ¯æ¶ˆè´¹å™¨å·²å¯åŠ¨ï¼Œconsumer_tag: {consumer_tag}")

        try:
            # ç­‰å¾…å…³é—­ä¿¡å·æˆ–è¢«å–æ¶ˆ
            await self.shutdown_event.wait()
            logger.info("ğŸ›‘ æ”¶åˆ°å…³é—­ä¿¡å·ï¼Œå‡†å¤‡é€€å‡ºæ¶ˆè´¹å¾ªç¯")
        except asyncio.CancelledError:
            if self._burst_mode:
                logger.info("ğŸ Burst æ¨¡å¼æ¶ˆè´¹è€…è¢«å–æ¶ˆ")
            else:
                logger.info("ğŸ›‘ å¸¸è§„æ¨¡å¼æ¶ˆè´¹è€…è¢«å–æ¶ˆ")
            raise
        finally:
            # å…³é”®æ”¹è¿›ï¼šåœæ­¢æ¶ˆæ¯æ¶ˆè´¹å™¨
            if self._consumer_tag and not self.channel.is_closed:
                try:
                    logger.info("ğŸ”§ æ­£åœ¨åœæ­¢æ¶ˆæ¯æ¶ˆè´¹å™¨...")
                    await self._safe_operation_with_timeout(
                        queue.cancel(self._consumer_tag),
                        "æ¶ˆæ¯æ¶ˆè´¹å™¨åœæ­¢",
                        timeout=5.0
                    )
                    logger.info("âœ… æ¶ˆæ¯æ¶ˆè´¹å™¨å·²åœæ­¢")
                    self._consumer_tag = None
                except Exception as e:
                    logger.warning(f"âš ï¸ åœæ­¢æ¶ˆè´¹å™¨æ—¶å‡ºç°é”™è¯¯: {e}")

            # æ¸…ç†åå°ä»»åŠ¡
            if self._health_check_task:
                logger.debug("ğŸ”§ å–æ¶ˆå¥åº·æ£€æŸ¥ä»»åŠ¡...")
                self._health_check_task.cancel()
                try:
                    await self._health_check_task
                except asyncio.CancelledError:
                    pass

            if self._burst_check_task:
                logger.debug("ğŸ”§ å–æ¶ˆ Burst æ£€æŸ¥ä»»åŠ¡...")
                self._burst_check_task.cancel()
                try:
                    await self._burst_check_task
                except asyncio.CancelledError:
                    pass

            logger.info("âœ… æ¶ˆè´¹å¾ªç¯æ¸…ç†å®Œæˆ")

    async def main(self) -> None:
        """
        Worker ä¸»å‡½æ•°
        """
        start_time = datetime.now(timezone.utc)

        try:

            # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼ˆäº‹ä»¶å¾ªç¯å·²ç»åœ¨è¿è¡Œï¼‰
            self._setup_signal_handlers()

            # Burst æ¨¡å¼å¯åŠ¨ä¿¡æ¯
            if self._burst_mode:
                logger.info(f"ğŸš€ å¯åŠ¨ Burst æ¨¡å¼ Worker (è¶…æ—¶: {self.worker_settings.burst_timeout}s)")
            else:
                logger.info("ğŸš€ å¯åŠ¨å¸¸è§„æ¨¡å¼ Worker")
            # åˆå§‹åŒ–è¿æ¥
            await self._init()

            # éªŒè¯ç»“æœå­˜å‚¨è¿æ¥ï¼ˆå¯é…ç½®é™çº§æˆ–è·³è¿‡ï¼‰
            if getattr(self.worker_settings, 'enable_job_result_storage', True):
                await self._validate_result_store()

            # å¯åŠ¨é’©å­
            if self.on_startup:
                logger.info("æ‰§è¡Œå¯åŠ¨é’©å­")
                await self.on_startup(self.ctx)

            # è®°å½•ä¸»ä»»åŠ¡
            self.main_task = asyncio.current_task()

            # å¼€å§‹æ¶ˆè´¹
            await self.consume()

        except KeyboardInterrupt:
            logger.info("ğŸ›‘ æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å· (SIGINT)ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        except asyncio.CancelledError:
            if self._burst_mode:
                # è®¡ç®—è¿è¡Œæ—¶é—´å’Œç»Ÿè®¡ä¿¡æ¯
                elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
                logger.info(f"ğŸ Burst æ¨¡å¼æ­£å¸¸ç»“æŸ (è¿è¡Œæ—¶é—´: {elapsed:.1f}s)")
                logger.info(f"ğŸ“Š ä»»åŠ¡ç»Ÿè®¡: å®Œæˆ {self.jobs_complete} ä¸ª, "
                            f"å¤±è´¥ {self.jobs_failed} ä¸ª, "
                            f"é‡è¯• {self.jobs_retried} ä¸ª")
            else:
                logger.info("ğŸ›‘ Worker æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        except SystemExit as e:
            logger.info(f"ğŸ›‘ ç³»ç»Ÿé€€å‡ºä¿¡å·: {e}")
        except Exception as e:
            logger.error(f"âŒ Worker è¿è¡Œå‡ºé”™: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œç»™å‡ºå»ºè®®
            if "connection" in str(e).lower() or "rabbitmq" in str(e).lower():
                logger.error("ğŸ’¡ è¯·æ£€æŸ¥:")
                logger.error("   1. RabbitMQ æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ")
                logger.error("   2. è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®")
                logger.error("   3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            raise
        finally:
            # ç­‰å¾…æœ€åçš„ä»»åŠ¡å®Œæˆï¼ˆå¦‚æœåœ¨ burst æ¨¡å¼ä¸”é…ç½®äº†ç­‰å¾…ï¼‰
            if self._burst_mode and self.worker_settings.burst_wait_for_tasks and self.tasks:
                logger.info(f"â³ ç­‰å¾…æœ€å {len(self.tasks)} ä¸ªä»»åŠ¡å®Œæˆ...")
                try:
                    await asyncio.wait_for(
                        self._sleep_until_tasks_complete(),
                        timeout=30  # æœ€å¤šç­‰å¾…30ç§’
                    )
                except asyncio.TimeoutError:
                    logger.warning("ç­‰å¾…ä»»åŠ¡å®Œæˆè¶…æ—¶ï¼Œå¼ºåˆ¶é€€å‡º")

            # å…³é—­é’©å­ - ä½¿ç”¨è¶…æ—¶ä¿æŠ¤
            if self.on_shutdown:
                logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œå…³é—­é’©å­...")
                # æœ€ç»ˆåŒæ­¥ç»Ÿè®¡æ•°æ®
                self.ctx['jobs_complete'] = self.jobs_complete
                self.ctx['jobs_failed'] = self.jobs_failed
                self.ctx['jobs_retried'] = self.jobs_retried
                self.ctx['jobs_ongoing'] = len(self.tasks)

                # ä½¿ç”¨è¶…æ—¶ä¿æŠ¤æ‰§è¡Œå…³é—­é’©å­
                await self._safe_operation_with_timeout(
                    self.on_shutdown(self.ctx),
                    "å…³é—­é’©å­ (on_shutdown)",
                    timeout=30.0
                )
                logger.info("âœ… å…³é—­é’©å­æ‰§è¡Œå®Œæˆ")

            # å…³é—­è¿æ¥ - ä½¿ç”¨è¶…æ—¶ä¿æŠ¤
            if self.connection and not self.connection.is_closed:
                logger.info("ğŸ”§ å¼€å§‹å…³é—­ RabbitMQ è¿æ¥...")
                await self._safe_operation_with_timeout(
                    self.connection.close(),
                    "RabbitMQ è¿æ¥å…³é—­",
                    timeout=10.0
                )
                logger.info("âœ… RabbitMQ è¿æ¥å·²å…³é—­")
            elif self.connection and self.connection.is_closed:
                logger.info("â„¹ï¸ RabbitMQ è¿æ¥å·²ç»å…³é—­")

    async def graceful_shutdown(self, reason: str = "ç”¨æˆ·è¯·æ±‚") -> None:
        """
        Workerçš„ä¼˜é›…å…³é—­æ–¹æ³• - åŒ…å«ç»“æœå­˜å‚¨å¤„ç†
        
        é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œå¢åŠ ç»“æœå­˜å‚¨çš„å…³é—­å¤„ç†
        
        Args:
            reason: å…³é—­åŸå› ï¼Œç”¨äºæ—¥å¿—è®°å½•
        """
        running_tasks = len(self.tasks)
        logger.info(
            f'ğŸ”„ å¼€å§‹ä¼˜é›…å…³é—­ Worker - åŸå› : {reason}'
            f' - ç»Ÿè®¡ä¿¡æ¯: âœ…å®Œæˆ:{self.jobs_complete} âŒå¤±è´¥:{self.jobs_failed} '
            f'ğŸ”„é‡è¯•:{self.jobs_retried} â³è¿è¡Œä¸­:{running_tasks}'
        )

        # åœæ­¢æ¥æ”¶æ–°ä»»åŠ¡
        self.allow_pick_jobs = False

        # ç«‹å³å–æ¶ˆæ¶ˆè´¹è€…ï¼Œåœæ­¢æ¥æ”¶æ–°æ¶ˆæ¯
        await self._cancel_consumer()

        # å¦‚æœæœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œç­‰å¾…å®ƒä»¬å®Œæˆ
        if running_tasks > 0:
            # ç»Ÿä¸€çš„è¶…æ—¶æ—¶é—´è·å–é€»è¾‘
            timeout = (getattr(self.worker_settings, 'wait_for_job_completion_on_signal_second', None)
                       if self.worker_settings else None) or 30
            logger.info(f'â³ ç­‰å¾… {running_tasks} ä¸ªæ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰')

            try:
                await asyncio.wait_for(
                    self._sleep_until_tasks_complete(),
                    timeout=timeout
                )
                logger.info('âœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼Œå¼€å§‹å…³é—­è¿æ¥')
            except asyncio.TimeoutError:
                remaining = len(self.tasks)
                logger.warning(f'â° ç­‰å¾…è¶…æ—¶ï¼Œå¼ºåˆ¶å–æ¶ˆ {remaining} ä¸ªæœªå®Œæˆçš„ä»»åŠ¡')
                for t in self.tasks.values():
                    if not t.done():
                        t.cancel()

        # å…³é—­ç»“æœå­˜å‚¨è¿æ¥
        if self.result_store:
            try:
                await self.result_store.close()
                logger.info('âœ… ç»“æœå­˜å‚¨è¿æ¥å·²å…³é—­')
            except Exception as e:
                logger.warning(f'âš ï¸ å…³é—­ç»“æœå­˜å‚¨æ—¶å‡ºé”™: {e}')

        # å…³é—­è¿æ¥ - ä½¿ç”¨è¶…æ—¶ä¿æŠ¤
        if self.connection and not self.connection.is_closed:
            await self._safe_operation_with_timeout(
                self.connection.close(),
                "RabbitMQ è¿æ¥å…³é—­ (graceful_shutdown)",
                timeout=10.0
            )

        # è®¾ç½®å…³é—­äº‹ä»¶
        self.shutdown_event.set()
        logger.info('âœ… Worker ä¼˜é›…å…³é—­å®Œæˆ')

    @classmethod
    def run(cls, worker_settings: WorkerSettings) -> None:
        """
        åŒæ­¥å¯åŠ¨ Workerï¼Œä»…æ¥å— WorkerSettings å®ä¾‹ã€‚

        Args:
            worker_settings: Worker é…ç½®å¯¹è±¡ï¼ˆå¿…é¡»ä¸º WorkerSettings å®ä¾‹ï¼‰
        """
        if not isinstance(worker_settings, WorkerSettings):
            raise TypeError("Worker.run ä»…æ¥å— WorkerSettings å®ä¾‹")

        worker = cls(worker_settings)
        asyncio.run(worker.main())

from ..types import _axis_type, _device_type, _dtype_type, _scalar_type, _shape_type, _size_type
from .backprop_utils import AutoGradNode as AutoGradNode, backprop as backprop
from .utils import maybe_wrap_dim as maybe_wrap_dim, maybe_wrap_dim_unsqueeze as maybe_wrap_dim_unsqueeze, slice_none_Placeholder as slice_none_Placeholder
from _typeshed import Incomplete
from collections import OrderedDict as OrderedDict
from pyvqnet.device import DEV_CPU as DEV_CPU, DEV_GPU_0 as DEV_GPU_0, get_readable_device_str as get_readable_device_str
from pyvqnet.dtype import dtype_map as dtype_map, get_default_dtype as get_default_dtype, get_readable_dtype_str as get_readable_dtype_str, kcomplex128 as kcomplex128, kcomplex32 as kcomplex32, kcomplex64 as kcomplex64, kfloat16 as kfloat16, kfloat32 as kfloat32, kfloat64 as kfloat64, vqnet_complex_dtypes as vqnet_complex_dtypes, vqnet_float_dtypes as vqnet_float_dtypes

long = int
integer_types: Incomplete
vqnet_core: Incomplete
numeric_types: Incomplete
MIN_FLOAT: Incomplete
MAX_FLOAT: Incomplete
if_grad_enabled: int

class VQNet_Native_Impl_Backend:
    vqnet_complex_dtypes: Incomplete
    vqnet_float_dtypes: Incomplete
    @classmethod
    def op_class_dict(cls): ...
    @staticmethod
    def create_new_qtensor(cls, t): ...
    @staticmethod
    def inplace_requires_grad(self): ...
    @staticmethod
    def toCPU(self): ...
    to_cpu = toCPU
    @staticmethod
    def GPU(self, device=...): ...
    @staticmethod
    def CPU(self): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def convert_to_device_dtype(t, device, dtype): ...
    @staticmethod
    def rmsprop(params, grads, E, state_steps, lr, beta, epsilon) -> None: ...
    @staticmethod
    def sgd(params, grads, ms, lr, momentum, nesterov) -> None: ...
    @staticmethod
    def adamax(params, grads, ms, vs, state_steps, lr, beta1, beta2, epsilon) -> None: ...
    @staticmethod
    def adamax_init_params(self, group, params_with_grad, grads, ms, vs, state_steps) -> None: ...
    @staticmethod
    def adagrad(params, grads, G, state_steps, lr, epsilon) -> None: ...
    @staticmethod
    def adagrad_init_params(self, group, params_with_grad, grads, G, state_steps) -> None: ...
    @staticmethod
    def adadelta(params, grads, E, A, state_steps, lr, beta, epsilon) -> None: ...
    @staticmethod
    def adadelta_init_params(self, group, params_with_grad, grads, E, A, state_steps) -> None: ...
    @staticmethod
    def adam_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
    @staticmethod
    def sgd_init_params(self, group, params_with_grad, grads, ms, state_steps) -> None: ...
    @staticmethod
    def rmsprop_init_params(self, group, params_with_grad, grads, E, state_steps) -> None: ...
    @staticmethod
    def adamw_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
    @staticmethod
    def adamw(params, grads, ms, vs, max_vcaps, state_steps, lr, beta1, beta2, epsilon, weight_decay, amsgrad) -> None: ...
    @staticmethod
    def adam(params, grads, ms, vs, max_vcaps, state_steps, weight_decay, lr, beta1, beta2, epsilon, amsgrad) -> None: ...
    @staticmethod
    def item(self_tensor): ...
    @staticmethod
    def getdata(self_tensor): ...
    @staticmethod
    def add_(self_t, t): ...
    @staticmethod
    def sub_(self_t, t): ...
    @staticmethod
    def triu(t, diagonal: int = 0): ...
    @staticmethod
    def tril(t, diagonal: int = 0): ...
    @staticmethod
    def trace(t, k: int = 0): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def dense_to_csr(x): ...
    @staticmethod
    def csr_to_dense(x): ...
    @staticmethod
    def as_qtensor(x): ...
    @staticmethod
    def set_random_seed(seed) -> None: ...
    @staticmethod
    def get_random_seed(): ...
    @staticmethod
    def log(t): ...
    @staticmethod
    def frobenius_norm(t, axis: int = None, keepdims: bool = False): ...
    @staticmethod
    def randn(shape: _size_type, mean: float | int = 0.0, std: float | int = 1.0, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def randu(shape: _size_type, min: float | int = 0.0, max: float | int = 1.0, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def default_if_grad_enabled():
        """
        get default pyvqnet if_grad_enabled
        """
    get_grad_enabled = default_if_grad_enabled
    @staticmethod
    def set_default_if_grad_enabled(flag) -> None:
        """
        set default pyvqnet value of if_grad_enabled
        """
    set_grad_enabled = set_default_if_grad_enabled
    @staticmethod
    def diagonal(t, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def square_sum(a, b): ...
    @staticmethod
    def log_softmax(t, axis: int = -1): ...
    @staticmethod
    def concatenate(args, axis: int = 0): ...
    cat = concatenate
    @staticmethod
    def flatten(t, start: int = 0, end: int = -1): ...
    @staticmethod
    def stack(tensors, axis: int = 0): ...
    @staticmethod
    def tile(t, reps: list[int] | tuple[int, ...]): ...
    @staticmethod
    def grad_setter(t, new_value) -> None: ...
    @staticmethod
    def less_equal(t1, t2): ...
    @staticmethod
    def less(t1, t2): ...
    @staticmethod
    def logical_not(t): ...
    @staticmethod
    def logical_and(t1, t2): ...
    @staticmethod
    def logical_or(t1, t2): ...
    @staticmethod
    def bitwise_and(t1, t2): ...
    @staticmethod
    def logical_xor(t1, t2): ...
    @staticmethod
    def greater(t1, t2): ...
    @staticmethod
    def greater_equal(t1, t2): ...
    @staticmethod
    def conj(t): ...
    @staticmethod
    def adjoint(t): ...
    @staticmethod
    def is_nonzero(t): ...
    @staticmethod
    def nonzero(t): ...
    @staticmethod
    def isfinite(t): ...
    @staticmethod
    def isinf(t): ...
    @staticmethod
    def isnan(t): ...
    @staticmethod
    def isneginf(t): ...
    @staticmethod
    def isposinf(t): ...
    @staticmethod
    def masked_fill(t, mask, value: float | int): ...
    @staticmethod
    def fill_(t, v: _scalar_type): ...
    @staticmethod
    def all(t): ...
    @staticmethod
    def any(t): ...
    @staticmethod
    def reshape_(self_tensor, new_shape: _size_type): ...
    @staticmethod
    def argmax(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def argmin(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def max(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False): ...
    @staticmethod
    def min(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False): ...
    @staticmethod
    def median(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def csr_members(data): ...
    @staticmethod
    def mean(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def var(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def std(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def broadcast_to(t, ref: _size_type): ...
    @staticmethod
    def equal(t1, t2): ...
    @staticmethod
    def maximum(t1, t2): ...
    @staticmethod
    def minimum(t1, t2): ...
    @staticmethod
    def where(condition, t1, t2): ...
    @staticmethod
    def view_as_complex(t): ...
    @staticmethod
    def reciprocal(t): ...
    @staticmethod
    def round(t): ...
    @staticmethod
    def sign(t): ...
    @staticmethod
    def eigh(t): ...
    @staticmethod
    def eigvalsh(t): ...
    @staticmethod
    def ry_matrix(params): ...
    @staticmethod
    def crz_matrix(params): ...
    @staticmethod
    def adaptive_avg_pool2d(t, size): ...
    @staticmethod
    def functional_conv2d(x, weight, bias, stride, padding, dilation, groups): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def neg(t): ...
    @staticmethod
    def real(t): ...
    @staticmethod
    def imag(t): ...
    @staticmethod
    def full(shape: _size_type, value: float | int, device=..., dtype=None): ...
    @staticmethod
    def squeeze(t, axis: list[int] | tuple[int, ...] | int | None = None):
        """
        Remove axes of length one .if `axis` is not specified, remove all single-dimensional axis from the shape of a tensor. 

        :param t: input QTensor
        :param axis: squeeze axis
        :return: A QTensor

        Examples::

            a = np.arange(6).reshape(1,6,1).astype(np.float32)
            A = QTensor(a)
            AA = tensor.squeeze(A,0)

        """
    @staticmethod
    def not_equal(t1, t2): ...
    @staticmethod
    def pow_scalar(t1, scalar_exp): ...
    @staticmethod
    def power(t1, t2): ...
    pow = power
    @staticmethod
    def floor(t): ...
    @staticmethod
    def ceil(t): ...
    @staticmethod
    def pack_padded_sequence(input, lengths: list[int], batch_first: bool = False, enforce_sorted: bool = True): ...
    @staticmethod
    def pad_packed_sequence(data, batch_sizes, sort_indice, unsorted_indices, batch_first, padding_value, total_length): ...
    @staticmethod
    def zeros_like(t, device=None, dtype=None): ...
    @staticmethod
    def sub(t1, t2):
        """
        Element-wise subtracts two tensors.

        :param t1: 'QTensor' - first tensor
        :param t2: 'QTensor' - second tensor
        :return:  A QTensor

        Example::

            from pyvqnet.tensor import QTensor, sub
            t1 = QTensor([1, 2, 3])
            t2 = QTensor([4, 5, 6])
            x = sub(t1, t2)
        """
    @staticmethod
    def rsub_scalar(t, scalar: _scalar_type): ...
    @staticmethod
    def add_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def sums(t, axis: _axis_type = None, keepdims: bool = False): ...
    sum = sums
    @staticmethod
    def reshape(t, shape: _size_type): ...
    @staticmethod
    def view(self_tensor, shape: _size_type): ...
    @staticmethod
    def grad(t): ...
    @staticmethod
    def create_tensor(data, device, dtype): ...
    @staticmethod
    def backward(self_tensor, grad=None, save_grad: bool = False, save_intern_act: bool = True): ...
    @staticmethod
    def astype(self_tensor, dtype: int): ...
    @staticmethod
    def full_like(t, value: float | int, device=None, dtype=None): ...
    @staticmethod
    def mul_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def zero_grad(t) -> None: ...
    @staticmethod
    def clone(t): ...
    @staticmethod
    def uniform_(self, min_value: _scalar_type, max_value: _scalar_type): ...
    @staticmethod
    def fill_rand_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def fill_rand_signed_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def diag(t, k: int = 0): ...
    @staticmethod
    def mul_(self, t): ...
    @staticmethod
    def div_(self, t): ...
    @staticmethod
    def clamp_(self, min: int | float, max: int | float): ...
    @staticmethod
    def fill_rand_normal_(self, m: _scalar_type = 0, s: _scalar_type = 1, fast_math: bool = True): ...
    @staticmethod
    def fill_rand_binary_(self, v: _scalar_type = 0.5): ...
    @staticmethod
    def detach(t): ...
    @staticmethod
    def is_not_dense(self): ...
    @staticmethod
    def to_numpy(t, copy: bool = True): ...
    @staticmethod
    def swapaxis(t, axis1: int, axis2: int): ...
    @staticmethod
    def transpose(t, dim): ...
    permute = transpose
    @staticmethod
    def zeros(shape: _size_type, device: _device_type = ..., dtype: _dtype_type = None): ...
    @staticmethod
    def is_csr(self): ...
    @staticmethod
    def move_to_device(self, device) -> None: ...
    @staticmethod
    def toGPU(self, device=...): ...
    @staticmethod
    def requires_grad_setter(data, new_value) -> None: ...
    @staticmethod
    def unsqueeze(t, axis): ...
    @staticmethod
    def multinomial(t, num_samples: int): ...
    @staticmethod
    def argtopk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def argsort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def sort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def narrow(input, dim, start, length): ...
    @staticmethod
    def unbind(t, dim: int = 0): ...
    @staticmethod
    def select_1dim(t, dim, index): ...
    @staticmethod
    def select(t, index: list):
        """
        default vqnet impl.
        """
    @staticmethod
    def FLOAT_2_COMPLEX(param): ...
    @staticmethod
    def getitem(self_tensor, item): ...
    @staticmethod
    def index_select(t, dim: int, indice): ...
    @staticmethod
    def set_select(t, index, set_tensor): ...
    @staticmethod
    def setitem(self_tensor, key, value) -> None: ...
    @staticmethod
    def size(data): ...
    @staticmethod
    def stride(data): ...
    @staticmethod
    def get_vqnet_device(device): ...
    @staticmethod
    def requires_grad_getter(t): ...
    @staticmethod
    def get_vqnet_dtype(dtype): ...
    @staticmethod
    def copy_data_from(dst, src) -> None: ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def broadcast(t1, t2): ...
    @staticmethod
    def arange(start: float | int, end: float | int, step: float | int = 1, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def empty(shape: _size_type, device: int = 0, dtype=None): ...
    @staticmethod
    def empty_like(t, device=None, dtype=None): ...
    @staticmethod
    def ones_like(input): ...
    @staticmethod
    def ones(shape: _size_type, device=..., dtype=None): ...
    @staticmethod
    def linspace(start: float | int, end: float | int, nums: int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def logspace(start: float | int, end: float | int, nums: int, base: float | int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def eye(size: int, offset: int = ..., device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def rdivide(t1, t2): ...
    @staticmethod
    def divide(t1, t2): ...
    div = divide
    @staticmethod
    def add(t1, t2): ...
    @staticmethod
    def view_as_real(t): ...
    @staticmethod
    def mv(t1, t2): ...
    @staticmethod
    def bmm(t1, t2): ...
    @staticmethod
    def default_matmul2d(t1, t2): ...
    @staticmethod
    def matmul(t1, t2): ...
    @staticmethod
    def exp(t): ...
    @staticmethod
    def acos(t): ...
    @staticmethod
    def asin(t): ...
    @staticmethod
    def atan(t): ...
    @staticmethod
    def tanh(t): ...
    @staticmethod
    def sinh(t): ...
    @staticmethod
    def cosh(t): ...
    @staticmethod
    def tan(t): ...
    @staticmethod
    def sin(t): ...
    @staticmethod
    def cos(t): ...
    @staticmethod
    def atan2(y, x): ...
    @staticmethod
    def silu(input): ...
    @staticmethod
    def sigmoid(input): ...
    @staticmethod
    def binomial(total_counts, probs): ...
    @staticmethod
    def sqrt(t): ...
    @staticmethod
    def rsqrt(t): ...
    @staticmethod
    def square(t): ...
    @staticmethod
    def clip(t, min_val: int | float | None = None, max_val: int | float | None = None): ...
    @staticmethod
    def abs(t): ...
    @staticmethod
    def softmax(t, axis): ...
    @staticmethod
    def tensordot(x, y, dim1: _size_type, dim2: _size_type): ...
    @staticmethod
    def topk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def cumsum(t, axis: int = -1): ...
    @staticmethod
    def flip(t, flip_dims: _size_type): ...
    @staticmethod
    def pad(t, pad: _shape_type, value: _scalar_type = 0): ...
    @staticmethod
    def gather(t, dim: int, index): ...
    @staticmethod
    def scatter(input, dim: int, indices, src): ...
    @staticmethod
    def expval_pauli(measurements, q_machine, obs): ...
    @staticmethod
    def pad_sequence(qtensor_list, batch_first: bool = False, padding_value: int | float = 0): ...
    @staticmethod
    def moveaxis(t, src: _size_type, dst: _size_type): ...
    @staticmethod
    def marginal_prob(prob, num_wires, wires): ...
    @staticmethod
    def probs(q_state, num_wires, wires): ...

def default_matmul2d(t1, t2): ...
def bool_mask_select_set(self, bool_mask, value): ...
def bool_mask_select(self, bool_mask): ...
def infer_neg1_dim(shape_target: _size_type, origin_size: _size_type): ...

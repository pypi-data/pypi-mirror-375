{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hello Kernel World üî•"
      ],
      "metadata": {
        "id": "RO1o5fS5W8xc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"left\">\n",
        "  <img src=\"https://raw.githubusercontent.com/WecoAI/weco-cli/main/assets/example-optimization.gif\"\n",
        "       alt=\"Optimization demo\"\n",
        "       width=\"720\">\n",
        "</p>\n",
        "\n",
        "## üñ•Ô∏è Weco CLI Resources\n",
        "\n",
        "- üìñ [CLI Reference](https://docs.weco.ai/cli/cli-reference) - Explore our docs for an in-depth look at what the tool can do\n",
        "- ‚ú® [Examples](https://docs.weco.ai/examples) - Explore automated R&D across kernel engineering, ML engineering and prompt engineering"
      ],
      "metadata": {
        "id": "yorBWlqGuC7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Dependencies"
      ],
      "metadata": {
        "id": "5BQGGqbJW2Eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "%pip install -q weco ipywidgets numpy torch\n",
        "\n",
        "# Enable custom widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "89doT3fbWcGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to determine what `DEVICE` we can run this on, a CPU or GPU..."
      ],
      "metadata": {
        "id": "gYxLwOXzfmiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from rich import print as rprint\n",
        "\n",
        "# Check if you're connected to a GPU (it's free!)\n",
        "if not torch.cuda.is_available():\n",
        "    DEVICE = \"cpu\"\n",
        "    rprint(\n",
        "        \"\"\"\n",
        "[bold yellow]‚ö†Ô∏è  GPU is not enabled.[/bold yellow] The notebook will fall back to [bold]CPU[/bold], but [italic]performance may be lower[/italic].\n",
        "\n",
        "[bold]üëâ To enable GPU (FREE):[/bold]\n",
        "‚Ä¢ Go to [green]Runtime > Change runtime type[/green]\n",
        "‚Ä¢ Set [bold]'Hardware Accelerator'[/bold] to [bold green]'GPU'[/bold green]\n",
        "‚Ä¢ Click [bold]Save[/bold] and [bold]rerun all cells[/bold]\n",
        "\n",
        "[dim]Continuing with CPU for now...[/dim]\n",
        "\"\"\"\n",
        "    )\n",
        "else:\n",
        "    DEVICE = \"cuda\"\n",
        "    rprint(\"[bold green]‚úÖ GPU is enabled.[/bold green] Proceeding with [bold green]CUDA[/bold green]...\")"
      ],
      "metadata": {
        "id": "PFAn_bzAXLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the example files from CLI repo\n",
        "!wget https://github.com/WecoAI/weco-cli/archive/refs/heads/main.zip -O repo.zip\n",
        "!unzip -j repo.zip \"weco-cli-main/examples/hello-kernel-world/*\" -d .\n",
        "!rm repo.zip"
      ],
      "metadata": {
        "id": "dFGClqxpzwyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google AI Studio has a free API usage quota. Create a key [here](https://aistudio.google.com/apikey) to use `weco` for free!"
      ],
      "metadata": {
        "id": "PGlEsI78bMtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Pass your API key below\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"\"\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "# os.environ[\"ANTHROPIC_API_KEY\"] = \"\"\n",
        "\n",
        "\n",
        "if not any([os.environ.get(key) for key in [\"GEMINI_API_KEY\", \"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\"]]):\n",
        "    rprint(\n",
        "\"[bold red]‚ùå No API keys found.[/bold red]\\n\"\n",
        "\"\\n\"\n",
        "\"Please set one of the following environment variables:\\n\"\n",
        "\"  ‚Ä¢ [cyan]GEMINI_API_KEY[/cyan]\\n\"\n",
        "\"  ‚Ä¢ [cyan]OPENAI_API_KEY[/cyan]\\n\"\n",
        "\"  ‚Ä¢ [cyan]ANTHROPIC_API_KEY[/cyan]\\n\"\n",
        "\"\\n\"\n",
        "\"Setup your [cyan]GEMINI_API_KEY[/cyan] for free - [underline white]https://aistudio.google.com/apikey[/underline white] !\"\n",
        "    )\n",
        "else:\n",
        "    rprint(\"[bold green]‚úÖ API keys found.[/bold green]\\n\\nWe'll only be able to know if they are correct once the optimization starts.\")"
      ],
      "metadata": {
        "id": "b4XuOeNzYTdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's Start Optimizing!"
      ],
      "metadata": {
        "id": "sbvA8oQceOt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got our dependecies, GPU and LLM API key sorted out, let's take a look at what code we're optimizing!\n",
        "\n",
        "Earlier, we downloaded two files:\n",
        "1. An evaluation script to help score *how good a solution is* (`evaluate.py`)\n",
        "2. A snippet of code we'd like to optimize (`optimize.py`)\n",
        "\n",
        "Let's take a look at what the code we want to optimize looks like..."
      ],
      "metadata": {
        "id": "4OjXTBkjc4Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "from pygments import highlight\n",
        "from pygments.lexers import PythonLexer\n",
        "from pygments.formatters import HtmlFormatter\n",
        "\n",
        "def view_code_block(path: str):\n",
        "    with open(path) as f:\n",
        "        display(HTML(highlight(f.read(), PythonLexer(), HtmlFormatter(full=True, style=\"monokai\"))))\n",
        "\n",
        "view_code_block(\"optimize.py\")"
      ],
      "metadata": {
        "id": "rUTxqxWgcC34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-world code is often more complex but this is a good place to start. You can find more advanced examples [here](https://docs.weco.ai/examples), however, we'd recommend starting with this notebook as the optimization setup is the exact same, no matter the complexity!"
      ],
      "metadata": {
        "id": "5C5dvasXdmNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's simple to start optimizing any piece of code! You just need to set:\n",
        "1. Path to source code - we can point this to our `optimize.py`\n",
        "2. Command to run evaluation - notice how we are using the `DEVICE` we setup earlier\n",
        "3. The metric we are optimizing for - in this case, the evaluation script (`evaluate.py`) prints the `'speedup'` achieved to the terminal\n",
        "4. Whether you want to maximize or minimize the metric you mentioned above - in our case, we want to make this code faster!\n",
        "5. Number of steps to optimize for - we'll keep it low to avoid any rate limits being hit on your free Gemini API key\n",
        "6. Additional context - anything information you think should guide the optimization process"
      ],
      "metadata": {
        "id": "YfDg-pP9fAdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get straight into it. Keep an eye on the `Best Solution` panel!\n",
        "\n",
        "Note that you can track the optimization in the logs directory (`.runs/`) and on our dashboard (links shown in the `Summary` panel)."
      ],
      "metadata": {
        "id": "TbG_3nwEhs5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, weco.cli as weco_cli\n",
        "\n",
        "# When running in a terminal, you can use this instead:\n",
        "# weco run --source optimize.py \\\n",
        "#      --eval-command f\"python evaluate.py --solution-path optimize.py --device {DEVICE}\" \\\n",
        "#      --metric speedup \\\n",
        "#      --goal maximize \\\n",
        "#      --steps 10 \\\n",
        "#      --additional-instructions \"Fuse operations in the forward method while ensuring the max float deviation remains small.\"\n",
        "\n",
        "sys.argv = [\n",
        "    \"weco\", \"run\",\n",
        "    \"--source\", \"optimize.py\",\n",
        "    \"--eval-command\", f\"python evaluate.py --solution-path optimize.py --device {DEVICE}\",\n",
        "    \"--metric\", \"speedup\",\n",
        "    \"--goal\", \"maximize\",\n",
        "    \"--steps\", \"10\",\n",
        "    \"--additional-instructions\", \"Fuse operations in the forward method while ensuring the max float deviation remains small.\"\n",
        "]\n",
        "\n",
        "try: weco_cli.main()\n",
        "except SystemExit: pass"
      ],
      "metadata": {
        "id": "17YZ2euZplDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what our optimized code looks like (`optimize.py`)!"
      ],
      "metadata": {
        "id": "990ueX_JsO_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_code_block(\"optimize.py\")"
      ],
      "metadata": {
        "id": "9dqfzXkajQKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Happy Optimizing from the [Weco](https://www.weco.ai/) Team!\n",
        "\n",
        "If you'd like to learn more about what Weco can do, here are some spots to check out:\n",
        "- üìñ [CLI Reference](https://docs.weco.ai/cli/cli-reference) - Explore our docs for an in-depth look at what the tool can do\n",
        "- ‚ú® [Examples](https://docs.weco.ai/examples) - Explore automated R&D across kernel engineering, ML engineering and prompt engineering"
      ],
      "metadata": {
        "id": "17oICow9yjn8"
      }
    }
  ]
}
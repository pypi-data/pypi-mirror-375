# -*- coding: UTF-8 -*-
"""
{{project_name}} é¡¹ç›®é…ç½®æ–‡ä»¶
=============================
åŸºäº Crawlo æ¡†æ¶çš„çˆ¬è™«é¡¹ç›®é…ç½®ã€‚

ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼š

# æ–¹å¼1ï¼šä½¿ç”¨é»˜è®¤å•æœºæ¨¡å¼ï¼ˆæ¨èï¼‰
from crawlo.crawler import CrawlerProcess
process = CrawlerProcess()  # æ— éœ€ä»»ä½•é…ç½®

# æ–¹å¼2ï¼šä½¿ç”¨é…ç½®å·¥å‚
from crawlo.config import CrawloConfig
config = CrawloConfig.standalone()  # å•æœºæ¨¡å¼
config = CrawloConfig.distributed(redis_host='192.168.1.100')  # åˆ†å¸ƒå¼æ¨¡å¼
process = CrawlerProcess(settings=config.to_dict())

# æ–¹å¼3ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
from crawlo.config import CrawloConfig
config = CrawloConfig.from_env()  # ä»ç¯å¢ƒå˜é‡è¯»å–
"""
import os
from crawlo.config import CrawloConfig

# ============================== é¡¹ç›®åŸºæœ¬ä¿¡æ¯ ==============================
PROJECT_NAME = '{{project_name}}'
VERSION = '1.0.0'

# ============================== è¿è¡Œæ¨¡å¼é€‰æ‹© ==============================

# ğŸ¯ é€‰æ‹©ä¸€ç§é…ç½®æ–¹å¼ï¼š

# æ–¹å¼1ï¼šä½¿ç”¨é…ç½®å·¥å‚ï¼ˆæ¨èï¼‰
# å•æœºæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
CONFIG = CrawloConfig.standalone(
    concurrency=8,
    download_delay=1.0
)

# åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆå»æ‰æ³¨é‡Šå¹¶ä¿®æ”¹ Redis åœ°å€ï¼‰
# CONFIG = CrawloConfig.distributed(
#     redis_host='127.0.0.1',
#     redis_password='your_password',  # å¦‚æœæœ‰å¯†ç 
#     project_name='{{project_name}}',
#     concurrency=16,
#     download_delay=1.0
# )

# è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
# CONFIG = CrawloConfig.auto(concurrency=12)

# æ–¹å¼2ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆé€‚åˆéƒ¨ç½²ï¼‰
# CONFIG = CrawloConfig.from_env()

# æ–¹å¼3ï¼šä½¿ç”¨é¢„è®¾é…ç½®
# from crawlo.config import Presets
# CONFIG = Presets.development()  # å¼€å‘ç¯å¢ƒ
# CONFIG = Presets.production()   # ç”Ÿäº§ç¯å¢ƒ

# è·å–æœ€ç»ˆé…ç½®
locals().update(CONFIG.to_dict())

# ============================== ç½‘ç»œè¯·æ±‚é…ç½® ==============================

# ä¸‹è½½å™¨é€‰æ‹©ï¼ˆæ¨èä½¿ç”¨ CurlCffiï¼Œæ”¯æŒæµè§ˆå™¨æŒ‡çº¹æ¨¡æ‹Ÿï¼‰
DOWNLOADER = "crawlo.downloader.cffi_downloader.CurlCffiDownloader"  # æ”¯æŒæµè§ˆå™¨æŒ‡çº¹
# DOWNLOADER = "crawlo.downloader.aiohttp_downloader.AioHttpDownloader"  # è½»é‡çº§é€‰æ‹©
# DOWNLOADER = "crawlo.downloader.httpx_downloader.HttpXDownloader"     # HTTP/2 æ”¯æŒ

# è¯·æ±‚è¶…æ—¶ä¸å®‰å…¨
DOWNLOAD_TIMEOUT = 30
VERIFY_SSL = True
USE_SESSION = True

# è¯·æ±‚å»¶è¿Ÿæ§åˆ¶ï¼ˆé˜²åçˆ¬ï¼‰
DOWNLOAD_DELAY = 1.0
RANDOM_RANGE = (0.8, 1.2)
RANDOMNESS = True

# é‡è¯•ç­–ç•¥
MAX_RETRY_TIMES = 3
RETRY_PRIORITY = -1
RETRY_HTTP_CODES = [408, 429, 500, 502, 503, 504, 522, 524]
IGNORE_HTTP_CODES = [403, 404]
ALLOWED_CODES = []

# è¿æ¥æ± é…ç½®
CONNECTION_POOL_LIMIT = 50
DOWNLOAD_MAXSIZE = 10 * 1024 * 1024    # 10MB
DOWNLOAD_WARN_SIZE = 1024 * 1024       # 1MB

# ============================== å¹¶å‘ä¸è°ƒåº¦é…ç½® ==============================
CONCURRENCY = 8
INTERVAL = 5
DEPTH_PRIORITY = 1
MAX_RUNNING_SPIDERS = 3

# ============================== é˜Ÿåˆ—é…ç½®ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ï¼‰ ==============================

# é˜Ÿåˆ—ç±»å‹ï¼š'auto'ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰, 'memory'ï¼ˆå†…å­˜é˜Ÿåˆ—ï¼‰, 'redis'ï¼ˆåˆ†å¸ƒå¼é˜Ÿåˆ—ï¼‰
QUEUE_TYPE = 'auto'
SCHEDULER_MAX_QUEUE_SIZE = 2000
SCHEDULER_QUEUE_NAME = f'{{project_name}}:requests'
QUEUE_MAX_RETRIES = 3
QUEUE_TIMEOUT = 300

# å¤§è§„æ¨¡çˆ¬å–ä¼˜åŒ–
LARGE_SCALE_BATCH_SIZE = 1000
LARGE_SCALE_CHECKPOINT_INTERVAL = 5000
LARGE_SCALE_MAX_MEMORY_USAGE = 500

# ============================== æ•°æ®å­˜å‚¨é…ç½® ==============================

# --- MySQL é…ç½® ---
MYSQL_HOST = os.getenv('MYSQL_HOST', '127.0.0.1')
MYSQL_PORT = int(os.getenv('MYSQL_PORT', 3306))
MYSQL_USER = os.getenv('MYSQL_USER', 'root')
MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD', '123456')
MYSQL_DB = os.getenv('MYSQL_DB', '{{project_name}}')
MYSQL_TABLE = '{{project_name}}_data'
MYSQL_BATCH_SIZE = 100

# MySQL è¿æ¥æ± 
MYSQL_FLUSH_INTERVAL = 5
MYSQL_POOL_MIN = 5
MYSQL_POOL_MAX = 20
MYSQL_ECHO = False

# --- MongoDB é…ç½® ---
MONGO_URI = os.getenv('MONGO_URI', 'mongodb://localhost:27017')
MONGO_DATABASE = f'{{project_name}}_db'
MONGO_COLLECTION = '{{project_name}}_items'
MONGO_MAX_POOL_SIZE = 200
MONGO_MIN_POOL_SIZE = 20

# ============================== å»é‡è¿‡æ»¤é…ç½® ==============================

REQUEST_DIR = '.'

# å»é‡è¿‡æ»¤å™¨ï¼ˆæ¨èåˆ†å¸ƒå¼é¡¹ç›®ä½¿ç”¨ Redis è¿‡æ»¤å™¨ï¼‰
FILTER_CLASS = 'crawlo.filters.memory_filter.MemoryFilter'
# FILTER_CLASS = 'crawlo.filters.aioredis_filter.AioRedisFilter'  # åˆ†å¸ƒå¼å»é‡

# --- Redis é…ç½®ï¼ˆç”¨äºåˆ†å¸ƒå¼å»é‡å’Œé˜Ÿåˆ—ï¼‰ ---
REDIS_HOST = os.getenv('REDIS_HOST', '127.0.0.1')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
REDIS_PASSWORD = os.getenv('REDIS_PASSWORD', '')

# æ ¹æ®æ˜¯å¦æœ‰å¯†ç ç”Ÿæˆ URL
if REDIS_PASSWORD:
    REDIS_URL = f'redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/0'
else:
    REDIS_URL = f'redis://{REDIS_HOST}:{REDIS_PORT}/0'

REDIS_KEY = f'{{project_name}}:fingerprint'
REDIS_TTL = 0
CLEANUP_FP = 0
FILTER_DEBUG = True
DECODE_RESPONSES = True

# ============================== ä¸­é—´ä»¶é…ç½® ==============================

MIDDLEWARES = [
    # === è¯·æ±‚é¢„å¤„ç†é˜¶æ®µ ===
    'crawlo.middleware.request_ignore.RequestIgnoreMiddleware',
    'crawlo.middleware.download_delay.DownloadDelayMiddleware',
    'crawlo.middleware.default_header.DefaultHeaderMiddleware',
    'crawlo.middleware.proxy.ProxyMiddleware',
    
    # === å“åº”å¤„ç†é˜¶æ®µ ===
    'crawlo.middleware.retry.RetryMiddleware',
    'crawlo.middleware.response_code.ResponseCodeMiddleware',
    'crawlo.middleware.response_filter.ResponseFilterMiddleware',
]

# ============================== æ•°æ®ç®¡é“é…ç½® ==============================

PIPELINES = [
    # æ ¹æ®è¿è¡Œæ¨¡å¼è‡ªåŠ¨é€‰æ‹©é»˜è®¤å»é‡ç®¡é“
    # å•æœºæ¨¡å¼ï¼šcrawlo.pipelines.MemoryDedupPipeline
    # åˆ†å¸ƒå¼æ¨¡å¼ï¼šcrawlo.pipelines.RedisDedupPipeline
    'crawlo.pipelines.console_pipeline.ConsolePipeline',
    # '{{project_name}}.pipelines.DatabasePipeline',        # è‡ªå®šä¹‰æ•°æ®åº“ç®¡é“
    # 'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',  # MySQL å­˜å‚¨
    # 'crawlo.pipelines.mongo_pipeline.MongoPipeline',      # MongoDB å­˜å‚¨
]

# ============================== æ‰©å±•ç»„ä»¶ ==============================

EXTENSIONS = [
    'crawlo.extension.log_interval.LogIntervalExtension',
    'crawlo.extension.log_stats.LogStats',
    'crawlo.extension.logging_extension.CustomLoggerExtension',
]

# ============================== æ—¥å¿—é…ç½® ==============================

LOG_LEVEL = 'INFO'
STATS_DUMP = True
LOG_FILE = f'logs/{{project_name}}.log'
LOG_FORMAT = '%(asctime)s - [%(name)s] - %(levelname)sï¼š %(message)s'
LOG_ENCODING = 'utf-8'

# ============================== ä»£ç†é…ç½® ==============================

PROXY_ENABLED = False
PROXY_API_URL = ""  # è¯·å¡«å…¥çœŸå®çš„ä»£ç†APIåœ°å€
PROXY_EXTRACTOR = "proxy"
PROXY_REFRESH_INTERVAL = 60
PROXY_API_TIMEOUT = 10

# ============================== æµè§ˆå™¨æŒ‡çº¹é…ç½® ==============================

# CurlCffi ä¸‹è½½å™¨ä¸“ç”¨é…ç½®
CURL_BROWSER_TYPE = "chrome"
CURL_BROWSER_VERSION_MAP = {
    "chrome": "chrome136",
    "edge": "edge101",
    "safari": "safari184",
    "firefox": "firefox135",
}

# é»˜è®¤è¯·æ±‚å¤´
DEFAULT_REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# ============================== å¼€å‘ä¸è°ƒè¯• ==============================

# å¼€å‘æ¨¡å¼é…ç½®
DEBUG = False
TESTING = False

# æ€§èƒ½ç›‘æ§
ENABLE_PERFORMANCE_MONITORING = True
MEMORY_USAGE_WARNING_THRESHOLD = 500  # MB

# ============================== è‡ªå®šä¹‰é…ç½®åŒºåŸŸ ==============================
# åœ¨æ­¤å¤„æ·»åŠ é¡¹ç›®ç‰¹å®šçš„é…ç½®é¡¹

# ç¤ºä¾‹ï¼šç›®æ ‡ç½‘ç«™ç‰¹å®šé…ç½®
# TARGET_DOMAIN = '{{domain}}'
# MAX_PAGES_PER_DOMAIN = 10000
# CUSTOM_RATE_LIMIT = 1.5
import numpy as np
from .abstract_backend import ExtendedBackend as ExtendedBackend
from _typeshed import Incomplete
from tensornetwork.backends.numpy import numpy_backend
from typing import Any, Callable, Sequence

logger: Incomplete
dtypestr: str
npdtype: Incomplete
Tensor = Any

class NumpyBackend(numpy_backend.NumPyBackend, ExtendedBackend):
    """
    see the original backend API at `numpy backend
    <https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_
    """
    def eye(self, N: int, dtype: str | None = None, M: int | None = None) -> Tensor: ...
    def ones(self, shape: Sequence[int], dtype: str | None = None) -> Tensor: ...
    def zeros(self, shape: Sequence[int], dtype: str | None = None) -> Tensor: ...
    def copy(self, a: Tensor) -> Tensor: ...
    def expm(self, a: Tensor) -> Tensor: ...
    def abs(self, a: Tensor) -> Tensor: ...
    def sin(self, a: Tensor) -> Tensor: ...
    def cos(self, a: Tensor) -> Tensor: ...
    def acos(self, a: Tensor) -> Tensor: ...
    def acosh(self, a: Tensor) -> Tensor: ...
    def asin(self, a: Tensor) -> Tensor: ...
    def asinh(self, a: Tensor) -> Tensor: ...
    def atan(self, a: Tensor) -> Tensor: ...
    def atan2(self, y: Tensor, x: Tensor) -> Tensor: ...
    def atanh(self, a: Tensor) -> Tensor: ...
    def cosh(self, a: Tensor) -> Tensor: ...
    def tan(self, a: Tensor) -> Tensor: ...
    def tanh(self, a: Tensor) -> Tensor: ...
    def sinh(self, a: Tensor) -> Tensor: ...
    def size(self, a: Tensor) -> Tensor: ...
    def eigvalsh(self, a: Tensor) -> Tensor: ...
    def kron(self, a: Tensor, b: Tensor) -> Tensor: ...
    def dtype(self, a: Tensor) -> str: ...
    def numpy(self, a: Tensor) -> Tensor: ...
    def i(self, dtype: Any = None) -> Tensor: ...
    def stack(self, a, axis: int = 0) -> Tensor: ...
    def concat(self, a: Sequence[Tensor], axis: int = 0) -> Tensor: ...
    def tile(self, a: Tensor, rep: Tensor) -> Tensor: ...
    def mean(self, a: Tensor, axis: Sequence[int] | None = None, keepdims: bool = False) -> Tensor: ...
    def std(self, a: Tensor, axis: Sequence[int] | None = None, keepdims: bool = False) -> Tensor: ...
    def unique_with_counts(self, a: Tensor, **kws: Any) -> tuple[Tensor, Tensor]: ...
    def min(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def max(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def argmax(self, a: Tensor, axis: int = 0) -> Tensor: ...
    def argmin(self, a: Tensor, axis: int = 0) -> Tensor: ...
    def sigmoid(self, a: Tensor) -> Tensor: ...
    def relu(self, a: Tensor) -> Tensor: ...
    def softmax(self, a: Sequence[Tensor], axis: int | None = None) -> Tensor: ...
    def onehot(self, a: Tensor, num: int) -> Tensor: ...
    def cumsum(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def is_tensor(self, a: Any) -> bool: ...
    def real(self, a: Tensor) -> Tensor: ...
    def imag(self, a: Tensor) -> Tensor: ...
    def cast(self, a: Tensor, dtype: str) -> Tensor: ...
    def arange(self, start: int, stop: int | None = None, step: int = 1) -> Tensor: ...
    def mod(self, x: Tensor, y: Tensor) -> Tensor: ...
    def right_shift(self, x: Tensor, y: Tensor) -> Tensor: ...
    def left_shift(self, x: Tensor, y: Tensor) -> Tensor: ...
    def solve(self, A: Tensor, b: Tensor, assume_a: str = 'gen') -> Tensor: ...
    def searchsorted(self, a: Tensor, v: Tensor, side: str = 'left') -> Tensor: ...
    g: Incomplete
    def set_random_state(self, seed: int | None = None, get_only: bool = False) -> Any: ...
    def stateful_randn(self, g: np.random.Generator, shape: int | Sequence[int] = 1, mean: float = 0, stddev: float = 1, dtype: str = '32') -> Tensor: ...
    def stateful_randu(self, g: np.random.Generator, shape: int | Sequence[int] = 1, low: float = 0, high: float = 1, dtype: str = '32') -> Tensor: ...
    def stateful_randc(self, g: np.random.Generator, a: int | Sequence[int] | Tensor, shape: int | Sequence[int], p: Sequence[float] | Tensor | None = None) -> Tensor: ...
    def scatter(self, operand: Tensor, indices: Tensor, updates: Tensor) -> Tensor: ...
    def coo_sparse_matrix(self, indices: Tensor, values: Tensor, shape: Tensor) -> Tensor: ...
    def sparse_dense_matmul(self, sp_a: Tensor, b: Tensor) -> Tensor: ...
    def to_dense(self, sp_a: Tensor) -> Tensor: ...
    def is_sparse(self, a: Tensor) -> bool: ...
    def cond(self, pred: bool, true_fun: Callable[[], Tensor], false_fun: Callable[[], Tensor]) -> Tensor: ...
    def switch(self, index: Tensor, branches: Sequence[Callable[[], Tensor]]) -> Tensor: ...
    def device(self, a: Tensor) -> str: ...
    def device_move(self, a: Tensor, dev: Any) -> Tensor: ...
    def stop_gradient(self, a: Tensor) -> Tensor: ...
    def grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., Any]: ...
    def value_and_grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., tuple[Any, Any]]: ...
    def jit(self, f: Callable[..., Any], static_argnums: int | Sequence[int] | None = None, jit_compile: bool | None = None) -> Callable[..., Any]: ...
    def vmap(self, f: Callable[..., Any], vectorized_argnums: int | Sequence[int] = 0) -> Any: ...
    def vectorized_value_and_grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, vectorized_argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., tuple[Any, Any]]: ...
    vvag = vectorized_value_and_grad
    def vjp(self, f: Callable[..., Any], inputs: Tensor | Sequence[Tensor], v: Tensor | Sequence[Tensor]) -> tuple[Tensor | Sequence[Tensor], Tensor | Sequence[Tensor]]: ...
    def jvp(self, f: Callable[..., Any], inputs: Tensor | Sequence[Tensor], v: Tensor | Sequence[Tensor]) -> tuple[Tensor | Sequence[Tensor], Tensor | Sequence[Tensor]]: ...

{
  "json_schema_major_version": "1",
  "json_schema_minor_version": "0",
  "json_content_version": "1",
  "function_name": "write_nos",
  "function_version": "1.0",
  "function_type": "table_operator",
  "function_alias_name": "write_nos",
  "function_r_name": "write.nos",
  "supports_view": false,
  "short_description": "This function enables access to write data from Vantage to external storage, like Amazon S3, Azure Blob storage, or Google Cloud Storage.",
  "long_description": "This function enables access to write data from Vantage to external storage, like Amazon S3, Azure Blob storage, or Google Cloud Storage. You must have the EXECUTE FUNCTION privilege on TD_SYSFNLIB.WRITE_NOS.",
  "input_tables": [
    {
      "requiredInputKind": [
        "PartitionByKey"
      ],
      "isOrdered": false,
      "isLocalOrdered": true,
      "partitionByOne": false,
      "hashByKey": true,
      "name": "input",
      "alternateNames": [],
      "isRequired": true,
      "rDescription": "Specifies the teradataml DataFrame containing the input data which will be written to external storage.",
      "description": "Specifies the table containing the input data which will be written to external storage.",
      "datatype": "TABLE_ALIAS",
      "allowsLists": false,
      "rName": "data",
      "useInR": true,
      "rOrderNum": 1
    }
  ],
  "argument_clauses": [
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "LOCATION",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the location value, which is a Uniform Resource Identifier (URI) pointing to location in the external object storage system.\nA URI identifying the external storage system in the format:\n /connector/endpoint/bucket_or_container/prefix \nThe LOCATION string cannot exceed 2048 characters.",
      "description": "Specifies the location value, which is a Uniform Resource Identifier (URI) pointing to location in the external object storage system.\nA URI identifying the external storage system in the format:\n /connector/endpoint/bucket_or_container/prefix \nThe LOCATION string cannot exceed 2048 characters.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "location",
      "useInR": true,
      "rOrderNum": 2
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "AUTHORIZATION",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the authorization for accessing the external storage. On any platform, you can specify an authorization object ([DatabaseName.]AuthorizationObjectName). You must have the EXECUTE privilege on AuthorizationObjectName.\nOn Amazon S3 and Azure Blob storage and Azure Data Lake Storage Gen2, you can specify either an authorization object or a string in JSON format. The string specifies the USER (identification) and PASSWORD (secret_key) for accessing external storage. The following table shows the supported credentials for USER and PASSWORD (used in the CREATE AUTHORIZATION command):\nSystem/Scheme                       |USER                           |PASSWORD\nAWS                                 |Access Key ID                  |Access Key Secret\nAzure / Shared Key                  |Storage Account Name           |Storage Account Key\nAzure Shared Access Signature (SAS) |Storage Account Name           |Account SAS Token\nGoogle Cloud (S3 interop mode)      |Access Key ID                  |Access Key Secret\nGoogle Cloud (native)               |Client Email                   |Private Key\nOn-premises object stores           |Access Key ID                  |Access Key Secret\nPublic access object stores         |<empty string>                 |<empty string>\n                                    |Enclose the empty string in    |Enclose the empty string in\n                                    |single straight quotes: USER ''| single straight quotes: PASSWORD ''\nIf you use a function mapping to define a wrapper for READ_NOS, you can specify the authorization in the function mapping. With function mappings, you can use only [ INVOKER | DEFINER ] TRUSTED, not system-wide authorization.\nIf an AWS IAM credential provides access, you can omit the AUTHORIZATION clause.",
      "description": "Specifies the authorization for accessing the external storage. On any platform, you can specify an authorization object ([DatabaseName.]AuthorizationObjectName). You must have the EXECUTE privilege on AuthorizationObjectName.\nOn Amazon S3 and Azure Blob storage and Azure Data Lake Storage Gen2, you can specify either an authorization object or a string in JSON format. The string specifies the USER (identification) and PASSWORD (secret_key) for accessing external storage. The following table shows the supported credentials for USER and PASSWORD (used in the CREATE AUTHORIZATION command):\nSystem/Scheme                       |USER                           |PASSWORD\nAWS                                 |Access Key ID                  |Access Key Secret\nAzure / Shared Key                  |Storage Account Name           |Storage Account Key\nAzure Shared Access Signature (SAS) |Storage Account Name           |Account SAS Token\nGoogle Cloud (S3 interop mode)      |Access Key ID                  |Access Key Secret\nGoogle Cloud (native)               |Client Email                   |Private Key\nOn-premises object stores           |Access Key ID                  |Access Key Secret\nPublic access object stores         |<empty string>                 |<empty string>\n                                    |Enclose the empty string in    |Enclose the empty string in\n                                    |single straight quotes: USER ''| single straight quotes: PASSWORD ''\nIf you use a function mapping to define a wrapper for READ_NOS, you can specify the authorization in the function mapping. With function mappings, you can use only [ INVOKER | DEFINER ] TRUSTED, not system-wide authorization.\nIf an AWS IAM credential provides access, you can omit the AUTHORIZATION clause.",
      "datatype": ["STRING", "JSON"],
      "allowsLists": false,
      "rName": "authorization",
      "useInR": true,
      "rOrderNum": 3
    },
    {
      "permittedValues": ["PARQUET"],
      "rDefaultValue": "PARQUET",
      "isOutputColumn": false,
      "name": "STOREDAS",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the formatting style of the external data.\nPARQUET means the external data is formatted as Parquet. Objects created in external storage by write_nos are written only in Parquet format.",
      "description": "Specifies the formatting style of the external data.\nPARQUET means the external data is formatted as Parquet. Objects created in external storage by write_nos are written only in Parquet format.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "stored.as",
      "useInR": true,
      "rOrderNum": 4
    },
    {
      "permittedValues": ["DISCRETE", "RANGE"],
      "defaultValue": "RANGE",
      "isOutputColumn": false,
      "name": "NAMING",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the how the objects containing the rows of data are named in the external storage:\nDiscrete naming uses the ordering column values as part of the object names in external storage. For example, if the PARTITION BY clause has ORDER BY dateColumn, intColumn, the discrete form name of the objects written to external storage would include the values for those columns as part of the object name, which would look similar to this:\nS3/ceph-s3.teradata.com/xz186000/2019-03-01/13/object_33_0_1.parquet\n2019-03-01 is the value for dateColumn, the first ordering column, and 13 is the value for the second ordering column, intColumn. All rows stored in this external Parquet-formatted object contain those two values.\nRange naming, the default, includes as part of the object name the range of values included in the partition for each ordering column. For example, using the same ORDER BY as above the object names would look similar to this:\nS3/ceph-s3.teradata.com/xz186000/2019-01-01/2019-03-02/9/10000/object_33_0_1.parquet\nwhere 2019-01-01 is the minimum value in that object for the first ordering column, dateColumn, 2019-03-02 is the maximum value for the rows stored in this external Parquet-formatted object. Value 9 is the minimum value for the second ordering column, intColumn, and 10000 is the maximum value for that column.",
      "description": "Specifies the how the objects containing the rows of data are named in the external storage:\nDiscrete naming uses the ordering column values as part of the object names in external storage. For example, if the PARTITION BY clause has ORDER BY dateColumn, intColumn, the discrete form name of the objects written to external storage would include the values for those columns as part of the object name, which would look similar to this:\nS3/ceph-s3.teradata.com/xz186000/2019-03-01/13/object_33_0_1.parquet\n2019-03-01 is the value for dateColumn, the first ordering column, and 13 is the value for the second ordering column, intColumn. All rows stored in this external Parquet-formatted object contain those two values.\nRange naming, the default, includes as part of the object name the range of values included in the partition for each ordering column. For example, using the same ORDER BY as above the object names would look similar to this:\nS3/ceph-s3.teradata.com/xz186000/2019-01-01/2019-03-02/9/10000/object_33_0_1.parquet\nwhere 2019-01-01 is the minimum value in that object for the first ordering column, dateColumn, 2019-03-02 is the maximum value for the rows stored in this external Parquet-formatted object. Value 9 is the minimum value for the second ordering column, intColumn, and 10000 is the maximum value for that column.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "naming",
      "useInR": true,
      "rOrderNum": 5
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "MANIFESTFILE",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the fully qualified path and file name where the manifest file is written. Use the format\n/connector/end point/bucket_or_container/prefix/manifest_file_name\nFor example:\n/S3/ceph-s3.teradata.com/xz186000/manifest/manifest.json\nIf you do not include the manifestfile parameter, no manifest file is written.",
      "description": "Specifies the fully qualified path and file name where the manifest file is written. Use the format\n/connector/end point/bucket_or_container/prefix/manifest_file_name\nFor example:\n/S3/ceph-s3.teradata.com/xz186000/manifest/manifest.json\nIf you do not include the manifestfile parameter, no manifest file is written.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "manifest.file",
      "useInR": true,
      "rOrderNum": 6
    },
    {
      "permittedValues": [],
      "defaultValue": false,
      "isOutputColumn": false,
      "name": "MANIFESTONLY",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies wheather to write only a manifest file in external storage or not. No actual data objects are written to external storage if you use manifestonly = True. You must also use the manifest_file option to create a manifest file in external storage. Use this option to create a new manifest file in the event that a write_nos operation fails due to a database abort or restart, or when network connectivity issues interrupt and stop a write_nos operation before all data has been written to external storage. The manifest is created from the table or query result set that is input to write_nos. The input must be a list of storage object names and sizes, with one row per object.\nNote: The input to write_nos with manifestonly can itself incorporate read_nos, similar to this, which uses function mappings for write_nos and read_nos:\n (TODO: Add manually or update json)\nA query like this can be used if a write_nos operation fails before it can create a manifest file. The new manifest file created using read_nos will reflect all data objects currently in the external storage location, and can aid in determining which data objects resulted from the incomplete write_nos operation. For more information, see Teradata Vantage™ - Native Object Store Getting Started Guide, B035-1214.",
      "description": "Specifies whether to write only a manifest file in external storage or not. No actual data objects are written to external storage if you use manifestonly = True. You must also use the manifest_file option to create a manifest file in external storage. Use this option to create a new manifest file in the event that a write_nos operation fails due to a database abort or restart, or when network connectivity issues interrupt and stop a write_nos operation before all data has been written to external storage. The manifest is created from the table or query result set that is input to write_nos. The input must be a list of storage object names and sizes, with one row per object.\nNote: The input to write_nos with manifestonly can itself incorporate read_nos, similar to this, which uses function mappings for write_nos and read_nos:\n (TODO: Add manually or update json)\nA query like this can be used if a write_nos operation fails before it can create a manifest file. The new manifest file created using read_nos will reflect all data objects currently in the external storage location, and can aid in determining which data objects resulted from the incomplete write_nos operation. For more information, see Teradata Vantage™ - Native Object Store Getting Started Guide, B035-1214.",
      "datatype": "BOOLEAN",
      "allowsLists": false,
      "rName": "manifest.only",
      "useInR": true,
      "rOrderNum": 7
    },
    {
      "permittedValues": [],
      "defaultValue": false,
      "isOutputColumn": false,
      "name": "OVERWRITE",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies whether an existing manifest file in external storage will be overwritten with a new manifest file that has the same name. If False, the default, write_nos returns an error if a manifest file exists in external storage that is named identically to the value of manifestfile.\nNote: overwrite must be used with manifest.only = True",
      "description": "Specifies whether an existing manifest file in external storage will be overwritten with a new manifest file that has the same name. If False, the default, write_nos returns an error if a manifest file exists in external storage that is named identically to the value of manifestfile.\nNote: overwrite must be used with MANIFESTONLY = True",
      "datatype": "BOOLEAN",
      "allowsLists": false,
      "rName": "overwrite",
      "useInR": true,
      "rOrderNum": 8
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "INCLUDE_ORDERING",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies whether the ORDER BY columns and their values are written to external storage.",
      "description": "Specifies whether the ORDER BY columns and their values are written to external storage.",
      "datatype": "BOOLEAN",
      "allowsLists": false,
      "rName": "include.ordering",
      "useInR": true,
      "rOrderNum": 9
    },
    {
      "permittedValues": [],
      "isOutputColumn": false,
      "name": "INCLUDE_HASHBY",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies whether the HASH BY columns and their values are written to external storage.",
      "description": "Specifies whether the HASH BY columns and their values are written to external storage.",
      "datatype": "BOOLEAN",
      "allowsLists": false,
      "rName": "include.hashby",
      "useInR": true,
      "rOrderNum": 10
    },
    {
      "permittedValues": [],
      "defaultValue": "16MB",
      "isOutputColumn": false,
      "name": "MAXOBJECTSIZE",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the maximum output object size in megabytes, where max.object.size is a number between 4 and 16. The default is the value of the DefaultRowGroupSize field in DBS Control. For more information on DBS Control, see Teradata Vantage™ - Database Utilities , B035-1102.",
      "description": "Specifies the maximum output object size in megabytes, where MAXOBJECTSIZE is a number between 4 and 16. The default is the value of the DefaultRowGroupSize field in DBS Control. For more information on DBS Control, see Teradata Vantage™ - Database Utilities , B035-1102.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "max.object.size",
      "useInR": true,
      "rOrderNum": 11
    },
    {
      "permittedValues": ["GZIP", "SNAPPY"],
      "isOutputColumn": false,
      "name": "COMPRESSION",
      "alternateNames": [],
      "isRequired": false,
      "rDescription": "Specifies the compression algorithm used to compress the objects written to external storage.\nNote: For Parquet files the compression occurs inside parts of the parquet file instead of for the entire file, so the file extension on external objects remains .parquet.",
      "description": "Specifies the compression algorithm used to compress the objects written to external storage.\nNote: For Parquet files the compression occurs inside parts of the parquet file instead of for the entire file, so the file extension on external objects remains .parquet.",
      "datatype": "STRING",
      "allowsLists": false,
      "rName": "compression",
      "useInR": true,
      "rOrderNum": 12
    }
  ]
}
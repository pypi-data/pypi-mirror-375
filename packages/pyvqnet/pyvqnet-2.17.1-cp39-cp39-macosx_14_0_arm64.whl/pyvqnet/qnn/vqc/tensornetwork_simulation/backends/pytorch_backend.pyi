from .abstract_backend import ExtendedBackend as ExtendedBackend
from _typeshed import Incomplete
from tensornetwork.backends.pytorch import pytorch_backend
from typing import Any, Callable, Sequence

dtypestr: str
Tensor = Any
pytree = Any
torchlib: Any
logger: Incomplete

class torch_jit_func:
    """
    Delay the tracing of torch jit to the first run time:
    consistent with tf and jax mechanism
    """
    compiled: bool
    f: Incomplete
    def __init__(self, f: Callable[..., Any]) -> None: ...
    def __call__(self, *args: Any, **kws: Any) -> Any: ...

class torch_optimizer:
    optimizer: Incomplete
    is_init: bool
    def __init__(self, optimizer: Any) -> None: ...
    def update(self, grads: pytree, params: pytree) -> pytree: ...

class PyTorchBackend(pytorch_backend.PyTorchBackend, ExtendedBackend):
    """
    See the original backend API at `pytorch backend
    <https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_

    Note the functionality provided by pytorch backend is incomplete,
    it currenly lacks native efficicent jit and vmap support.
    """
    name: str
    def __init__(self) -> None: ...
    def eye(self, N: int, dtype: str | None = None, M: int | None = None) -> Tensor: ...
    def ones(self, shape: Sequence[int], dtype: str | None = None) -> Tensor: ...
    def zeros(self, shape: Sequence[int], dtype: str | None = None) -> Tensor: ...
    def copy(self, a: Tensor) -> Tensor: ...
    def expm(self, a: Tensor) -> Tensor: ...
    def exp(self, a: Tensor) -> Tensor: ...
    def sin(self, a: Tensor) -> Tensor: ...
    def cos(self, a: Tensor) -> Tensor: ...
    def acos(self, a: Tensor) -> Tensor: ...
    def acosh(self, a: Tensor) -> Tensor: ...
    def asin(self, a: Tensor) -> Tensor: ...
    def asinh(self, a: Tensor) -> Tensor: ...
    def atan(self, a: Tensor) -> Tensor: ...
    def atan2(self, y: Tensor, x: Tensor) -> Tensor: ...
    def atanh(self, a: Tensor) -> Tensor: ...
    def cosh(self, a: Tensor) -> Tensor: ...
    def tan(self, a: Tensor) -> Tensor: ...
    def tanh(self, a: Tensor) -> Tensor: ...
    def sinh(self, a: Tensor) -> Tensor: ...
    def size(self, a: Tensor) -> Tensor: ...
    def eigvalsh(self, a: Tensor) -> Tensor: ...
    def kron(self, a: Tensor, b: Tensor) -> Tensor: ...
    def numpy(self, a: Tensor) -> Tensor: ...
    def i(self, dtype: Any = None) -> Tensor: ...
    def det(self, a: Tensor) -> Tensor: ...
    def real(self, a: Tensor) -> Tensor: ...
    def imag(self, a: Tensor) -> Tensor: ...
    def dtype(self, a: Tensor) -> str: ...
    def stack(self, a, axis: int = 0) -> Tensor: ...
    def concat(self, a: Sequence[Tensor], axis: int = 0) -> Tensor: ...
    def tile(self, a: Tensor, rep: Tensor) -> Tensor: ...
    def mean(self, a: Tensor, axis: Sequence[int] | None = None, keepdims: bool = False) -> Tensor: ...
    def std(self, a: Tensor, axis: Sequence[int] | None = None, keepdims: bool = False) -> Tensor: ...
    def min(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def max(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def argmax(self, a: Tensor, axis: int = 0) -> Tensor: ...
    def argmin(self, a: Tensor, axis: int = 0) -> Tensor: ...
    def unique_with_counts(self, a: Tensor, **kws: Any) -> tuple[Tensor, Tensor]: ...
    def sigmoid(self, a: Tensor) -> Tensor: ...
    def relu(self, a: Tensor) -> Tensor: ...
    def softmax(self, a: Sequence[Tensor], axis: int | None = None) -> Tensor: ...
    def onehot(self, a: Tensor, num: int) -> Tensor: ...
    def cumsum(self, a: Tensor, axis: int | None = None) -> Tensor: ...
    def is_tensor(self, a: Any) -> bool: ...
    def cast(self, a: Tensor, dtype: str) -> Tensor: ...
    def arange(self, start: int, stop: int | None = None, step: int = 1) -> Tensor: ...
    def mod(self, x: Tensor, y: Tensor) -> Tensor: ...
    def right_shift(self, x: Tensor, y: Tensor) -> Tensor: ...
    def left_shift(self, x: Tensor, y: Tensor) -> Tensor: ...
    def solve(self, A: Tensor, b: Tensor, **kws: Any) -> Tensor: ...
    def searchsorted(self, a: Tensor, v: Tensor, side: str = 'left') -> Tensor: ...
    def reverse(self, a: Tensor) -> Tensor: ...
    def tree_map(self, f: Callable[..., Any], *pytrees: Any) -> Any: ...
    def tree_flatten(self, pytree: Any) -> tuple[Any, Any]: ...
    def tree_unflatten(self, treedef: Any, leaves: Any) -> Any: ...
    def from_dlpack(self, a: Any) -> Tensor: ...
    def to_dlpack(self, a: Tensor) -> Any: ...
    def cond(self, pred: bool, true_fun: Callable[[], Tensor], false_fun: Callable[[], Tensor]) -> Tensor: ...
    def switch(self, index: Tensor, branches: Sequence[Callable[[], Tensor]]) -> Tensor: ...
    def device(self, a: Tensor) -> str: ...
    def device_move(self, a: Tensor, dev: Any) -> Tensor: ...
    def stop_gradient(self, a: Tensor) -> Tensor: ...
    def grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., Any]: ...
    def value_and_grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., tuple[Any, Any]]: ...
    def vjp(self, f: Callable[..., Any], inputs: Tensor | Sequence[Tensor], v: Tensor | Sequence[Tensor]) -> tuple[Tensor | Sequence[Tensor], Tensor | Sequence[Tensor]]: ...
    def jvp(self, f: Callable[..., Any], inputs: Tensor | Sequence[Tensor], v: Tensor | Sequence[Tensor]) -> tuple[Tensor | Sequence[Tensor], Tensor | Sequence[Tensor]]: ...
    def compile(self, f: Callable[..., Any]) -> Any: ...
    def vmap(self, f: Callable[..., Any], vectorized_argnums: int | Sequence[int] = 0, out_dims: int | Sequence[int] = 0) -> Any: ...
    def jit(self, f: Callable[..., Any], static_argnums: int | Sequence[int] | None = None, jit_compile: bool | None = None, **kws: Any) -> Any: ...
    def vectorized_value_and_grad(self, f: Callable[..., Any], argnums: int | Sequence[int] = 0, vectorized_argnums: int | Sequence[int] = 0, has_aux: bool = False) -> Callable[..., tuple[Any, Any]]: ...
    vvag = vectorized_value_and_grad
    optimizer = torch_optimizer

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIVATE\n",
    "\n",
    "import submitit\n",
    "\n",
    "ex_ml = submitit.get_executor(\n",
    "    slurm_partition='ml_gpu-rtx2080',\n",
    "    timeout_min=60*24,\n",
    ")\n",
    "\n",
    "ex_bosch_cpu = submitit.get_executor(\n",
    "    slurm_partition='bosch_cpu-cascadelake',\n",
    "    cpus_per_task=8,\n",
    "    timeout_min=60*24,\n",
    "    slurm_gres='gpu:0',\n",
    ")\n",
    "\n",
    "ex_bosch = submitit.get_executor(\n",
    "    slurm_partition='bosch_gpu-rtx2080',\n",
    "    timeout_min=60*24,\n",
    ")\n",
    "\n",
    "ex = submitit.get_executor(\n",
    "    slurm_partition='alldlc_gpu-rtx2080',\n",
    "    timeout_min=60*24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(input_size, hidden_sizes, activation=\"relu\", num_outputs=1, weight_multiplier=1.0):\n",
    "    layers = []\n",
    "    layer_sizes = [input_size] + hidden_sizes + [num_outputs]\n",
    "    \n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        linear_layer = torch.nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "        \n",
    "        # Apply weight multiplier\n",
    "        with torch.no_grad():\n",
    "            linear_layer.weight.mul_(weight_multiplier)\n",
    "            linear_layer.bias.mul_(weight_multiplier)\n",
    "        \n",
    "        layers.append(linear_layer)\n",
    "        \n",
    "        if i < len(layer_sizes) - 2:  # Don't add activation after the last layer\n",
    "            if activation == \"relu\":\n",
    "                layers.append(torch.nn.ReLU())\n",
    "            elif activation == \"tanh\":\n",
    "                layers.append(torch.nn.Tanh())\n",
    "            else:\n",
    "                raise ValueError(\"Activation must be either 'relu' or 'tanh'\")\n",
    "    \n",
    "    return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "def train_student(teacher_mlp, student_mlp, num_features, test_examples, num_steps=1000, batch_size=256, learning_rate=0.001,\n",
    "                  use_input_optimization=False, gen_learning_rate=0.001, old_generator_prob=0.5, apply_scheduler_to_gen=False, return_saved_generators=False,\n",
    "                  device=None,\n",
    "                 ):\n",
    "    # Use CUDA if available\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher_mlp = teacher_mlp.to(device)\n",
    "    student_mlp = student_mlp.to(device)\n",
    "\n",
    "    # Initialize optimizer for the student\n",
    "    optimizer = torch.optim.Adam(student_mlp.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Add cosine annealing scheduler\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "    \n",
    "    # Initialize generator for input distribution if input optimization is enabled\n",
    "    input_generator = None\n",
    "    gen_optimizer = None\n",
    "    gen_scheduler = None\n",
    "    if use_input_optimization:\n",
    "        class InputGenerator(torch.nn.Module):\n",
    "            def __init__(self, batch_size, input_dim):\n",
    "                super().__init__()\n",
    "                self.means = torch.nn.Parameter(torch.rand(batch_size // 2, input_dim, device=device) * 2 - 1)\n",
    "                self.log_stds = torch.nn.Parameter(torch.zeros(batch_size // 2, input_dim, device=device))\n",
    "            \n",
    "            def forward(self):\n",
    "                stds = torch.exp(self.log_stds)\n",
    "                epsilon = torch.randn_like(self.means)\n",
    "                return self.means + stds * epsilon\n",
    "\n",
    "        input_generator = InputGenerator(batch_size, num_features).to(device)\n",
    "        gen_optimizer = torch.optim.Adam(input_generator.parameters(), lr=gen_learning_rate)\n",
    "        if apply_scheduler_to_gen:\n",
    "            gen_scheduler = lr_scheduler.CosineAnnealingLR(gen_optimizer, T_max=num_steps)\n",
    "    \n",
    "    # Manually implement KL divergence\n",
    "    def kl_divergence(log_p, log_q):\n",
    "        return torch.sum(log_p.exp() * (log_p - log_q), dim=1).mean()\n",
    "\n",
    "    saved_generators = []\n",
    "    total_time = 0.0\n",
    "    log_interval = 100\n",
    "    current_loss = 0.0\n",
    "    agg_losses = []\n",
    "    for step in range(num_steps):\n",
    "        start_time = time()\n",
    "        if use_input_optimization:\n",
    "            gen_inputs = torch.clamp(input_generator(), min=-1, max=1)\n",
    "            \n",
    "            if step == 0 or random.random() > old_generator_prob:\n",
    "                random_inputs = torch.rand(batch_size // 2, num_features, device=device) * 2 - 1\n",
    "            else:\n",
    "                random_inputs = saved_generators[random.randint(0, len(saved_generators)-1)]['means'].to(device)\n",
    "\n",
    "            inputs = torch.cat([gen_inputs, random_inputs], dim=0)\n",
    "        else:\n",
    "            inputs = torch.rand(batch_size, num_features, device=device) * 2 - 1\n",
    "        \n",
    "        teacher_log_probs = torch.log_softmax(teacher_mlp(inputs), dim=1)\n",
    "        \n",
    "        student_log_probs = torch.log_softmax(student_mlp(inputs), dim=1)\n",
    "        \n",
    "        loss = kl_divergence(teacher_log_probs, student_log_probs)\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if use_input_optimization:\n",
    "            for param in input_generator.parameters():\n",
    "                param.grad = -param.grad\n",
    "            gen_optimizer.step()\n",
    "            gen_optimizer.zero_grad()\n",
    "            if apply_scheduler_to_gen:\n",
    "                gen_scheduler.step()\n",
    "            with torch.no_grad():\n",
    "                input_generator.means.data.clamp_(-1, 1)\n",
    "        \n",
    "        step_time = time() - start_time\n",
    "        total_time += step_time\n",
    "\n",
    "        if (step + 1) % log_interval == 0:\n",
    "            avg_loss = current_loss / log_interval\n",
    "            avg_time_per_step = total_time / (step + 1)\n",
    "            print(f\"Step [{step+1}/{num_steps}], Average Loss: {avg_loss:.4f}, Current Loss: {loss.item():.4f}, Avg Time/Step: {avg_time_per_step:.4f}s, LR {optimizer.param_groups[0]['lr']}\")\n",
    "            agg_losses.append(avg_loss)\n",
    "            current_loss = 0.0  # Reset current_loss after logging\n",
    "\n",
    "        if step % 100 == 0 and use_input_optimization:\n",
    "                saved_generators.append({name: param.detach().clone().cpu() for name, param in input_generator.named_parameters()})\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # Test the student model in terms of KL\n",
    "    with torch.no_grad():\n",
    "        student_log_probs = torch.log_softmax(student_mlp(test_examples.to(device)), dim=1)\n",
    "        teacher_log_probs = torch.log_softmax(teacher_mlp(test_examples.to(device)), dim=1)\n",
    "        kl_div = kl_divergence(teacher_log_probs, student_log_probs)\n",
    "    print(f\"KL Divergence: {kl_div.item():.4f}\")\n",
    "\n",
    "    if return_saved_generators and use_input_optimization:\n",
    "        return agg_losses, saved_generators\n",
    "    else:\n",
    "        return agg_losses, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(\n",
    "    num_features=10,\n",
    "    teacher_hidden_sizes=[32, 32],\n",
    "    student_hidden_sizes=[64, 64, 32],\n",
    "    teacher_activation=\"tanh\",\n",
    "    student_activation=\"relu\",\n",
    "    teacher_weight_multiplier=8.0,\n",
    "    num_outputs=2,\n",
    "    num_test_examples=100,\n",
    "    grid_size=3\n",
    "):\n",
    "    # Create teacher and student MLPs\n",
    "    teacher_mlp = create_mlp(input_size=num_features, hidden_sizes=teacher_hidden_sizes, \n",
    "                             activation=teacher_activation, num_outputs=num_outputs, \n",
    "                             weight_multiplier=teacher_weight_multiplier)\n",
    "    teacher_mlp.requires_grad_(False)\n",
    "    \n",
    "    student_mlp = create_mlp(input_size=num_features, hidden_sizes=student_hidden_sizes, \n",
    "                             activation=student_activation, num_outputs=num_outputs)\n",
    "\n",
    "    # Create test examples\n",
    "    test_examples = torch.rand(num_test_examples, num_features) * 2 - 1\n",
    "\n",
    "    # Evaluate teacher_mlp on a grid\n",
    "    grid_points = torch.linspace(-1, 1, grid_size)\n",
    "    grid = torch.stack(torch.meshgrid(*[grid_points for _ in range(num_features)], indexing='ij')).reshape(num_features, -1).t()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = teacher_mlp(grid)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        class_0_probs = probs[:, 0]\n",
    "\n",
    "    min_prob = class_0_probs.min().item()\n",
    "    max_prob = class_0_probs.max().item()\n",
    "\n",
    "    print(f\"Minimum probability for class 0: {min_prob:.4f}\")\n",
    "    print(f\"Maximum probability for class 0: {max_prob:.4f}\")\n",
    "\n",
    "    return teacher_mlp, student_mlp, test_examples\n",
    "\n",
    "# Usage example with default values:\n",
    "num_features = 2\n",
    "teacher_mlp, student_mlp, test_examples = setup_experiment(num_features=num_features)\n",
    "\n",
    "# Or you can override specific parameters:\n",
    "# teacher_mlp, student_mlp, test_examples = setup_experiment(\n",
    "#     num_features=12,\n",
    "#     teacher_hidden_sizes=[64, 64],\n",
    "#     student_hidden_sizes=[128, 128, 64],\n",
    "#     teacher_weight_multiplier=10.0\n",
    "# )\n",
    "\n",
    "def get_student_mlp_copy():\n",
    "    return copy.deepcopy(student_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def visualize_mlp_predictions(mlp1, mlp2=None, resolution=100):\n",
    "    # Create a grid of points\n",
    "    x = np.linspace(-1, 1, resolution)\n",
    "    y = np.linspace(-1, 1, resolution)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Reshape the grid points into a 2D tensor\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    xy_tensor = torch.FloatTensor(xy)\n",
    "    \n",
    "    # Get predictions from the MLPs\n",
    "    with torch.no_grad():\n",
    "        logits1 = mlp1(xy_tensor)\n",
    "        assert logits1.shape[1] == 2\n",
    "        probs1 = torch.softmax(logits1, dim=1)\n",
    "        class_1_probs1 = probs1[:, 0].numpy()\n",
    "        visualize_probs = class_1_probs1\n",
    "        \n",
    "        if mlp2 is not None:\n",
    "            logits2 = mlp2(xy_tensor)\n",
    "            assert logits2.shape[1] == 2\n",
    "            probs2 = torch.softmax(logits2, dim=1)\n",
    "            class_1_probs2 = probs2[:, 0].numpy()\n",
    "            visualize_probs = class_1_probs2 - class_1_probs1\n",
    "    \n",
    "    # Reshape the predictions back to the grid shape\n",
    "    Z = visualize_probs.reshape(X.shape)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(X, Y, Z, levels=20, cmap='viridis')\n",
    "    plt.colorbar(label='Probability of Class 1')\n",
    "    plt.title('MLP Predictions - Probability of Class 1')\n",
    "    plt.xlabel('Input 1')\n",
    "    plt.ylabel('Input 2')\n",
    "    plt.show()\n",
    "\n",
    "visualize_mlp_predictions(teacher_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the copy of student_mlp\n",
    "student_mlp_standard = get_student_mlp_copy()\n",
    "train_student(teacher_mlp, student_mlp_standard, num_features, test_examples, num_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_mlp_adaptive = get_student_mlp_copy()\n",
    "losses, saved_generators = train_student(teacher_mlp, student_mlp_adaptive, num_features, test_examples, num_steps=10000,\n",
    "                                 use_input_optimization=True, gen_learning_rate=0.01, return_saved_generators=True, old_generator_prob=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mlp_predictions(teacher_mlp)\n",
    "visualize_mlp_predictions(student_mlp_adaptive, teacher_mlp)\n",
    "#visualize_mlp_predictions(student_mlp_standard, student_mlp_adaptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the first 10 means\n",
    "first_means = np.array([s['means'][:100,:2].detach().numpy() for s in saved_generators])\n",
    "\n",
    "print(first_means.shape)\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Define a colormap for the paths\n",
    "cmap = plt.get_cmap('viridis')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, first_means.shape[1])]\n",
    "\n",
    "# Plot the paths of the first 10 means\n",
    "for i in range(first_means.shape[1]):\n",
    "    x = first_means[:, i, 0]\n",
    "    y = first_means[:, i, 1]\n",
    "    #ax.plot(x, y, c=colors[i], alpha=0.7, linewidth=2)\n",
    "    #ax.scatter(x[-1], y[-1], c=[colors[i]], s=100, marker='o')  # Highlight the end point\n",
    "    ax.scatter(x,y, c=colors[i], s=10, alpha=.5)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Paths of First 10 Means During Training')\n",
    "\n",
    "# Add a colorbar to show the progression\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=9))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.set_label('Mean Index')\n",
    "\n",
    "# Set equal aspect ratio and limit the plot to [-1, 1] on both axes\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Extract the first 10 stds\n",
    "first_10_stds = np.array([s['log_stds'][:100,0].detach().numpy() for s in saved_generators])\n",
    "\n",
    "sns.lineplot(data=first_10_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    num_repeats = 10,\n",
    "    num_features=10,\n",
    "    \n",
    "    # Standard training parameters\n",
    "    num_steps=10000,\n",
    "    learning_rate=0.001,\n",
    "    \n",
    "    # Adaptive training parameters\n",
    "    adaptive_gen_learning_rate=0.01,\n",
    "    adaptive_old_generator_prob=0.5,\n",
    "    apply_scheduler_to_gen=False,\n",
    "\n",
    "    # Experiment setup parameters\n",
    "    **setup_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup args:\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    all_losses = []\n",
    "    for repeat_ind in range(num_repeats):\n",
    "        # Setup the experiment\n",
    "        teacher_mlp, student_mlp, test_examples = setup_experiment(num_features=num_features, **setup_kwargs)\n",
    "    \n",
    "        # Train standard student\n",
    "        print(\"Training standard student:\")\n",
    "        student_mlp_standard = copy.deepcopy(student_mlp)\n",
    "        losses_default, _ = train_student(teacher_mlp, student_mlp_standard, num_features, test_examples,\n",
    "                      num_steps=num_steps, \n",
    "                      learning_rate=learning_rate)\n",
    "    \n",
    "        # Train adaptive student\n",
    "        print(\"\\nTraining adaptive student:\")\n",
    "        student_mlp_adaptive = copy.deepcopy(student_mlp)\n",
    "        losses_adaptive, _ = train_student(teacher_mlp, student_mlp_adaptive, num_features, test_examples,\n",
    "                                         num_steps=num_steps, \n",
    "                                         learning_rate=learning_rate,\n",
    "                                         use_input_optimization=True,\n",
    "                                         gen_learning_rate=adaptive_gen_learning_rate,\n",
    "                                         old_generator_prob=adaptive_old_generator_prob,\n",
    "                                         apply_scheduler_to_gen=apply_scheduler_to_gen,\n",
    "                                         return_saved_generators=False)\n",
    "        all_losses += [(losses_default, losses_adaptive)]\n",
    "\n",
    "        \n",
    "    return all_losses\n",
    "    \n",
    "# Example usage:\n",
    "exp_result = run_experiment(\n",
    "    num_features=2,\n",
    "    teacher_hidden_sizes=[32, 32],\n",
    "    student_hidden_sizes=[64, 64, 32],\n",
    "    teacher_activation=\"tanh\",\n",
    "    student_activation=\"relu\",\n",
    "    teacher_weight_multiplier=8.0,\n",
    "    num_outputs=2,\n",
    "    num_steps=10,\n",
    "    num_test_examples=1000,\n",
    "    grid_size=3,\n",
    "    learning_rate=0.001,\n",
    "    adaptive_gen_learning_rate=0.0001,\n",
    "    adaptive_old_generator_prob=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfns.utils import product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_learning1 = ex_bosch_cpu.submit_group('adaptive_learning1', run_experiment, list(product_dict({\n",
    "    'num_features': [2,10],\n",
    "    'teacher_activation': ['tanh', 'relu'],\n",
    "    'teacher_weight_multiplier': [1.,2.,4.,8.],\n",
    "    'num_steps': [1_000, 10_000, 100_000,],\n",
    "    'adaptive_gen_learning_rate': [.1,.01,.001,.0001],\n",
    "    'learning_rate': [.003,.001],\n",
    "    'adaptive_old_generator_prob': [0.,.5,1.],\n",
    "    'apply_scheduler_to_gen': [False,True]\n",
    "})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(adaptive_learning1:=ex.get_group('adaptive_learning1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for j in adaptive_learning1:\n",
    "    if j.done():\n",
    "        result = j.result() # a list of tuples each containing a list of default and adaptive losses\n",
    "        for i, result_type in enumerate(['default', 'adaptive']):\n",
    "            results.append({\n",
    "                'loss': sum([seed_result[i][-1] for seed_result in result])/len(result),\n",
    "                'type': result_type,\n",
    "                **j.config\n",
    "            })\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "filtered_df = results_df[(results_df['num_features'] == 10) & (results_df.type != 'sadaptive') & \n",
    "                         (results_df.adaptive_old_generator_prob == 1.) &\n",
    "                         (results_df.adaptive_gen_learning_rate == .01) &\n",
    "                         (results_df.learning_rate == .001) & (results_df.teacher_activation == 'relu') &\n",
    "                         (results_df.teacher_weight_multiplier == 8.) & (results_df.apply_scheduler_to_gen)\n",
    "]\n",
    "\n",
    "sns.lineplot(data=filtered_df, x='num_steps', y='loss', style='type', hue='adaptive_gen_learning_rate')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.where(torch.tensor([[0,1],[0,1]]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "from speechless.onnx import PyannoteONNX\n",
    "from speechless.transcribe_audio import transcribe_audio\n",
    "from speechless.speaker_diarization import merge_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = '../data/audio/sk_slsp_chalupa.mp3'\n",
    "PATH_TO_ONNX_MODEL = '../pyannote-onnx/segmentation-3.0.onnx'\n",
    "OVERLAP = 0.7\n",
    "NUM_SPEAKERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_diarization(\n",
    "    audio_path: str,\n",
    "    num_speakers: int = 2,\n",
    "    sample_rate: int = 16000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform speaker diarization using PyannoteONNX and optionally plot VAD probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_path : str\n",
    "        Path to the input audio file.\n",
    "    plot : bool, default=False\n",
    "        Whether to plot the VAD probabilities.\n",
    "    sample_rate : int, default=16000\n",
    "        The sample rate to load the audio with, matching the model's expected rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Initialize the PyannoteONNX model\n",
    "    pyannote = PyannoteONNX(\n",
    "        num_speakers,\n",
    "        sample_rate,\n",
    "        PATH_TO_ONNX_MODEL,\n",
    "    )\n",
    "    output = []\n",
    "    print(\"Model loaded with PyannoteONNX.\")\n",
    "\n",
    "    # Load the audio file as a waveform\n",
    "    wav, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    print(f\"Audio loaded with sample rate: {sr}\")\n",
    "\n",
    "    # Perform diarization and print each detected segment\n",
    "    print(\"Performing diarization...\")\n",
    "    for turn in pyannote.itertracks(wav):\n",
    "        output.append(turn)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization_result = perform_diarization(\n",
    "    audio_path=AUDIO_PATH,\n",
    "    num_speakers=NUM_SPEAKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcribe_audio(\n",
    "    AUDIO_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_transcript = merge_transcript(\n",
    "    transcript,\n",
    "    diarization_result,\n",
    "    OVERLAP\n",
    ")\n",
    "\n",
    "merged_transcript"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechless-ncOQqsHn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

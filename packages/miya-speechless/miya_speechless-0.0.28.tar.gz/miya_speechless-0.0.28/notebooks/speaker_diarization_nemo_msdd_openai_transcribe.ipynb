{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef71c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "import tempfile\n",
    "import librosa\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
    "\n",
    "from ctc_forced_aligner import (\n",
    "    generate_emissions,\n",
    "    get_alignments,\n",
    "    get_spans,\n",
    "    load_alignment_model,\n",
    "    postprocess_results,\n",
    "    preprocess_text,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    create_config,\n",
    "    process_language_arg,\n",
    "    get_realigned_ws_mapping_with_punctuation,\n",
    "    get_sentences_speaker_mapping,\n",
    "    get_speaker_aware_transcript,\n",
    "    get_words_speaker_mapping,\n",
    "    langs_to_iso,\n",
    "    process_language_arg,\n",
    "    punct_model_langs,\n",
    "    write_srt,\n",
    ")\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "# Notebook params (replicates argparse inputs)\n",
    "AUDIO_PATH = \"../data/audio/call_1.mp3\"\n",
    "STREAMING = True                  \n",
    "SUPPRESS_NUMERALS = False      \n",
    "MODEL_NAME = \"medium\"        \n",
    "BATCH_SIZE = 8                   \n",
    "LANGUAGE = None                 \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEMP_PATH = \"temp_outputs\"\n",
    "ALIGN_SR = 16000\n",
    "\n",
    "# precision map\n",
    "mtypes = {\"cpu\": \"int8\", \"cuda\": \"float16\"}\n",
    "\n",
    "# normalize/validate language same as script\n",
    "LANGUAGE = process_language_arg(LANGUAGE, MODEL_NAME)\n",
    "\n",
    "print(f\"device={DEVICE}  model={MODEL_NAME}  batch_size={BATCH_SIZE}  language={LANGUAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"sk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STREAMING:\n",
    "    # Isolate vocals from the rest of the audio\n",
    "    return_code = os.system(\n",
    "        f'python -m demucs.separate -n htdemucs --two-stems=vocals \"{AUDIO_PATH}\" -o temp_outputs'\n",
    "    )\n",
    "\n",
    "    if return_code != 0:\n",
    "        logging.warning(\n",
    "            \"Source splitting failed, using original audio file. \"\n",
    "            \"Use stemming=False to skip this step.\"\n",
    "        )\n",
    "        vocal_target = AUDIO_PATH\n",
    "    else:\n",
    "        vocal_target = os.path.join(\n",
    "            \"temp_outputs\",\n",
    "            \"htdemucs\",\n",
    "            os.path.splitext(os.path.basename(AUDIO_PATH))[0],\n",
    "            \"vocals.wav\",\n",
    "        )\n",
    "else:\n",
    "    vocal_target = AUDIO_PATH\n",
    "\n",
    "print(\"vocal_target:\", vocal_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def _as_audio_file(vocal_target) -> str:\n",
    "    \"\"\"\n",
    "    Accepts either a file path (str) or a NumPy float waveform (mono).\n",
    "    Returns a filesystem path to a temporary WAV if needed.\n",
    "    \"\"\"\n",
    "    if isinstance(vocal_target, str) and os.path.exists(vocal_target):\n",
    "        return vocal_target\n",
    "\n",
    "    # If it's a numpy array, write a temp WAV (16 kHz mono PCM16)\n",
    "    if isinstance(vocal_target, np.ndarray):\n",
    "        import soundfile as sf  # pip install soundfile\n",
    "        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "        # assume float32/-1..1; resample if you need a specific rate\n",
    "        sf.write(tmp.name, vocal_target, 16000, subtype=\"PCM_16\")\n",
    "        tmp.close()\n",
    "        return tmp.name\n",
    "\n",
    "    raise ValueError(\"vocal_target must be a path or a NumPy waveform.\")\n",
    "\n",
    "audio_path = _as_audio_file(vocal_target)\n",
    "\n",
    "with open(audio_path, \"rb\") as f:\n",
    "    resp = client.audio.transcriptions.create(\n",
    "        model=\"gpt-4o-transcribe\",   # or \"gpt-4o-mini-transcribe\"\n",
    "        file=f,\n",
    "        language=language or None,   # omit to auto-detect\n",
    "    )\n",
    "\n",
    "full_transcript = getattr(resp, \"text\", \"\") or \"\"\n",
    "detected_language: Optional[str] = getattr(resp, \"language\", None)\n",
    "\n",
    "print(\"Detected language:\", detected_language or \"(unknown)\")\n",
    "print(\"Transcript preview:\", full_transcript[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_waveform, sr = librosa.load(audio_path, sr=ALIGN_SR, mono=True)\n",
    "audio_waveform = audio_waveform.astype(np.float32)  \n",
    "\n",
    "info = SimpleNamespace(language=(detected_language or language or \"en\"))\n",
    "info.language = info.language.split(\"-\")[0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "alignment_model, alignment_tokenizer = load_alignment_model(\n",
    "    DEVICE, dtype=dtype\n",
    ")\n",
    "\n",
    "# safer device resolution than .device on Module\n",
    "align_dev = getattr(alignment_model, \"device\", next(alignment_model.parameters()).device)\n",
    "\n",
    "emissions, stride = generate_emissions(\n",
    "    alignment_model,\n",
    "    torch.from_numpy(audio_waveform).to(dtype).to(align_dev),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "del alignment_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokens_starred, text_starred = preprocess_text(\n",
    "    full_transcript,\n",
    "    romanize=True,\n",
    "    language=langs_to_iso[info.language],\n",
    ")\n",
    "\n",
    "segments, scores, blank_token = get_alignments(\n",
    "    emissions,\n",
    "    tokens_starred,\n",
    "    alignment_tokenizer,\n",
    ")\n",
    "\n",
    "spans = get_spans(tokens_starred, segments, blank_token)\n",
    "\n",
    "word_timestamps = postprocess_results(text_starred, spans, stride, scores)\n",
    "print(\"Aligned words:\", len(word_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36096de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TEMP_PATH, exist_ok=True)\n",
    "\n",
    "mono_wav = os.path.join(TEMP_PATH, \"mono_file.wav\")\n",
    "if not os.path.exists(mono_wav):\n",
    "    torchaudio.save(mono_wav, torch.from_numpy(audio_waveform).unsqueeze(0).float(), 16000)\n",
    "\n",
    "cfg = create_config(TEMP_PATH)\n",
    "msdd = NeuralDiarizer(cfg=cfg).to(DEVICE)\n",
    "msdd.diarize()\n",
    "del msdd; torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38708a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"word_timestamps\" in globals(), \"word_timestamps not defined (run alignment cell).\"\n",
    "assert \"info\" in globals(), \"info (from transcription) not defined.\"\n",
    "assert \"audio_waveform\" in globals(), \"audio_waveform not defined.\"\n",
    "assert \"audio_path\" in globals(), \"audio_path not set.\"\n",
    "\n",
    "# Ensure temp dir + mono wav exist\n",
    "ROOT = os.getcwd()\n",
    "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
    "mono_wav = os.path.join(temp_path, \"mono_file.wav\")\n",
    "assert os.path.exists(mono_wav), f\"Expected mono wav at {mono_wav}\"\n",
    "\n",
    "# Parse RTTM to speaker_ts\n",
    "rttm_path = os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\")\n",
    "if not os.path.exists(rttm_path):\n",
    "    raise FileNotFoundError(f\"RTTM not found at {rttm_path}. Re-run diarization and check create_config(out_dir).\")\n",
    "\n",
    "speaker_ts = []\n",
    "with open(rttm_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip() or line.startswith(\"#\"):\n",
    "            continue\n",
    "        # Keep indices exactly like your script:\n",
    "        line_list = line.split(\" \")\n",
    "        s = int(float(line_list[5]) * 1000)\n",
    "        e = s + int(float(line_list[8]) * 1000)\n",
    "        speaker_id = int(line_list[11].split(\"_\")[-1])\n",
    "        if e > s:\n",
    "            speaker_ts.append([s, e, speaker_id])\n",
    "\n",
    "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
    "\n",
    "def chunked(seq, n):\n",
    "    it = iter(seq)\n",
    "    while True:\n",
    "        chunk = list(islice(it, n))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "if info.language in punct_model_langs:\n",
    "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
    "    words_list = [x[\"word\"] for x in wsm]\n",
    "\n",
    "    labeled_words = []\n",
    "    for chunk in chunked(words_list, 220):\n",
    "        labeled_words.extend(punct_model.predict(chunk))  # returns (token, punct, score)\n",
    "\n",
    "    ending_puncts = \".?!\"\n",
    "    model_puncts = \".,;:!?\"\n",
    "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
    "\n",
    "    for word_dict, labeled in zip(wsm, labeled_words):\n",
    "        word = word_dict[\"word\"]\n",
    "        punct = labeled[1] if isinstance(labeled, (list, tuple)) and len(labeled) > 1 else \"\"\n",
    "        if (\n",
    "            word\n",
    "            and punct in ending_puncts\n",
    "            and (word[-1] not in model_puncts or is_acronym(word))\n",
    "        ):\n",
    "            word += punct\n",
    "            if word.endswith(\"..\"):\n",
    "                word = word.rstrip(\".\")\n",
    "            word_dict[\"word\"] = word\n",
    "else:\n",
    "    logging.warning(\n",
    "        f\"Punctuation restoration is not available for {info.language} language. Using the original punctuation.\"\n",
    "    )\n",
    "\n",
    "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
    "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
    "\n",
    "base = os.path.splitext(audio_path)[0]\n",
    "txt_out = Path(f\"{base}.txt\")\n",
    "srt_out = Path(f\"{base}.srt\")\n",
    "\n",
    "with open(txt_out, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    get_speaker_aware_transcript(ssm, f)\n",
    "\n",
    "with open(srt_out, \"w\", encoding=\"utf-8-sig\") as srt:\n",
    "    write_srt(ssm, srt)\n",
    "\n",
    "print(f\"Done\\n Transcript: {txt_out}\\n - Subtitles:  {srt_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c036d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miya-speechless",
   "language": "python",
   "name": "miya-speechless"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

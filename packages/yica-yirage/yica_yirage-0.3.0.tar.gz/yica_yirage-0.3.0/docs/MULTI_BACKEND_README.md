# YiRage å¤šåç«¯æ”¯æŒ

YiRageç°åœ¨æ”¯æŒå¤šç§è®¡ç®—åç«¯ï¼Œä¸ºä¸åŒç¡¬ä»¶ç¯å¢ƒæä¾›ä¼˜åŒ–çš„æ€§èƒ½ã€‚

## æ”¯æŒçš„åç«¯

### ğŸ–¥ï¸ CPU åç«¯
- **é€‚ç”¨åœºæ™¯**: æ— GPUç¯å¢ƒã€å¼€å‘è°ƒè¯•ã€å°æ¨¡å‹æ¨ç†
- **ä¼˜åŒ–ç‰¹æ€§**: 
  - OpenMPå¹¶è¡ŒåŒ–
  - SIMDæŒ‡ä»¤ä¼˜åŒ–
  - é«˜æ•ˆå†…å­˜ç®¡ç†
  - BLASåº“é›†æˆ

### ğŸš€ CUDA åç«¯
- **é€‚ç”¨åœºæ™¯**: NVIDIA GPUåŠ é€Ÿ
- **ä¼˜åŒ–ç‰¹æ€§**:
  - åŸç”ŸCUDAå†…æ ¸
  - cuBLASä¼˜åŒ–
  - å¤šGPUæ”¯æŒ
  - å†…å­˜æ± ç®¡ç†

### ğŸ MPS åç«¯
- **é€‚ç”¨åœºæ™¯**: Apple Silicon (M1/M2/M3) ä¼˜åŒ–
- **ä¼˜åŒ–ç‰¹æ€§**:
  - Metal Performance Shaders
  - ç»Ÿä¸€å†…å­˜æ¶æ„
  - Apple GPUä¼˜åŒ–
  - ä½åŠŸè€—æ¨ç†

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
import yirage as yr

# æŸ¥çœ‹å¯ç”¨åç«¯
print("Available backends:", [b.value for b in yr.get_available_backends()])

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³åç«¯
yr.set_backend('auto')
print("Current backend:", yr.get_backend().value)

# æ‰‹åŠ¨é€‰æ‹©åç«¯
yr.set_backend('cuda')  # æˆ– 'cpu', 'mps'
```

### åˆ›å»ºåç«¯ç‰¹å®šçš„å›¾

```python
# ä¸ºä¸åŒåç«¯åˆ›å»ºå›¾
cuda_graph = yr.new_kernel_graph(backend='cuda')
cpu_graph = yr.new_kernel_graph(backend='cpu')
mps_graph = yr.new_kernel_graph(backend='mps')
```

### PersistentKernelå¤šåç«¯æ”¯æŒ

```python
mpk = yr.PersistentKernel(
    world_size=1,
    mpi_rank=0,
    num_workers=32,
    num_local_schedulers=16,
    num_remote_schedulers=0,
    max_seq_length=1024,
    eos_token_id=2,
    meta_tensors=[step, tokens],
    profiler_tensor=None,
    spec_decode_config=None,
    backend='mps'  # æŒ‡å®šåç«¯
)
```

## å‘½ä»¤è¡Œå·¥å…·

### åç«¯ç®¡ç†å™¨

```bash
# æŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯å’Œå¯ç”¨åç«¯
python tools/yirage_backend_manager.py info

# è‡ªåŠ¨ä¼˜åŒ–é…ç½®
python tools/yirage_backend_manager.py optimize --backend cuda --apply

# æ€§èƒ½åŸºå‡†æµ‹è¯•
python tools/yirage_backend_manager.py benchmark --duration 30

# è®¾ç½®å½“å‰åç«¯
python tools/yirage_backend_manager.py set cuda

# æµ‹è¯•åç«¯åŠŸèƒ½
python tools/yirage_backend_manager.py test cpu
```

### å¤šåç«¯æ¼”ç¤º

```bash
# è¿è¡Œå¤šåç«¯æ¼”ç¤º
python demo/demo_multi_backend.py --backend all --iterations 20

# ä»…æµ‹è¯•CPUåç«¯
python demo/demo_multi_backend.py --backend cpu

# è·³è¿‡PersistentKernelæ¼”ç¤º
python demo/demo_multi_backend.py --skip-persistent
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•
python benchmark/multi_backend_benchmark.py --iterations 50 --output results.json

# è‡ªå®šä¹‰æµ‹è¯•é…ç½®
python benchmark/multi_backend_benchmark.py \
    --batch-sizes 1 4 8 \
    --seq-lengths 128 512 1024 \
    --hidden-sizes 768 1024 \
    --dtype float16
```

## ç¯å¢ƒé…ç½®

### ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®é»˜è®¤åç«¯
export YIRAGE_BACKEND=cuda
export YIRAGE_BACKEND=cpu
export YIRAGE_BACKEND=mps
export YIRAGE_BACKEND=auto

# è®¾ç½®æ—¥å¿—çº§åˆ«
export YIRAGE_LOG_LEVEL=DEBUG
```

### ç¼–è¯‘é€‰é¡¹

```bash
# é€‰æ‹©è¦ç¼–è¯‘çš„åç«¯
cmake -DYIRAGE_USE_CUDA=ON \
      -DYIRAGE_USE_CPU=ON \
      -DYIRAGE_USE_MPS=ON \
      -DCMAKE_BUILD_TYPE=Release \
      ..
```

## é«˜çº§åŠŸèƒ½

### è‡ªåŠ¨é…ç½®ä¼˜åŒ–

```python
# è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜é…ç½®
config = yr.auto_configure_backend()
print("Optimal config:", config)

# ç³»ç»Ÿä¿¡æ¯åˆ†æ
optimizer = yr.BackendOptimizer()
optimizer.print_system_info()
```

### æ€§èƒ½ç›‘æ§

```python
# è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯
memory_info = yr.get_memory_info('cuda')
print("Memory usage:", memory_info)

# å¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
results = yr.benchmark_backends(duration_seconds=10)
print("Performance results:", results)
```

### é…ç½®ç®¡ç†

```python
# ä¿å­˜ä¼˜åŒ–é…ç½®
optimizer = yr.BackendOptimizer()
optimizer.save_config('optimal_config.json', backend='cuda')

# åŠ è½½é…ç½®
config = optimizer.load_config('optimal_config.json')
```

## æ€§èƒ½è°ƒä¼˜æŒ‡å—

### CUDAåç«¯ä¼˜åŒ–

```python
# é«˜æ€§èƒ½GPU (>= 24GB)
yr.set_backend('cuda')
mpk = yr.PersistentKernel(
    num_workers=96,
    num_local_schedulers=48,
    # ... å…¶ä»–å‚æ•°
)
```

### CPUåç«¯ä¼˜åŒ–

```python
import os

# è®¾ç½®OpenMPçº¿ç¨‹æ•°
os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())

yr.set_backend('cpu')
mpk = yr.PersistentKernel(
    num_workers=8,
    num_local_schedulers=4,
    # ... å…¶ä»–å‚æ•°
)
```

### MPSåç«¯ä¼˜åŒ–

```python
# Apple Siliconä¼˜åŒ–
yr.set_backend('mps')
mpk = yr.PersistentKernel(
    num_workers=64,
    num_local_schedulers=32,
    # ... å…¶ä»–å‚æ•°
)
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **åç«¯ä¸å¯ç”¨**
   ```python
   # æ£€æŸ¥å¯ç”¨åç«¯
   available = yr.get_available_backends()
   print(f"Available: {[b.value for b in available]}")
   
   # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©
   yr.set_backend('auto')
   ```

2. **æ€§èƒ½ä¸ä½³**
   ```bash
   # è¿è¡Œç³»ç»Ÿåˆ†æ
   python tools/yirage_backend_manager.py info
   
   # è·å–ä¼˜åŒ–å»ºè®®
   python tools/yirage_backend_manager.py optimize --backend auto
   ```

3. **å†…å­˜ä¸è¶³**
   ```python
   # æ£€æŸ¥å†…å­˜ä½¿ç”¨
   memory_info = yr.get_memory_info(yr.get_backend().value)
   print("Memory status:", memory_info)
   
   # å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–å·¥ä½œçº¿ç¨‹æ•°
   ```

### è°ƒè¯•æ¨¡å¼

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
import os
os.environ['YIRAGE_LOG_LEVEL'] = 'DEBUG'
```

### æµ‹è¯•åç«¯åŠŸèƒ½

```bash
# è¿è¡Œåç«¯æµ‹è¯•å¥—ä»¶
python tests/test_multi_backend.py

# æµ‹è¯•ç‰¹å®šåç«¯
python tools/yirage_backend_manager.py test cuda
```

## ç¤ºä¾‹é¡¹ç›®

### ç®€å•çš„LLMæ¨ç†

```python
import yirage as yr
import torch

# è‡ªåŠ¨é…ç½®
yr.set_backend('auto')
print(f"Using backend: {yr.get_backend().value}")

# åˆ›å»ºæ¨¡æ‹Ÿçš„LLMç»„ä»¶
batch_size, seq_len, hidden_size = 1, 128, 768

# è¾“å…¥æ•°æ®
input_ids = torch.randint(0, 32000, (batch_size, seq_len))
embeddings = torch.randn(batch_size, seq_len, hidden_size)

# ç®€å•çš„å‰å‘ä¼ æ’­
def simple_forward(x):
    # Layer norm
    x = torch.layer_norm(x, (hidden_size,))
    
    # Linear transformation
    weight = torch.randn(hidden_size, hidden_size)
    x = torch.matmul(x, weight)
    
    # Activation
    x = torch.relu(x)
    
    return x

# æ‰§è¡Œæ¨ç†
with torch.no_grad():
    output = simple_forward(embeddings)
    print(f"Output shape: {output.shape}")
    print(f"Backend used: {yr.get_backend().value}")
```

### å¤šåç«¯æ€§èƒ½æ¯”è¾ƒ

```python
import yirage as yr
import time
import torch

def benchmark_operation(backend, operation_name, operation_func, *args):
    """åŸºå‡†æµ‹è¯•ç‰¹å®šæ“ä½œ"""
    yr.set_backend(backend)
    
    # çƒ­èº«
    for _ in range(5):
        result = operation_func(*args)
    
    # è®¡æ—¶
    start_time = time.time()
    for _ in range(100):
        result = operation_func(*args)
    end_time = time.time()
    
    avg_time = (end_time - start_time) / 100 * 1000  # ms
    return avg_time

# æµ‹è¯•æ“ä½œ
def matmul_op(a, b):
    return torch.matmul(a, b)

# æ¯”è¾ƒæ‰€æœ‰åç«¯
backends = [b.value for b in yr.get_available_backends()]
a = torch.randn(1024, 1024)
b = torch.randn(1024, 1024)

results = {}
for backend in backends:
    try:
        time_ms = benchmark_operation(backend, 'matmul', matmul_op, a, b)
        results[backend] = time_ms
        print(f"{backend}: {time_ms:.2f} ms")
    except Exception as e:
        print(f"{backend}: Error - {e}")

# æ˜¾ç¤ºæœ€ä½³åç«¯
if results:
    best_backend = min(results, key=results.get)
    print(f"\nBest backend: {best_backend} ({results[best_backend]:.2f} ms)")
```

## è´¡çŒ®æŒ‡å—

### æ·»åŠ æ–°åç«¯

1. åˆ›å»ºåç«¯æ¥å£å®ç°
2. å®ç°å†…æ ¸æ¥å£
3. æ·»åŠ åˆ°å·¥å‚æ–¹æ³•
4. æ›´æ–°CMakeé…ç½®
5. æ·»åŠ æµ‹è¯•ç”¨ä¾‹

### æ€§èƒ½ä¼˜åŒ–

1. åˆ†ææ€§èƒ½ç“¶é¢ˆ
2. å®ç°ä¼˜åŒ–å†…æ ¸
3. æ·»åŠ åŸºå‡†æµ‹è¯•
4. éªŒè¯æ­£ç¡®æ€§

## è®¸å¯è¯

æœ¬é¡¹ç›®ä½¿ç”¨Apache License 2.0è®¸å¯è¯ã€‚è¯¦è§LICENSEæ–‡ä»¶ã€‚

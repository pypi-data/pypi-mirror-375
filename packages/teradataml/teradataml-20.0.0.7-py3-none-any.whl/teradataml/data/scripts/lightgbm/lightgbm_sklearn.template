import sys, json
import pickle, base64, importlib, numpy as np
from collections import OrderedDict

DELIMITER = '\t'

func_name = <func_name>
params = json.loads('<params>')
is_lake_system = <is_lake_system>
model_file_prefix = <model_file_prefix>

def convert_to_type(val, typee):
    if typee == 'int':
        return int(val) if val != "" else np.nan
    if typee == 'float':
        if isinstance(val, str):
            val = val.replace(' ', '')
        return float(val) if val != "" else np.nan
    if typee == 'bool':
        return eval(val) if val != "" else None
    return str(val) if val != "" else None

def splitter(strr, delim=",", convert_to="str"):
    """
    Split the string based on delimiter and convert to the type specified.
    """
    if strr == "None":
        return []
    return [convert_to_type(i, convert_to) for i in strr.split(delim)]


if not is_lake_system:
    db = sys.argv[0].split("/")[1]

data_partition_column_indices = <partition_cols_indices>
data_column_types = <types_of_data_cols>

data_partition_column_types = [data_column_types[idx] for idx in data_partition_column_indices]

# Data related arguments information of indices and types.
data_args_indices_types = OrderedDict()

# Data related arguments values - prepare dictionary and populate data later.
data_args_values = {}

data_args_info_str = <data_args_info_str>
for data_arg in data_args_info_str.split("--"):
    arg_name, indices, types = data_arg.split("-")
    indices = splitter(indices, convert_to="int")
    types = splitter(types)

    data_args_indices_types[arg_name] = {"indices": indices, "types": types}
    data_args_values[arg_name] = [] # Keeping empty for each data arg name and populate data later.

data_partition_column_values = []
data_present = False

model = None

# Read data - columns information is passed as command line argument and stored in
# data_args_indices_types dictionary. 
while 1:
    try:
        line = input()
        if line == '':  # Exit if user provides blank line
            break
        else:
            data_present = True
            values = line.split(DELIMITER)
            if not data_partition_column_values:
                # Partition column values is same for all rows. Hence, only read once.
                for i, val in enumerate(data_partition_column_indices):
                    data_partition_column_values.append(
                        convert_to_type(values[val], typee=data_partition_column_types[i])
                        )

                # Prepare the corresponding model file name and extract model.
                partition_join = "_".join([str(x) for x in data_partition_column_values])
                # Replace '-' with '_' as '-' because partition_columns can be negative.
                partition_join = partition_join.replace("-", "_")


                model_file_path = f"{model_file_prefix}_{partition_join}"\
                    if is_lake_system else \
                    f"./{db}/{model_file_prefix}_{partition_join}"

                with open(model_file_path, "rb") as fp:
                    model = pickle.loads(fp.read())

                if model is None:
                    sys.exit("Model file is not installed in Vantage.")

            # Prepare data dictionary containing only arguments related to data.
            for arg_name in data_args_values:
                data_indices = data_args_indices_types[arg_name]["indices"]
                types = data_args_indices_types[arg_name]["types"]
                cur_row = []
                for idx, data_idx in enumerate(data_indices):
                    cur_row.append(convert_to_type(values[data_idx], types[idx]))
                data_args_values[arg_name].append(cur_row)
    except EOFError:  # Exit if reached EOF or CTRL-D
        break

if not data_present:
    sys.exit(0)

# Handle callbacks.
rec_eval = None
if "callbacks" in params and params["callbacks"] is not None:
    callbacks = params["callbacks"]
    callbacks = [callbacks] if not isinstance(callbacks, list) else callbacks
    for i, callback in enumerate(callbacks):
        c_module_name = callback["module"]
        c_func_name = callback["func_name"]
        c_kwargs = callback["kwargs"]
        c_module = importlib.import_module(c_module_name)
        if c_func_name == "record_evaluation":
            # record_evaluation function takes empty dict. If the argument has elements in the
            # dict, they will be deleted as per the documentation from lightgbm as described below:
            # eval_result (dict) -
            #   Dictionary used to store all evaluation results of all validation sets. This should
            #   be initialized outside of your call to record_evaluation() and should be empty. Any
            #   initial contents of the dictionary will be deleted.
            rec_eval = {}
            callbacks[i] = getattr(c_module, c_func_name)(rec_eval)
        else:
            callbacks[i] = getattr(c_module, c_func_name)(**c_kwargs)
    
    params["callbacks"] = callbacks

# Update data as numpy arrays.
for arg_name in data_args_values:
    np_values = np.array(data_args_values[arg_name])
    data_args_values[arg_name] = np_values
    if arg_name == "sample_weight":
        data_args_values[arg_name] = np_values.ravel()

# Combine all arguments.
all_args = {**data_args_values, **params}

trained_model = getattr(model, func_name)(**all_args)

model_data = 0
if func_name == "fit":
    model_str = pickle.dumps(trained_model)

    if is_lake_system:
        model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"

    # Write to file in Vantage, to be used in predict/scoring.
    with open(model_file_path, "wb") as fp:
        fp.write(model_str)

    model_data = model_file_path if is_lake_system else base64.b64encode(model_str)

elif func_name == "score":
    model_data = trained_model

print(*(data_partition_column_values + [model_data]), sep=DELIMITER)

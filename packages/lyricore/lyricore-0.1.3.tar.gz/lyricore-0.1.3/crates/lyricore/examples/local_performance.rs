use futures::stream::{FuturesUnordered, StreamExt};
use lyricore::serialization::SerializationStrategy;
use lyricore::{ActorContext, Message, SchedulerConfig};
use serde::{Deserialize, Serialize};
use std::time::{Duration, Instant};

struct TestActor {
    count: usize,
}

#[derive(Clone, Debug, Serialize, Deserialize)]
struct TestMessage {
    content: String,
}

impl Message for TestMessage {
    type Response = String;
}

#[async_trait::async_trait]
impl lyricore::Actor for TestActor {
    type Message = TestMessage;
    type State = ();

    async fn on_start(&mut self, _ctx: &mut ActorContext) -> lyricore::error::Result<()> {
        Ok(())
    }

    async fn on_message(
        &mut self,
        msg: Self::Message,
        _ctx: &mut ActorContext,
    ) -> lyricore::error::Result<()> {
        self.count += 1;
        Ok(())
    }

    async fn handle_message(
        &mut self,
        msg: Self::Message,
        ctx: &mut ActorContext,
    ) -> lyricore::error::Result<String> {
        self.on_message(msg.clone(), ctx).await?;
        Ok(format!("Response to: {}", msg.content))
    }

    async fn on_stop(&mut self, _ctx: &mut ActorContext) -> lyricore::error::Result<()> {
        Ok(())
    }
}

struct PerformanceStats {
    total_messages: usize,
    total_duration: Duration,
    qps: f64,
    avg_latency_ms: f64,
    p95_latency_ms: f64,
    p99_latency_ms: f64,
    cpu_efficiency: f64,
}

impl PerformanceStats {
    fn new(
        total_messages: usize,
        total_successful: usize,
        total_duration: Duration,
        latencies: &mut [Duration],
    ) -> Self {
        latencies.sort();

        let qps = total_successful as f64 / total_duration.as_secs_f64();
        let avg_latency_ms = latencies
            .iter()
            .map(|d| d.as_secs_f64() * 1000.0)
            .sum::<f64>()
            / latencies.len() as f64;

        let success_rate = total_successful as f64 / total_messages as f64 * 100.0;
        println!("âœ… Success Rate: {:.2}%", success_rate);

        let p95_index = (latencies.len() as f64 * 0.95) as usize;
        let p99_index = (latencies.len() as f64 * 0.99) as usize;
        let p95_latency_ms = latencies[p95_index.min(latencies.len() - 1)].as_secs_f64() * 1000.0;
        let p99_latency_ms = latencies[p99_index.min(latencies.len() - 1)].as_secs_f64() * 1000.0;

        // CPU efficiency: QPS per second per core (assuming 8 cores)
        let cpu_efficiency = qps / 8.0;

        Self {
            total_messages,
            total_duration,
            qps,
            avg_latency_ms,
            p95_latency_ms,
            p99_latency_ms,
            cpu_efficiency,
        }
    }

    fn print_analysis(&self, test_name: &str) {
        println!("=== {} Analysis ===", test_name);
        println!("ğŸš€ QPS: {:.2}", self.qps);
        println!("âš¡ Avg Latency: {:.3}ms", self.avg_latency_ms);
        println!("ğŸ“Š P95 Latency: {:.3}ms", self.p95_latency_ms);
        println!("ğŸ¯ CPU Efficiency: {:.2} QPS/core", self.cpu_efficiency);
        println!("â±ï¸  Duration: {:.2}s", self.total_duration.as_secs_f64());

        if self.qps > 900000.0 {
            println!("ğŸ† Performance Grade: EXCELLENT");
        } else if self.qps > 700000.0 {
            println!("ğŸ¥ˆ Performance Grade: GOOD");
        } else if self.qps > 500000.0 {
            println!("ğŸ¥‰ Performance Grade: FAIR");
        } else {
            println!("ğŸ“‰ Performance Grade: NEEDS IMPROVEMENT");
        }
        println!("================================\n");
    }
}

async fn run_optimal_streaming(
    actor_count: usize,
    messages_per_actor: usize,
    concurrency_limit: usize,
    wait_reply: bool,
) -> lyricore::error::Result<PerformanceStats> {
    println!("ğŸ† Running OPTIMAL STREAMING test...");

    let strategy = SerializationStrategy::fast_local();
    let mut system = lyricore::ActorSystem::new(
        "optimal_node".to_string(),
        "127.0.0.1:50070".to_string(),
        SchedulerConfig::default(),
        Some(strategy),
    )?;

    system.start_server().await?;

    let mut actors = Vec::with_capacity(actor_count);
    for i in 0..actor_count {
        actors.push(system.spawn_local(format!("actor_{}", i), TestActor { count: 0 }));
    }

    let test_start = Instant::now();
    let mut latencies = Vec::new();
    let mut total_successful = 0;
    let mut futures = FuturesUnordered::new();

    for actor_idx in 0..actor_count {
        for msg_idx in 0..messages_per_actor {
            let actor_ref = actors[actor_idx].clone();

            let task = async move {
                let req_start = Instant::now();
                let result = if wait_reply {
                    actor_ref
                        .ask(TestMessage {
                            content: format!("msg_{}", msg_idx),
                        })
                        .await
                        .map_err(|e| {
                            eprintln!("Error: {:?}", e);
                            e
                        })
                        .is_ok()
                } else {
                    actor_ref
                        .tell(TestMessage {
                            content: format!("msg_{}", msg_idx),
                        })
                        .await
                        .is_ok()
                };
                (req_start.elapsed(), result)
            };

            futures.push(task);

            // Simple backpressure control
            if futures.len() >= concurrency_limit {
                if let Some((latency, is_success)) = futures.next().await {
                    latencies.push(latency);
                    if is_success {
                        total_successful += 1;
                    }
                }
            }
        }
    }

    let total_messages = actor_count * messages_per_actor;

    // Handle remaining futures
    while let Some((latency, is_success)) = futures.next().await {
        latencies.push(latency);
        if is_success {
            total_successful += 1;
        }
    }

    let total_duration = test_start.elapsed();
    let stats = PerformanceStats::new(
        total_messages,
        total_successful,
        total_duration,
        &mut latencies,
    );
    stats.print_analysis("OPTIMAL STREAMING");

    Ok(stats)
}

#[tokio::main]
async fn main() -> lyricore::error::Result<()> {
    println!("ğŸ“ Performance Optimization Masterclass\n");

    println!("ğŸ“š Key Lessons from Previous Tests:");
    println!("1. ğŸ† Basic Streaming: 928,119 QPS (Winner!)");
    println!("2. ğŸš€ Ultra Optimized: 913,289 QPS (-1.6%)");
    println!("3. âš¡ Lockfree: 698,232 QPS (-24.8%)");
    println!();

    println!("ğŸ’¡ Why Simple Won:");
    println!("â€¢ Fewer CPU instructions per message");
    println!("â€¢ Better branch prediction");
    println!("â€¢ Compiler-friendly code patterns");
    println!("â€¢ Reduced memory indirection");
    println!("â€¢ No synchronization overhead");
    println!();

    let actor_count = 500_000; // 500K actors for optimal streaming
    let messages_per_actor = 10; // 1M total for faster testing
    let wait_reply = true;

    println!("ğŸ§ª Demonstrating the principles:");

    run_optimal_streaming(actor_count, messages_per_actor, 1000, wait_reply).await?;

    tokio::time::sleep(Duration::from_millis(100)).await;

    println!("ğŸ“Š Performance Comparison:");
    println!("\nğŸ¯ Golden Rules for High-Performance Rust:");
    println!("1. ğŸ“ Profile first, optimize second");
    println!("2. ğŸ¯ Keep it simple and direct");
    println!("3. ğŸš« Avoid premature optimization");
    println!("4. ğŸ” Focus on algorithmic improvements");
    println!("5. âš¡ Trust the compiler and runtime");
    println!("6. ğŸ“Š Measure everything, assume nothing");

    println!("\nğŸ’­ In your case:");
    println!("â€¢ Network and serialization are the bottlenecks");
    println!("â€¢ Basic streaming is already near-optimal");
    println!("â€¢ Focus on actor system tuning, not task management");
    println!("â€¢ Consider message batching at the protocol level");

    Ok(())
}

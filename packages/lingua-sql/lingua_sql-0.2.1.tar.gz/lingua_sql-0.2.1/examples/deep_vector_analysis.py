#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å‘é‡åº“æ·±åº¦åˆ†æè„šæœ¬
æ·±å…¥åˆ†æä¸åŒå‘é‡åº“ç›®å½•çš„å†…å®¹ã€ç»“æ„å’Œæ•°æ®
"""

import os
import sys
import json
import sqlite3
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

def analyze_chroma_database():
    """åˆ†æ ChromaDB æ•°æ®åº“æ–‡ä»¶"""
    print("=== ChromaDB æ•°æ®åº“æ·±åº¦åˆ†æ ===\n")
    
    current_dir = Path(__file__).parent
    chroma_db_file = current_dir / "chroma.sqlite3"
    
    if not chroma_db_file.exists():
        print("âŒ ChromaDB æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect(chroma_db_file)
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰è¡¨å
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        
        print(f"æ•°æ®åº“ä¸­çš„è¡¨:")
        for table in tables:
            table_name = table[0]
            print(f"  âœ“ {table_name}")
            
            # è·å–è¡¨ç»“æ„
            cursor.execute(f"PRAGMA table_info({table_name});")
            columns = cursor.fetchall()
            print(f"    åˆ—ç»“æ„:")
            for col in columns:
                col_id, col_name, col_type, not_null, default_val, pk = col
                print(f"      - {col_name}: {col_type} {'(PK)' if pk else ''}")
            
            # è·å–è®°å½•æ•°é‡
            cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
            count = cursor.fetchone()[0]
            print(f"    è®°å½•æ•°: {count}")
            
            # è·å–æ ·æœ¬æ•°æ®
            if count > 0:
                cursor.execute(f"SELECT * FROM {table_name} LIMIT 3;")
                sample_data = cursor.fetchall()
                print(f"    æ ·æœ¬æ•°æ®:")
                for i, row in enumerate(sample_data, 1):
                    print(f"      {i}. {row[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ªå­—æ®µ
            
            print()
        
        conn.close()
        
    except Exception as e:
        print(f"âŒ åˆ†æ ChromaDB æ•°æ®åº“å¤±è´¥: {e}")

def analyze_vector_directories():
    """åˆ†æå‘é‡åº“ç›®å½•"""
    print("=== å‘é‡åº“ç›®å½•æ·±åº¦åˆ†æ ===\n")
    
    current_dir = Path(__file__)
    vector_dirs = [
        "27fb8419-3b63-4973-a72b-6be68a1166b9",
        "ae102e87-e644-4fee-899c-97d9d654fab2", 
        "d4f94d14-543b-42c0-80fb-e5ee4260efc6"
    ]
    
    for dir_name in vector_dirs:
        dir_path = current_dir.parent / dir_name
        if not dir_path.exists():
            continue
            
        print(f"åˆ†æç›®å½•: {dir_name}")
        print(f"è·¯å¾„: {dir_path}")
        
        # åˆ†ææ¯ä¸ªæ–‡ä»¶
        for file_name in ["data_level0.bin", "header.bin", "length.bin", "link_lists.bin"]:
            file_path = dir_path / file_name
            if file_path.exists():
                size = file_path.stat().st_size
                print(f"  {file_name}:")
                print(f"    å¤§å°: {size} bytes ({size/1024/1024:.2f} MB)")
                
                # åˆ†ææ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼‰
                if file_name in ["header.bin", "length.bin"] and size < 10000:  # å°æ–‡ä»¶æ‰å°è¯•è¯»å–
                    try:
                        with open(file_path, 'rb') as f:
                            content = f.read()
                            # å°è¯•è§£ç ä¸ºæ–‡æœ¬
                            try:
                                text_content = content.decode('utf-8')
                                print(f"    å†…å®¹é¢„è§ˆ: {text_content[:100]}...")
                            except UnicodeDecodeError:
                                # å¦‚æœæ˜¯äºŒè¿›åˆ¶ï¼Œæ˜¾ç¤ºåå…­è¿›åˆ¶
                                hex_content = content[:50].hex()
                                print(f"    äºŒè¿›åˆ¶å†…å®¹é¢„è§ˆ: {hex_content}...")
                    except Exception as e:
                        print(f"    è¯»å–å¤±è´¥: {e}")
            else:
                print(f"  {file_name}: ç¼ºå¤±")
        
        print()
    
    print("="*60)

def test_vector_store_operations():
    """æµ‹è¯•å‘é‡å­˜å‚¨æ“ä½œ"""
    print("=== å‘é‡å­˜å‚¨æ“ä½œæµ‹è¯• ===\n")
    
    try:
        from lingua_sql import LinguaSQL
        from lingua_sql.config import LinguaSQLConfig, DatabaseConfig, APIConfig
        
        current_dir = Path(__file__).parent
        
        # æµ‹è¯•1: ä½¿ç”¨ä¸»ç›®å½•
        print("æµ‹è¯•1: ä½¿ç”¨ä¸»ç›®å½•")
        config1 = LinguaSQLConfig(
            api=APIConfig(
                api_key="test_key",
                model="deepseek-chat",
                client="persistent",
                path=str(current_dir),
            ),
            database=DatabaseConfig(
                type="mysql",
                host="localhost",
                port=3306,
                user="test",
                password="test",
                database="test",
                auto_connect=False,
                auto_import_ddl=False,
            ),
            debug=False
        )
        
        nl1 = LinguaSQL(config=config1)
        
        # æµ‹è¯•å„ç§æ“ä½œ
        print("  æµ‹è¯• DDL é›†åˆ...")
        if hasattr(nl1, 'ddl_collection') and nl1.ddl_collection:
            ddl_info = nl1.ddl_collection.get()
            if ddl_info and 'ids' in ddl_info:
                print(f"    âœ“ DDLé›†åˆåŒ…å« {len(ddl_info['ids'])} æ¡è®°å½•")
                if ddl_info['ids']:
                    print(f"    æ ·æœ¬ID: {ddl_info['ids'][:3]}")
            else:
                print(f"    âš  DDLé›†åˆä¸ºç©º")
        
        print("  æµ‹è¯• SQL é›†åˆ...")
        if hasattr(nl1, 'sql_collection') and nl1.sql_collection:
            sql_info = nl1.sql_collection.get()
            if sql_info and 'ids' in sql_info:
                print(f"    âœ“ SQLé›†åˆåŒ…å« {len(sql_info['ids'])} æ¡è®°å½•")
                if sql_info['ids']:
                    print(f"    æ ·æœ¬ID: {sql_info['ids'][:3]}")
            else:
                print(f"    âš  SQLé›†åˆä¸ºç©º")
        
        print("  æµ‹è¯•æ–‡æ¡£é›†åˆ...")
        if hasattr(nl1, 'documentation_collection') and nl1.documentation_collection:
            doc_info = nl1.documentation_collection.get()
            if doc_info and 'ids' in doc_info:
                print(f"    âœ“ æ–‡æ¡£é›†åˆåŒ…å« {len(doc_info['ids'])} æ¡è®°å½•")
                if doc_info['ids']:
                    print(f"    æ ·æœ¬ID: {doc_info['ids'][:3]}")
            else:
                print(f"    âš  æ–‡æ¡£é›†åˆä¸ºç©º")
        
        print()
        
        # æµ‹è¯•2: ä½¿ç”¨å­ç›®å½•
        print("æµ‹è¯•2: ä½¿ç”¨å­ç›®å½•")
        config2 = LinguaSQLConfig(
            api=APIConfig(
                api_key="test_key",
                model="deepseek-chat",
                client="persistent",
                path=str(current_dir / "27fb8419-3b63-4973-a72b-6be68a1166b9"),
            ),
            database=DatabaseConfig(
                type="mysql",
                host="localhost",
                port=3306,
                user="test",
                password="test",
                database="test",
                auto_connect=False,
                auto_import_ddl=False,
            ),
            debug=False
        )
        
        nl2 = LinguaSQL(config=config2)
        
        # æµ‹è¯•å„ç§æ“ä½œ
        print("  æµ‹è¯• DDL é›†åˆ...")
        if hasattr(nl2, 'ddl_collection') and nl2.ddl_collection:
            ddl_info = nl2.ddl_collection.get()
            if ddl_info and 'ids' in ddl_info:
                print(f"    âœ“ DDLé›†åˆåŒ…å« {len(ddl_info['ids'])} æ¡è®°å½•")
            else:
                print(f"    âš  DDLé›†åˆä¸ºç©º")
        
        print("  æµ‹è¯• SQL é›†åˆ...")
        if hasattr(nl2, 'sql_collection') and nl2.sql_collection:
            sql_info = nl2.sql_collection.get()
            if sql_info and 'ids' in sql_info:
                print(f"    âœ“ SQLé›†åˆåŒ…å« {len(sql_info['ids'])} æ¡è®°å½•")
            else:
                print(f"    âš  SQLé›†åˆä¸ºç©º")
        
        print("  æµ‹è¯•æ–‡æ¡£é›†åˆ...")
        if hasattr(nl2, 'documentation_collection') and nl2.documentation_collection:
            doc_info = nl2.documentation_collection.get()
            if doc_info and 'ids' in doc_info:
                print(f"    âœ“ æ–‡æ¡£é›†åˆåŒ…å« {len(doc_info['ids'])} æ¡è®°å½•")
            else:
                print(f"    âš  æ–‡æ¡£é›†åˆä¸ºç©º")
        
    except Exception as e:
        print(f"âŒ å‘é‡å­˜å‚¨æ“ä½œæµ‹è¯•å¤±è´¥: {e}")

def analyze_data_relationships():
    """åˆ†ææ•°æ®å…³ç³»"""
    print("\n=== æ•°æ®å…³ç³»åˆ†æ ===\n")
    
    try:
        from lingua_sql import LinguaSQL
        from lingua_sql.config import LinguaSQLConfig, DatabaseConfig, APIConfig
        
        current_dir = Path(__file__).parent
        
        # ä½¿ç”¨ä¸»ç›®å½•åˆå§‹åŒ–
        config = LinguaSQLConfig(
            api=APIConfig(
                api_key="test_key",
                model="deepseek-chat",
                client="persistent",
                path=str(current_dir),
            ),
            database=DatabaseConfig(
                type="mysql",
                host="localhost",
                port=3306,
                user="test",
                password="test",
                database="test",
                auto_connect=False,
                auto_import_ddl=False,
            ),
            debug=False
        )
        
        nl = LinguaSQL(config=config)
        
        print("åˆ†æå‘é‡å­˜å‚¨ä¸­çš„æ•°æ®å…³ç³»...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®
        if hasattr(nl, 'get_training_data'):
            try:
                training_data = nl.get_training_data()
                if training_data is not None and hasattr(training_data, 'shape'):
                    print(f"âœ“ è®­ç»ƒæ•°æ®å¯ç”¨ï¼Œå½¢çŠ¶: {training_data.shape}")
                    if len(training_data) > 0:
                        print(f"  æ ·æœ¬æ•°æ®:")
                        print(f"    {training_data.head(3).to_string()}")
                else:
                    print("âš  è®­ç»ƒæ•°æ®ä¸ºç©ºæˆ–ä¸å¯ç”¨")
            except Exception as e:
                print(f"âœ— è·å–è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼é—®é¢˜æŸ¥è¯¢åŠŸèƒ½
        if hasattr(nl, 'get_similar_question_sql'):
            try:
                similar_questions = nl.get_similar_question_sql("æµ‹è¯•é—®é¢˜")
                if similar_questions:
                    print(f"âœ“ ç›¸ä¼¼é—®é¢˜æŸ¥è¯¢åŠŸèƒ½æ­£å¸¸ï¼Œè¿”å› {len(similar_questions)} æ¡ç»“æœ")
                else:
                    print("âš  ç›¸ä¼¼é—®é¢˜æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
            except Exception as e:
                print(f"âœ— ç›¸ä¼¼é—®é¢˜æŸ¥è¯¢å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³DDLæŸ¥è¯¢åŠŸèƒ½
        if hasattr(nl, 'get_related_ddl'):
            try:
                related_ddl = nl.get_related_ddl("æµ‹è¯•æŸ¥è¯¢")
                if related_ddl:
                    print(f"âœ“ ç›¸å…³DDLæŸ¥è¯¢åŠŸèƒ½æ­£å¸¸ï¼Œè¿”å› {len(related_ddl)} æ¡ç»“æœ")
                else:
                    print("âš  ç›¸å…³DDLæŸ¥è¯¢è¿”å›ç©ºç»“æœ")
            except Exception as e:
                print(f"âœ— ç›¸å…³DDLæŸ¥è¯¢å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å…³ç³»åˆ†æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å‘é‡åº“æ·±åº¦åˆ†æå·¥å…·")
    print("=" * 60)
    
    # 1. åˆ†æ ChromaDB æ•°æ®åº“
    analyze_chroma_database()
    
    # 2. åˆ†æå‘é‡åº“ç›®å½•
    analyze_vector_directories()
    
    # 3. æµ‹è¯•å‘é‡å­˜å‚¨æ“ä½œ
    test_vector_store_operations()
    
    # 4. åˆ†ææ•°æ®å…³ç³»
    analyze_data_relationships()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ æ·±åº¦åˆ†æå®Œæˆï¼")
    print("\nğŸ“‹ åˆ†æç»“æœæ€»ç»“:")
    print("1. ChromaDB æ•°æ®åº“ç»“æ„åˆ†æ")
    print("2. å‘é‡åº“ç›®å½•æ–‡ä»¶åˆ†æ")
    print("3. å‘é‡å­˜å‚¨æ“ä½œæµ‹è¯•")
    print("4. æ•°æ®å…³ç³»åˆ†æ")
    print("\nğŸ’¡ è¿™äº›åˆ†æç»“æœå¯ä»¥å¸®åŠ©ä½ äº†è§£:")
    print("   - å‘é‡åº“çš„æ•°æ®å­˜å‚¨ç»“æ„")
    print("   - ä¸åŒç›®å½•çš„å…¼å®¹æ€§")
    print("   - æ•°æ®è®¿é—®å’ŒæŸ¥è¯¢èƒ½åŠ›")
    print("   - å‘é‡åº“çš„é€šç”¨æ€§ç¨‹åº¦")

if __name__ == "__main__":
    main()

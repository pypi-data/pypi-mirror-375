# Few-shot validation types
from typing import Dict, List, NamedTuple, TypedDict

class FewShotCase(TypedDict):
    example: Dict[str, str]
    reasoning: str
    quality: str
    reasoning_details: Dict[str, str]

class FewShotEvalResult(NamedTuple):
    success_cases: List[FewShotCase]
    failure_cases: List[FewShotCase]
    success_count: int
    total: int
    success_rate: float
"""
Pydantic models for the evaluation module.
"""
from datetime import datetime
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field


# A simple class to represent a run result when actual run object is not available
class RunResult:
    def __init__(self, status):
        self.status = status


class CommandOutput(BaseModel):
    """
    Commands extracted from run step outputs.
    
    Attributes
    ----------
    sql : str, optional
        SQL command extracted from the output.
    dax : str, optional
        DAX command extracted from the output.
    kql : str, optional
        KQL command extracted from the output.
    """
    sql: Optional[str] = None
    dax: Optional[str] = None
    kql: Optional[str] = None


class EvaluationContext(BaseModel):
    """
    Input parameters for data agent evaluation.
    
    Attributes
    ----------
    row : dict
        A single row from the input DataFrame containing question and expected_answer.
    data_agent_name : str
        Name of the Data Agent.
    workspace_name : str, optional
        Workspace Name if Data Agent is in different workspace.
    table_name : str
        Table name to store the evaluation result.
    critic_prompt : str, optional
        Prompt to evaluate the actual answer from Data Agent.
    data_agent_stage : str
        Data Agent stage ie., sandbox or production.
    max_workers : int
        Maximum worker nodes that need to run parallely.
    num_query_repeats : int
        Number of times to evaluate each question.
    eval_id : str
        Unique id for the evaluation run.
    run_timestamp : datetime
        Timestamp indicating when the evaluation run started.
    """
    row: dict
    data_agent_name: str
    workspace_name: Optional[str] = None
    table_name: str = 'evaluation_output'
    critic_prompt: Optional[str] = None
    data_agent_stage: str
    max_workers: int = 5
    num_query_repeats: int = 1
    eval_id: str
    run_timestamp: datetime


class RunSteps(BaseModel):
    """
    Steps executed during a data agent run.
    
    Attributes
    ----------
    id : str, optional
        Unique identifier for the input processing row.
    thread_id : str
        Unique identifier for the thread.
    run_id : str
        Unique identifier for the run.
    function_names : str
        Names of functions called during the run.
    function_queries : str
        Queries executed by functions during the run.
    function_outputs : str
        Outputs returned by functions during the run.
    sql_steps : str
        SQL commands executed during the run.
    dax_steps : str
        DAX commands executed during the run.
    kql_steps : str
        KQL commands executed during the run.
    """
    id: Optional[str] = None
    thread_id: str
    run_id: str
    function_names: str
    function_queries: str
    function_outputs: str
    sql_steps: str
    dax_steps: str
    kql_steps: str


class EvaluationResult(BaseModel):
    """
    Result of a data agent evaluation for a single query.
    
    Attributes
    ----------
    id : str
        Unique identifier for the evaluation row.
    evaluation_id : str
        Unique identifier for the evaluation run.
    thread_id : str
        Unique identifier for the thread.
    run_timestamp : datetime
        Timestamp of when the evaluation run started.
    question : str
        The question that was evaluated.
    expected_answer : str
        The expected answer for the question.
    actual_answer : str
        The actual answer generated by the data agent.
    execution_time_sec : float
        Execution time in seconds.
    status : str
        Status of the run.
    thread_url : str
        URL to the thread.
    evaluation_judgement : Optional[bool]
        Whether the actual answer matched the expected answer.
    evaluation_message : str
        Message from the evaluation.
    evaluation_status : str
        Status of the evaluation.
    evaluation_thread_url : str
        URL to the evaluation thread.
    """
    id: str
    evaluation_id: str
    thread_id: str
    run_timestamp: datetime
    question: str
    expected_answer: str
    actual_answer: str
    execution_time_sec: float
    status: str
    thread_url: str
    evaluation_judgement: Optional[bool] = None
    evaluation_message: str
    evaluation_status: str
    evaluation_thread_url: str
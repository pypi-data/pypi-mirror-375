"""
Test Runner (RFS v4)

ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì§€ì›
- ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ì¸¡ì •
- í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
"""

import asyncio
import json
import subprocess
import time
import xml.etree.ElementTree as ET
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

try:
    from rich.console import Console
    from rich.panel import Panel
    from rich.progress import (
        BarColumn,
        Progress,
        SpinnerColumn,
        TextColumn,
        TimeElapsedColumn,
    )
    from rich.table import Table
    from rich.tree import Tree

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
from ...core.result import Failure, Result, Success

if RICH_AVAILABLE:
    console = Console()
else:
    console = None


class TestFramework(Enum):
    """ì§€ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""

    PYTEST = "pytest"
    UNITTEST = "unittest"
    ASYNCIO = "asyncio"
    CUSTOM = "custom"


class TestType(Enum):
    """í…ŒìŠ¤íŠ¸ ìœ í˜•"""

    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    END_TO_END = "e2e"
    SECURITY = "security"


@dataclass
class TestConfig:
    """í…ŒìŠ¤íŠ¸ ì„¤ì •"""

    framework: TestFramework = TestFramework.PYTEST
    test_paths: List[str] = field(default_factory=list)
    patterns: List[str] = field(default_factory=list)
    parallel: bool = True
    max_workers: int = 4
    coverage: bool = True
    coverage_threshold: float = 80.0
    verbose: bool = True
    fail_fast: bool = False
    timeout: int = 300
    environment_vars: Dict[str, Any] = field(default_factory=dict)
    fixtures_path: Optional[str] = None
    mock_config: Optional[Dict[str, Any]] = None


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""

    framework: TestFramework
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    error_tests: int = 0
    execution_time: float = 0.0
    coverage_percentage: Optional[float] = None
    failed_test_details: List[Dict[str, Any]] = field(default_factory=list)
    coverage_report: Optional[Dict[str, Any]] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)

    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        if self.total_tests == 0:
            return 0.0
        return self.passed_tests / self.total_tests * 100

    @property
    def is_successful(self) -> bool:
        """í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€"""
        return self.failed_tests == 0 and self.error_tests == 0


class TestRunner:
    """ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""

    def __init__(self, config: Optional[TestConfig] = None):
        self.config = config or TestConfig()
        self.project_path = Path.cwd()
        self.test_history: List[TestResult] = []

    async def run_tests(
        self, test_type: Optional[TestType] = None, **kwargs
    ) -> Result[TestResult, str]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            for key, value in kwargs.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
            if console:
                console.print(
                    Panel(
                        f"ğŸ§ª RFS v4 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘\n\nğŸ”§ í”„ë ˆì„ì›Œí¬: {self.config.framework.value}\nğŸ“ ê²½ë¡œ: {', '.join(self.config.test_paths)}\nâš¡ ë³‘ë ¬ ì‹¤í–‰: {('ì˜ˆ' if self.config.parallel else 'ì•„ë‹ˆì˜¤')}\nğŸ“Š ì»¤ë²„ë¦¬ì§€: {('ì˜ˆ' if self.config.coverage else 'ì•„ë‹ˆì˜¤')}\nğŸ¯ íƒ€ì…: {(test_type.value if test_type else 'ëª¨ë“  íƒ€ì…')}",
                        title="í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
                        border_style="blue",
                    )
                )
            start_time = time.time()
            match self.config.framework:
                case TestFramework.PYTEST:
                    result = await self._run_pytest(test_type)
                case TestFramework.UNITTEST:
                    result = await self._run_unittest(test_type)
                case TestFramework.ASYNCIO:
                    result = await self._run_asyncio_tests(test_type)
                case _:
                    return Failure(
                        f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬: {self.config.framework}"
                    )
            if result.is_failure():
                return result
            test_result = result.unwrap()
            test_result.execution_time = time.time() - start_time
            if console:
                await self._display_test_results(test_result)
            self.test_history = self.test_history + [test_result]
            return Success(test_result)
        except Exception as e:
            return Failure(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    async def _run_pytest(
        self, test_type: Optional[TestType]
    ) -> Result[TestResult, str]:
        """pytest ì‹¤í–‰"""
        try:
            cmd = ["python", "-m", "pytest"]
            for path in self.config.test_paths:
                if Path(path).exists():
                    cmd = cmd + [path]
            if self.config.verbose:
                cmd = cmd + ["-v"]
            if self.config.fail_fast:
                cmd = cmd + ["-x"]
            if self.config.parallel and self.config.max_workers > 1:
                cmd = cmd + ["-n", str(self.config.max_workers)]
            if self.config.coverage:
                cmd = cmd + [
                    "--cov=.",
                    "--cov-report=xml",
                    "--cov-report=html",
                    "--cov-report=term-missing",
                ]
            cmd = cmd + ["--junit-xml=test-results.xml"]
            if test_type:
                cmd = cmd + ["-m", test_type.value]
            env = {**os.environ, **self.config.environment_vars}
            if console:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    console=console,
                ) as progress:
                    task = progress.add_task("pytest ì‹¤í–‰ ì¤‘...", total=None)
                    process = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.STDOUT,
                        env=env,
                    )
                    output_lines = []
                    while True:
                        line = await process.stdout.readline()
                        if not line:
                            break
                        line_str = line.decode().strip()
                        output_lines = output_lines + [line_str]
                    await process.wait()
                    progress.remove_task(task)
            test_result = await self._parse_pytest_results(
                process.returncode, output_lines
            )
            return Success(test_result)
        except Exception as e:
            return Failure(f"pytest ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    async def _parse_pytest_results(
        self, return_code: int, output_lines: List[str]
    ) -> TestResult:
        """pytest ê²°ê³¼ íŒŒì‹±"""
        result = TestResult(framework=TestFramework.PYTEST)
        try:
            xml_file = Path("test-results.xml")
            if xml_file.exists():
                tree = ET.parse(xml_file)
                root = tree.getroot()
                result.total_tests = int(root.get("tests", 0))
                result.failed_tests = int(root.get("failures", 0))
                result.error_tests = int(root.get("errors", 0))
                result.skipped_tests = int(root.get("skipped", 0))
                result.passed_tests = (
                    result.total_tests
                    - result.failed_tests
                    - result.error_tests
                    - result.skipped_tests
                )
                for testcase in root.findall(".//testcase"):
                    failure = testcase.find("failure")
                    error = testcase.find("error")
                    if failure is not None or error is not None:
                        result.failed_test_details = result.failed_test_details + [
                            {
                                "name": testcase.get("name", ""),
                                "classname": testcase.get("classname", ""),
                                "time": float(testcase.get("time", 0)),
                                "message": (
                                    (failure or error).get("message", "")
                                    if (failure or error) is not None
                                    else ""
                                ),
                                "details": (
                                    (failure or error).text
                                    if (failure or error) is not None
                                    else ""
                                ),
                            }
                        ]
            coverage_xml = Path("coverage.xml")
            if coverage_xml.exists() and self.config.coverage:
                try:
                    tree = ET.parse(coverage_xml)
                    root = tree.getroot()
                    coverage_elem = root.find(".//coverage")
                    if coverage_elem is not None:
                        result.coverage_percentage = (
                            float(coverage_elem.get("line-rate", 0)) * 100
                        )
                except:
                    pass
        except Exception as e:
            if console:
                console.print(f"âš ï¸  ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}", style="yellow")
        return result

    async def _run_unittest(
        self, test_type: Optional[TestType]
    ) -> Result[TestResult, str]:
        """unittest ì‹¤í–‰"""
        try:
            cmd = ["python", "-m", "unittest"]
            if self.config.verbose:
                cmd = cmd + ["-v"]
            cmd = cmd + ["discover", "-s", self.config.test_paths[0], "-p", "test_*.py"]
            process = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT
            )
            stdout, _ = await process.communicate()
            output = stdout.decode()
            result = TestResult(framework=TestFramework.UNITTEST)
            lines = output.split("\n")
            for line in lines:
                if "Ran" in line and "test" in line:
                    import re

                    match = re.search("Ran (\\d+) test", line)
                    if match:
                        result.total_tests = int(match.group(1))
                if "FAILED" in line:
                    match = re.search("FAILED \\(.*failures=(\\d+).*\\)", line)
                    if match:
                        result.failed_tests = int(match.group(1))
            result.passed_tests = result.total_tests - result.failed_tests
            return Success(result)
        except Exception as e:
            return Failure(f"unittest ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    async def _run_asyncio_tests(
        self, test_type: Optional[TestType]
    ) -> Result[TestResult, str]:
        """asyncio ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            cmd = ["python", "-m", "pytest", "--asyncio-mode=auto"]
            for path in self.config.test_paths:
                if Path(path).exists():
                    cmd = cmd + [path]
            if self.config.verbose:
                cmd = cmd + ["-v"]
            process = await asyncio.create_subprocess_exec(
                *cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.STDOUT
            )
            stdout, _ = await process.communicate()
            result = await self._parse_pytest_results(
                process.returncode, stdout.decode().split("\n")
            )
            result.framework = TestFramework.ASYNCIO
            return Success(result)
        except Exception as e:
            return Failure(f"asyncio í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    async def _display_test_results(self, result: TestResult) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ í‘œì‹œ"""
        if not console:
            return
        summary_table = Table(
            title="í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½", show_header=True, header_style="bold magenta"
        )
        summary_table.add_column("í•­ëª©", style="cyan", width=15)
        summary_table.add_column("ê°’", style="white", justify="right")
        summary_table.add_column("ë¹„ìœ¨", style="green", justify="right")
        summary_table.add_row("ì´ í…ŒìŠ¤íŠ¸", str(result.total_tests), "100%")
        summary_table.add_row(
            "í†µê³¼",
            f"[green]{result.passed_tests}[/green]",
            f"[green]{result.success_rate:.1f}%[/green]",
        )
        if result.failed_tests > 0:
            failure_rate = (
                result.failed_tests / result.total_tests * 100
                if result.total_tests > 0
                else 0
            )
            summary_table.add_row(
                "ì‹¤íŒ¨",
                f"[red]{result.failed_tests}[/red]",
                f"[red]{failure_rate:.1f}%[/red]",
            )
        if result.skipped_tests > 0:
            skip_rate = (
                result.skipped_tests / result.total_tests * 100
                if result.total_tests > 0
                else 0
            )
            summary_table.add_row(
                "ê±´ë„ˆëœ€",
                f"[yellow]{result.skipped_tests}[/yellow]",
                f"[yellow]{skip_rate:.1f}%[/yellow]",
            )
        summary_table.add_row("ì‹¤í–‰ ì‹œê°„", f"{result.execution_time:.2f}ì´ˆ", "")
        if result.coverage_percentage is not None:
            coverage_color = (
                "green"
                if result.coverage_percentage >= self.config.coverage_threshold
                else "red"
            )
            summary_table.add_row(
                "ì½”ë“œ ì»¤ë²„ë¦¬ì§€",
                f"[{coverage_color}]{result.coverage_percentage:.1f}%[/{coverage_color}]",
                f"ëª©í‘œ: {self.config.coverage_threshold}%",
            )
        console.print(summary_table)
        if result.failed_test_details:
            console.print("\n")
            failure_tree = Tree("âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸")
            for failure in result.failed_test_details:
                test_node = failure_tree.add(
                    f"[red]{failure['name']}[/red] ({failure['classname']})"
                )
                if failure.get("message"):
                    test_node.add(f"ë©”ì‹œì§€: {failure.get('message')}")
                if failure.get("time"):
                    test_node.add(f"ì‹¤í–‰ ì‹œê°„: {failure.get('time'):.3f}ì´ˆ")
            console.print(failure_tree)
        if result.is_successful:
            console.print(
                Panel(
                    f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n\nğŸ¯ ì„±ê³µë¥ : {result.success_rate:.1f}%\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {result.execution_time:.2f}ì´ˆ"
                    + (
                        f"\nğŸ“Š ì½”ë“œ ì»¤ë²„ë¦¬ì§€: {result.coverage_percentage:.1f}%"
                        if result.coverage_percentage
                        else ""
                    ),
                    title="í…ŒìŠ¤íŠ¸ ì„±ê³µ",
                    border_style="green",
                )
            )
        else:
            console.print(
                Panel(
                    f"âŒ {result.failed_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\n\nğŸ¯ ì„±ê³µë¥ : {result.success_rate:.1f}%\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {result.execution_time:.2f}ì´ˆ\n\nğŸ’¡ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
                    title="í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                    border_style="red",
                )
            )

    async def generate_test_template(
        self, test_name: str, test_type: TestType
    ) -> Result[str, str]:
        """í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        try:
            template = self._get_test_template(test_name, test_type)
            test_dir = Path(self.config.test_paths[0])
            test_dir.mkdir(parents=True, exist_ok=True)
            test_file = test_dir / f"test_{test_name.lower()}.py"
            with open(test_file, "w", encoding="utf-8") as f:
                f.write(template)
            if console:
                console.print(
                    Panel(
                        f"âœ… í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ\n\nğŸ“ íŒŒì¼: {test_file}\nğŸ§ª ìœ í˜•: {test_type.value}\nğŸ”§ í”„ë ˆì„ì›Œí¬: {self.config.framework.value}",
                        title="í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿",
                        border_style="green",
                    )
                )
            return Success(f"í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ: {test_file}")
        except Exception as e:
            return Failure(f"í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def _get_test_template(self, test_name: str, test_type: TestType) -> str:
        """í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        if self.config.framework == TestFramework.PYTEST:
            return f'''"""\n{test_name.title()} í…ŒìŠ¤íŠ¸\n\n{test_type.value} í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸\n"""\n\nimport pytest\nimport asyncio\nfrom unittest.mock import Mock, patch\n\n# RFS Framework ì„í¬íŠ¸\nfrom rfs import Result, Success, Failure\n\n\nclass Test{test_name.title()}:\n    """\n    {test_name.title()} í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤\n    """\n    \n    def setup_method(self):\n        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""\n        pass\n    \n    def teardown_method(self):\n        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ í›„ ì •ë¦¬"""\n        pass\n    \n    def test_{test_name}_success(self):\n        """ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    def test_{test_name}_failure(self):\n        """ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    @pytest.mark.asyncio\n    async def test_{test_name}_async(self):\n        """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    @pytest.mark.parametrize("input_data,expected", [\n        ("test1", "expected1"),\n        ("test2", "expected2"),\n    ])\n    def test_{test_name}_parametrized(self, input_data, expected):\n        """ë§¤ê°œë³€ìˆ˜í™” í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    def test_{test_name}_with_mock(self):\n        """ëª¨í‚¹ì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸"""\n        with patch('module.function') as mock_func:\n            # Given (ì¤€ë¹„)\n            mock_func.return_value = "mocked_result"\n            \n            # When (ì‹¤í–‰)\n            \n            # Then (ê²€ì¦)\n            mock_func.assert_called_once()\n            assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n'''
        else:
            return f'''"""\n{test_name.title()} í…ŒìŠ¤íŠ¸\n\n{test_type.value} í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ unittest ê¸°ë°˜ í…ŒìŠ¤íŠ¸\n"""\n\nimport unittest\nimport asyncio\nfrom unittest.mock import Mock, patch\n\n# RFS Framework ì„í¬íŠ¸\nfrom rfs import Result, Success, Failure\n\n\nclass Test{test_name.title()}(unittest.TestCase):\n    """\n    {test_name.title()} í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤\n    """\n    \n    def setUp(self):\n        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""\n        pass\n    \n    def tearDown(self):\n        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ í›„ ì •ë¦¬"""\n        pass\n    \n    def test_{test_name}_success(self):\n        """ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    def test_{test_name}_failure(self):\n        """ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n    \n    def test_{test_name}_async(self):\n        """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸"""\n        async def async_test():\n            # Given (ì¤€ë¹„)\n            \n            # When (ì‹¤í–‰)\n            \n            # Then (ê²€ì¦)\n            self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n        \n        asyncio.run(async_test())\n    \n    @patch('module.function')\n    def test_{test_name}_with_mock(self, mock_func):\n        """ëª¨í‚¹ì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸"""\n        # Given (ì¤€ë¹„)\n        mock_func.return_value = "mocked_result"\n        \n        # When (ì‹¤í–‰)\n        \n        # Then (ê²€ì¦)\n        mock_func.assert_called_once()\n        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„\n\n\nif __name__ == '__main__':\n    unittest.main()\n'''

    def get_test_history(self) -> List[TestResult]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
        return self.test_history.copy()

    def get_coverage_report(self) -> Optional[Dict[str, Any]]:
        """ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
        if not self.test_history:
            return None
        latest_result = self.test_history[-1]
        return latest_result.coverage_report

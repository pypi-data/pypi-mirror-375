{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61868986-8ba7-4e5e-a5dd-a72fe0764fed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Evaluate a Fabric Data Agent\n",
    "\n",
    "In this notebook, we'll walk through how to evaluate a Fabric Data Agent using the `fabric-data-agent-sdk`. We'll cover the full workflow, including:\n",
    "\n",
    "- ✅ Creating a new Data Agent from the SDK\n",
    "- 🗂️ Adding data sources and selecting relevant tables\n",
    "- 📋 Defining a ground truth dataset with questions and expected answers\n",
    "- 🧪 Running an automated evaluation to compare actual vs. expected responses\n",
    "- 📈 Reviewing evaluation summaries and detailed results\n",
    "\n",
    "This end-to-end example is designed to help you validate the accuracy of your Data Agent and iterate on improvements with structured feedback.\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b26cb-5856-4570-9244-bfd1cf47d0b1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> **Prerequisite: Load Sample Data into the Lakehouse**\n",
    "\n",
    "Before running this notebook, make sure you’ve created a Lakehouse and loaded the sample **AdventureWorks** dataset.\n",
    "\n",
    "Follow the steps in the official guide to create the Lakehouse and populate it with sample tables:\n",
    "👉 [Create a Lakehouse with AdventureWorksLH](https://learn.microsoft.com/en-us/fabric/data-science/data-agent-scenario#create-a-lakehouse-with-adventureworkslh)\n",
    "\n",
    "This ensures that the required tables are available for your Data Agent to access during evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e14e7-39da-4bd1-a8f1-5f723b20b804",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Install Fabric Data Agent SDK\n",
    "\n",
    "Before we begin, install the latest version of the `fabric-data-agent-sdk`. This SDK provides all the tools you need to create, configure, and evaluate your Data Agent programmatically.\n",
    "\n",
    "Run the following cell to install or upgrade the SDK in your notebook environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681343f-6f69-4bc3-8743-3c248ff27b25",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U fabric-data-agent-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d33a26-823e-410f-bfdb-bb47502344b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Connect to a Data Agent\n",
    "\n",
    "Now that our data is available in the Lakehouse, we’ll create a new **Fabric Data Agent** using the Python SDK.\n",
    "\n",
    "In this step:\n",
    "- We define a name for the agent (e.g., `\"ProductSalesDataAgent\"`)\n",
    "- Use `create_data_agent()` to create a new agent instance\n",
    "- Alternatively, use `FabricDataAgentManagement()` to connect to an existing agent with the same name\n",
    "\n",
    "This agent will be configured to understand your data and respond to natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736c52e-c465-4764-b5c9-36320addeed1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from fabric.dataagent.client import (\n",
    "    FabricDataAgentManagement,\n",
    "    create_data_agent,\n",
    "    delete_data_agent,\n",
    ")\n",
    "\n",
    "# Define the name for the Data Agent\n",
    "data_agent_name = \"AdvWorksDataAgent\"\n",
    "\n",
    "# Create a new Data Agent (run this once)\n",
    "data_agent = create_data_agent(data_agent_name)\n",
    "\n",
    "# If the Data Agent already exists, use this instead to connect:\n",
    "# data_agent = FabricDataAgentManagement(data_agent_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531d750-344c-4e84-8564-70b376c66552",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In this step, we configure the Data Agent to work with a **Lakehouse** data source.\n",
    "\n",
    "- We specify the Lakehouse name (e.g., `EvaluationLH`)\n",
    "- Optionally, we register it with the agent if it hasn’t been added yet\n",
    "- We then select specific tables from the `dbo` schema that the agent should use to answer questions\n",
    "\n",
    "These tables will form the structured foundation the agent relies on to generate accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab169252-98e1-4606-8b7a-516c9f2488ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Add a Lakehouse as the data source for the agent\n",
    "lakehouse_name = \"EvaluationLH\"\n",
    "\n",
    "# Supported types include: \"lakehouse\", \"kqldatabase\", \"datawarehouse\", or \"semanticmodel\"\n",
    "data_agent.add_datasource(lakehouse_name, type=\"lakehouse\")\n",
    "\n",
    "# Retrieve the data source object (assumes one was added)\n",
    "datasource = data_agent.get_datasources()[0]\n",
    "\n",
    "# Select relevant tables from the Lakehouse (schema: dbo)\n",
    "datasource.select(\"dbo\", \"dimcustomer\")\n",
    "datasource.select(\"dbo\", \"dimdate\")\n",
    "datasource.select(\"dbo\", \"dimgeography\")\n",
    "datasource.select(\"dbo\", \"dimproduct\")\n",
    "datasource.select(\"dbo\", \"dimproductcategory\")\n",
    "datasource.select(\"dbo\", \"dimpromotion\")\n",
    "datasource.select(\"dbo\", \"dimreseller\")\n",
    "datasource.select(\"dbo\", \"dimsalesterritory\")\n",
    "datasource.select(\"dbo\", \"factinternetsales\")\n",
    "datasource.select(\"dbo\", \"factresellersales\")\n",
    "\n",
    "# Publish the data agent\n",
    "data_agent.publish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411506b-fa48-4f68-a6b5-74bd34bf35ae",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Define ground truth questions and expected answers\n",
    "\n",
    "To evaluate the accuracy of your Data Agent, you'll need a test dataset consisting of natural language questions and their expected answers.\n",
    "\n",
    "In this step:\n",
    "- We define a small set of ground truth examples using a pandas DataFrame\n",
    "- Each row contains a `question` and the `expected_answer`\n",
    "- You can customize these examples based on the data and use cases relevant to your agent\n",
    "\n",
    "Optionally, you can load this dataset from a CSV file if you're working with a larger or pre-curated set of evaluation cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2a29f-6be6-49f9-b7c0-68f71ca11846",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame with \"question,expected_answer\". Please update the questions and expected_answers as per the requirement.\n",
    "df = pd.DataFrame(columns=[\"question\", \"expected_answer\"],\n",
    "                  data=[\n",
    "                    [\"What were our total sales in 2014?\", \"45,694.7\"],\n",
    "                    [\"What is the most sold product?\", \"Mountain-200 Black, 42\"],\n",
    "                    [\"What are the most expensive items that have never been sold?\", \"Road-450 Red, 60\"],\n",
    "                ])\n",
    "\n",
    "# You can also oad from input CSV file with data in format \"question,expected_answer\"\n",
    "# input_file_path = \"/lakehouse/default/Files/Data/Input/groundtruth.csv\"\n",
    "# df = pd.read_csv(input_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738d8aa-e598-4bb0-94ff-35cd93b47bb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configure Evaluation Parameters\n",
    "\n",
    "Before running the evaluation, we define a few optional parameters to control where and how results are stored:\n",
    "\n",
    "- `workspace_name`: (Optional) Use this if your Data Agent is located in a different workspace.\n",
    "- `table_name`: (Optional) The base name of the output table where evaluation results will be stored. Default to 'evaluation_output'. This will generate:\n",
    "  - `<table_name>`: A summary of the evaluation results.\n",
    "  - `<table_name>_steps`: A detailed log of reasoning steps for each question.\n",
    "- `data_agent_stage`: (Optional) Set to `\"sandbox\"/\"draft\"` or `\"production\"/\"publishing\"` depending on which version of the agent you want to evaluate. Default to production.\n",
    "- `max_workers`: (Optional) Maximun worker nodes that need to run parallely. Default to 5.\n",
    "- `no_of_variations`: (Optional) Number of times to evaluate each question. Default is 1.\n",
    "\n",
    "These settings help you organize and retrieve evaluation outputs from your Lakehouse environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748d60b-400f-48f1-81ee-7c6c57ae6b15",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from fabric.dataagent.evaluation import evaluate_data_agent\n",
    "\n",
    "\n",
    "# Workspace Name (Optional) if Data Agent is in different workspace\n",
    "workspace_name = None\n",
    "\n",
    "# Table name (Optional) to store the evaluation result. Default value is 'evaluation_output'\n",
    "# After evaluation there will be two tables one with provided <table_name> for evaluation output and other with <table_name>_steps for detailed steps.\n",
    "table_name = \"demo_evaluation_output\"\n",
    "\n",
    "# Data Agent stage ie., sandbox or production. Default to production.\n",
    "data_agent_stage = \"sandbox\"\n",
    "\n",
    "# Evaluation output table name\n",
    "table_name = \"demo_evaluation_output\"\n",
    "\n",
    "# Number of parallel workers to use for evaluation\n",
    "max_workers = 4  \n",
    "\n",
    "# Number of variations to generate for each question\n",
    "no_of_variations = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e51aa-23fe-4fce-9126-38ee1751b7a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "Now we're ready to evaluate the Data Agent using the ground truth dataset we defined earlier.\n",
    "\n",
    "The `evaluate_data_agent()` function will:\n",
    "- Run each question against the Data Agent\n",
    "- Compare the actual response to the expected answer\n",
    "- Log results and reasoning steps to the specified Lakehouse tables\n",
    "\n",
    "It returns a unique `evaluation_id` which you can use to retrieve summaries or detailed results later.\n",
    "\n",
    "Let's run the evaluation and capture the ID for this run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdf8c0-6b6d-4b41-b393-4001663bd2e5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the Data Agent. Returns the unique id for the evaluation run\n",
    "evaluation_id = evaluate_data_agent(df, data_agent_name, workspace_name=workspace_name, table_name=table_name, data_agent_stage=data_agent_stage, max_workers=max_workers, no_of_variations=no_of_variations)\n",
    "\n",
    "print(f\"Unique Id for the current evaluation run: {evaluation_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dda6ba-73b9-4264-bc63-f0645e639cba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## View evaluation summary\n",
    "\n",
    "After the evaluation run completes, you can retrieve a high-level summary using the `get_evaluation_summary()` function.\n",
    "\n",
    "This summary includes:\n",
    "- Total number of questions evaluated\n",
    "- Counts of correct, incorrect, and unclear responses\n",
    "- Overall accuracy metrics\n",
    "\n",
    "Use this step to quickly assess how well your Data Agent performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851e940-41d4-4308-8145-c571854c5bb4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Import the function to retrieve evaluation summaries\n",
    "from fabric.dataagent.evaluation import get_evaluation_summary\n",
    "\n",
    "# Retrieve the summary of the evaluation results using the specified table name\n",
    "# This returns a DataFrame with aggregated metrics like counts of true/false/unclear responses\n",
    "eval_summary_df = get_evaluation_summary(table_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293aa9b0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## View evaluation summary per question\n",
    "\n",
    "You can retrieve a high-level summary per question using the `get_evaluation_summary_per_question()` function.\n",
    "- `evaluation_id`: (Optional) Unique identifier of the evaluation run. If not used gives overall summary of all the evaluation runs in the output table.\n",
    "- `table_name`: (Optional) The base name of the output table where evaluation results will be stored. Default to 'evaluation_output'.\n",
    "- `verbose`: (Optional) Set to `True` to print a summary alongside the DataFrame.\n",
    "\n",
    "This summary includes:\n",
    "- Total number of questions evaluated\n",
    "- Counts of correct, incorrect, and unclear responses per question\n",
    "- Overall accuracy metrics\n",
    "\n",
    "Use this step to quickly assess how well your Data Agent performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80f1ce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the function to retrieve evaluation summaries\n",
    "from fabric.dataagent.evaluation import get_evaluation_summary_per_question\n",
    "\n",
    "# Retrieve the summary of the evaluation results per question using the specified evaluation_id and table name\n",
    "# This returns a DataFrame with aggregated metrics like counts of true/false/unclear responses\n",
    "eval_summary_df = get_evaluation_summary_per_question(evaluation_id, table_name, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7d757",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Display the DataFrame\n",
    "To avoid the column width limit, use the below line to display the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8400f47",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(eval_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6eb00-9de4-4f24-a66e-f5989290da7d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Retrieve Detailed Evaluation Results\n",
    "\n",
    "To analyze the agent's performance question-by-question, use the `get_evaluation_details()` function.\n",
    "\n",
    "This provides a detailed view of:\n",
    "- The original question\n",
    "- The expected answer\n",
    "- The agent's actual response\n",
    "- The evaluation outcome (`true`, `false`, or `unclear`)\n",
    "- A link to the Fabric thread (accessible only to the evaluator)\n",
    "\n",
    "You can also control:\n",
    "- `get_all_rows`: Set to `True` to return both successful and failed evaluations (defaults to `False`, which returns only failed cases).\n",
    "- `verbose`: Set to `True` to print a summary alongside the DataFrame.\n",
    "\n",
    "This is especially useful for debugging incorrect responses and improving your agent's accuracy over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268bc2d-3213-440a-9193-10fbcc514a4c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Import the function to retrieve detailed evaluation results\n",
    "from fabric.dataagent.evaluation import get_evaluation_details\n",
    "\n",
    "# Unique identifier for the evaluation run (already captured earlier)\n",
    "# You can hardcode an ID here if needed\n",
    "# evaluation_id = 'd36ce205-a88d-42bd-927d-260ec2e2a479'\n",
    "\n",
    "# Whether to return all evaluation results (True) or only failed ones (False, default)\n",
    "get_all_rows = True\n",
    "\n",
    "# Whether to print a summary of the evaluation results to the console (optional)\n",
    "verbose = True\n",
    "\n",
    "# Fetch detailed evaluation results as a DataFrame\n",
    "# This includes question, expected answer, actual answer, evaluation status, and diagnostic info\n",
    "eval_details_df = get_evaluation_details(\n",
    "    evaluation_id,\n",
    "    table_name,\n",
    "    get_all_rows=get_all_rows,\n",
    "    verbose=verbose\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c1f32-a6cc-4d9b-ab62-32e7665c6507",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use a custom prompt to evaluate agent responses\n",
    "\n",
    "In some cases, simple string matching may not be sufficient to determine if the agent's response is correct—especially when responses vary in format but are semantically equivalent.\n",
    "\n",
    "You can define a **custom critic prompt** using the `critic_prompt` parameter in `evaluate_data_agent()`. This prompt will be used by an LLM to decide whether the actual answer is equivalent to the expected answer.\n",
    "\n",
    "The prompt must include the following placeholders:\n",
    "- `{query}`: The original user question\n",
    "- `{expected_answer}`: The expected result\n",
    "- `{actual_answer}`: The agent's generated response\n",
    "\n",
    "Once the evaluation is complete, you can retrieve the summary results using `get_evaluation_summary()` and track the run using the printed `evaluation_id`.\n",
    "\n",
    "This method gives you more flexibility in how you assess correctness, especially for complex or domain-specific outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ca94e-45db-4421-852b-8d89da04dcb2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from fabric.dataagent.evaluation import evaluate_data_agent\n",
    "\n",
    "# Define a custom prompt to evaluate whether the agent's actual response matches the expected answer.\n",
    "# The prompt should include placeholders: {query}, {expected_answer}, and {actual_answer}\n",
    "critic_prompt = \"\"\"\n",
    "        Given the following query, expected answer, and actual answer, please determine if the actual answer is equivalent to expected answer. If they are equivalent, respond with 'yes'.\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Expected Answer:\n",
    "        {expected_answer}\n",
    "\n",
    "        Actual Answer:\n",
    "        {actual_answer}\n",
    "\n",
    "        Is the actual answer equivalent to the expected answer?\n",
    "        \"\"\"\n",
    "\n",
    "# Evaluate the Data Agent using the custom critic prompt\n",
    "# Returns a unique evaluation ID for tracking and analysis\n",
    "evaluation_id_critic = evaluate_data_agent(\n",
    "    df,\n",
    "    data_agent_name,\n",
    "    critic_prompt=critic_prompt,\n",
    "    table_name=table_name,\n",
    "    data_agent_stage=\"sandbox\"\n",
    ")\n",
    "\n",
    "# Retrieve the summary of this evaluation run\n",
    "eval_summary_df_critic = get_evaluation_summary(table_name)\n",
    "\n",
    "# Display the unique ID for reference\n",
    "print(f\"Unique Id for the current evaluation run: {evaluation_id_critic}\")\n",
    "\n",
    "eval_summary_df_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8792c-ec0c-4d80-9e3f-ecd1e8ea09c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "68491dd8-8ec1-4591-9874-29723aa54111",
    "default_lakehouse_name": "EvaluationLH",
    "default_lakehouse_workspace_id": "9524e38b-5c6e-4838-adb3-500b0a93df45",
    "known_lakehouses": [
     {
      "id": "68491dd8-8ec1-4591-9874-29723aa54111"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

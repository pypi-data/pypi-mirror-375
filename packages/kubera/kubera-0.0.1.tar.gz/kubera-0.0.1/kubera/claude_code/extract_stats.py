#!/usr/bin/env python3
"""
Extract statistics from Claude usage CSV data.

Analyzes the CSV generated by extract_trace.py to provide insights on:
- Token usage and costs
- Cache efficiency
- Session statistics
- Time-based patterns
"""

import argparse
import csv
import json
import os
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict

import humanize


def parse_timestamp(timestamp_str: str) -> datetime:
    """Parse ISO format timestamp."""
    # Handle both formats with and without microseconds
    try:
        return datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
    except:
        # Fallback for other formats
        return datetime.strptime(timestamp_str.split(".")[0], "%Y-%m-%dT%H:%M:%S")


def extract_statistics(csv_file: str) -> Dict[str, Any]:
    """Extract comprehensive statistics from the CSV file."""

    stats = {
        "overall": {
            "total_requests": 0,
            "total_input_tokens": 0,
            "total_cache_creation_tokens": 0,
            "total_cache_read_tokens": 0,
            "total_output_tokens": 0,
            "total_tokens": 0,
        },
        "sessions": defaultdict(
            lambda: {
                "requests": 0,
                "total_tokens": 0,
                "duration_minutes": 0,
                "timestamps": [],
            }
        ),
        "cache_efficiency": {"cache_token_fraction": 0.0},
    }

    rows = []

    # Read CSV file
    with open(csv_file, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)

            # Update overall stats
            stats["overall"]["total_requests"] += 1
            stats["overall"]["total_input_tokens"] += int(row["input_tokens"])
            stats["overall"]["total_cache_creation_tokens"] += int(
                row["cache_creation_input_tokens"]
            )
            stats["overall"]["total_cache_read_tokens"] += int(
                row["cache_read_input_tokens"]
            )
            stats["overall"]["total_output_tokens"] += int(row["output_tokens"])
            stats["overall"]["total_tokens"] += (
                int(row["input_tokens"])
                + int(row["output_tokens"])
                + int(row["cache_creation_input_tokens"])
                + int(row["cache_read_input_tokens"])
            )

            # Session stats
            session_id = row["sessionId"]
            stats["sessions"][session_id]["requests"] += 1
            stats["sessions"][session_id]["total_tokens"] += (
                int(row["total_input_tokens"])
                + int(row["output_tokens"])
                + int(row["cache_creation_input_tokens"])
            )

            # Track timestamps for session duration
            timestamp = parse_timestamp(row["timestamp"])
            stats["sessions"][session_id]["timestamps"].append(timestamp)

        # Cache efficiency
        if int(stats["overall"]["total_input_tokens"]) > 0:
            stats["cache_efficiency"]["cache_token_fraction"] += int(
                stats["overall"]["total_cache_read_tokens"]
            ) / int(stats["overall"]["total_tokens"])
        else:
            stats["cache_efficiency"]["cache_token_fraction"] = 0.0

    # Calculate session durations
    for session_id, session_data in stats["sessions"].items():
        if len(session_data["timestamps"]) > 1:
            duration = max(session_data["timestamps"]) - min(session_data["timestamps"])
            session_data["duration_minutes"] = duration.total_seconds() / 60
        del session_data["timestamps"]  # Remove timestamps from final output

    # Convert defaultdicts to regular dicts for JSON serialization
    stats["sessions"] = dict(stats["sessions"])

    return stats


def print_statistics(stats: Dict[str, Any]):
    """Print statistics in the specified format."""

    # Text format
    print("\n" + "=" * 60)
    print("CLAUDE CODE USAGE STATISTICS")
    print("=" * 60)

    print("\nðŸ“Š OVERALL STATISTICS")
    print("-" * 40)
    print(f"Total Requests: {humanize.intword(stats['overall']['total_requests'])}")
    print(f"Total Tokens: {humanize.intword(stats['overall']['total_tokens'])}")
    print(
        f"  - Input Tokens (neither cache read or write): {humanize.intword(stats['overall']['total_input_tokens'])}"
    )
    print(
        f"  - Input Tokens (cache creation): {humanize.intword(stats['overall']['total_cache_creation_tokens'])}"
    )
    print(
        f"  - Input Tokens (cache read): {humanize.intword(stats['overall']['total_cache_read_tokens'])}"
    )
    print(
        f"  - Output Tokens: {humanize.intword(stats['overall']['total_output_tokens'])}"
    )

    print("\nðŸ’¾ CACHE EFFICIENCY")
    print("-" * 40)
    print(
        f"Cache Token Fraction: {stats['cache_efficiency']['cache_token_fraction']:.2f}"
    )

    print("\nðŸ“… SESSION STATISTICS")
    print("-" * 40)
    total_sessions = len(stats["sessions"])
    print(f"Total Sessions: {humanize.intword(total_sessions)}")

    average_requests = (
        sum(session_data["requests"] for session_data in stats["sessions"].values())
        / total_sessions
    )
    average_total_tokens = (
        sum(session_data["total_tokens"] for session_data in stats["sessions"].values())
        / total_sessions
    )
    average_duration = (
        sum(
            session_data["duration_minutes"]
            for session_data in stats["sessions"].values()
        )
        / total_sessions
    )

    print("\nAverage Session Statistics:")
    print(f"  - Requests: {humanize.intword(average_requests)}")
    print(f"  - Total Tokens: {humanize.intword(average_total_tokens)}")
    print(f"  - Duration: {humanize.intword(average_duration)} minutes")

    print("\n" + "=" * 60)


def main():
    """Main function to process CSV and generate statistics."""
    parser = argparse.ArgumentParser(
        description="Extract statistics from Claude usage CSV."
    )
    parser.add_argument(
        "--input-file",
        type=str,
        default="./data/claude_code_trace.csv",
        help="Input CSV file path.",
    )
    parser.add_argument(
        "--output-file",
        type=str,
        default="./data/stats/claude_code_stats.json",
        help="Output file for statistics (optional, prints to stdout by default).",
    )

    args = parser.parse_args()

    # Check if input file exists
    if not os.path.exists(args.input_file):
        print(f"Error: Input file not found: {args.input_file}")
        return

    # Extract statistics
    print(f"Analyzing usage data from: {args.input_file}")

    stats = extract_statistics(args.input_file)

    # Output statistics
    if args.output_file:
        # Make sure the output directory exists
        output_dir = os.path.dirname(args.output_file)
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        # Save to file
        with open(args.output_file, "w") as f:
            json.dump(stats, f, indent=2, default=str)
        print(f"\nStatistics saved to: {args.output_file}")

    print_statistics(stats)


if __name__ == "__main__":
    main()

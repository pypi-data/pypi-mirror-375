"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° íšŒê·€ í…ŒìŠ¤íŠ¸ ë„êµ¬

HTTP ìš”ì²­ ì„±ëŠ¥ ì¸¡ì •, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§, ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ
ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ì„¤ì • ë° ë¹„êµ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import json
import os
import time
import statistics
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime, timedelta
from pathlib import Path

from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.text import Text

from .performance import PerformanceMonitor, BenchmarkResult


@dataclass
class BenchmarkBaseline:
    """ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ì„ """
    name: str
    version: str
    timestamp: datetime
    metrics: Dict[str, float]
    environment: Dict[str, Any] = field(default_factory=dict)
    description: Optional[str] = None


@dataclass
class RegressionTestConfig:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    name: str
    url: str
    method: str = "GET"
    requests: int = 100
    concurrency: int = 10
    timeout: float = 30.0
    acceptable_regression_percent: float = 10.0  # í—ˆìš© ê°€ëŠ¥í•œ ì„±ëŠ¥ ì €í•˜ %
    critical_regression_percent: float = 25.0    # ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ %
    headers: Dict[str, str] = field(default_factory=dict)
    data: Union[str, Dict[str, Any]] = field(default_factory=dict)


@dataclass
class RegressionTestResult:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    config_name: str
    baseline: BenchmarkBaseline
    current_result: BenchmarkResult
    regression_detected: bool
    regression_severity: str  # "none", "acceptable", "critical"
    performance_changes: Dict[str, Dict[str, float]]  # metric -> {baseline, current, change_percent}
    timestamp: datetime = field(default_factory=datetime.now)


class BenchmarkManager:
    """
    ë²¤ì¹˜ë§ˆí¬ ê´€ë¦¬ ë° íšŒê·€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤

    ì„±ëŠ¥ ê¸°ì¤€ì„  ì„¤ì •, ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰, íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, baseline_dir: str = ".benchmarks", console: Optional[Console] = None):
        self.baseline_dir = Path(baseline_dir)
        self.baseline_dir.mkdir(exist_ok=True)
        self.console = console or Console()
        self.performance_monitor = PerformanceMonitor(console)
        self.baselines: Dict[str, BenchmarkBaseline] = {}
        self.regression_configs: Dict[str, RegressionTestConfig] = {}

        # ê¸°ì¡´ ê¸°ì¤€ì„  ë¡œë“œ
        self._load_baselines()

    def _load_baselines(self):
        """ì €ì¥ëœ ê¸°ì¤€ì„ ë“¤ì„ ë¡œë“œ"""
        baseline_files = self.baseline_dir.glob("*.json")

        for baseline_file in baseline_files:
            try:
                with open(baseline_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                baseline = BenchmarkBaseline(
                    name=data['name'],
                    version=data['version'],
                    timestamp=datetime.fromisoformat(data['timestamp']),
                    metrics=data['metrics'],
                    environment=data.get('environment', {}),
                    description=data.get('description')
                )

                self.baselines[baseline.name] = baseline

            except Exception as e:
                self.console.print(f"[yellow]ê¸°ì¤€ì„  ë¡œë“œ ì‹¤íŒ¨ {baseline_file}: {e}[/yellow]")

    def create_baseline(
        self,
        name: str,
        version: str,
        url: str,
        method: str = "GET",
        requests: int = 200,
        concurrency: int = 10,
        description: Optional[str] = None,
        **kwargs
    ) -> BenchmarkBaseline:
        """ìƒˆë¡œìš´ ì„±ëŠ¥ ê¸°ì¤€ì„  ìƒì„±"""
        self.console.print(f"[blue]ì„±ëŠ¥ ê¸°ì¤€ì„  ìƒì„± ì¤‘: {name} (v{version})[/blue]")

        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        result = asyncio.run(
            self.performance_monitor.run_benchmark(
                name=f"{name}_baseline",
                url=url,
                method=method,
                concurrent_requests=concurrency,
                total_requests=requests,
                **kwargs
            )
        )

        # í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
        import platform
        import psutil

        environment = {
            "python_version": platform.python_version(),
            "platform": platform.platform(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "timestamp": datetime.now().isoformat()
        }

        # ê¸°ì¤€ì„  ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = {
            "requests_per_second": result.requests_per_second,
            "avg_response_time": result.avg_response_time,
            "p95_response_time": result.percentiles.get(95, 0),
            "p99_response_time": result.percentiles.get(99, 0),
            "error_rate": result.error_rate,
            "throughput_bytes_per_sec": result.throughput_bytes_per_sec,
            "peak_memory_mb": result.memory_usage.get('peak', 0),
            "avg_memory_mb": result.memory_usage.get('average', 0),
            "peak_cpu_percent": result.cpu_usage.get('peak', 0),
            "avg_cpu_percent": result.cpu_usage.get('average', 0)
        }

        baseline = BenchmarkBaseline(
            name=name,
            version=version,
            timestamp=datetime.now(),
            metrics=metrics,
            environment=environment,
            description=description
        )

        # ê¸°ì¤€ì„  ì €ì¥
        self._save_baseline(baseline)
        self.baselines[name] = baseline

        self.console.print(f"[green]ì„±ëŠ¥ ê¸°ì¤€ì„ ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {name} (v{version})[/green]")
        self._display_baseline(baseline)

        return baseline

    def _save_baseline(self, baseline: BenchmarkBaseline):
        """ê¸°ì¤€ì„ ì„ íŒŒì¼ë¡œ ì €ì¥"""
        filename = self.baseline_dir / f"{baseline.name}_{baseline.version}.json"

        data = {
            "name": baseline.name,
            "version": baseline.version,
            "timestamp": baseline.timestamp.isoformat(),
            "metrics": baseline.metrics,
            "environment": baseline.environment,
            "description": baseline.description
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _display_baseline(self, baseline: BenchmarkBaseline):
        """ê¸°ì¤€ì„  ì •ë³´ ì¶œë ¥"""
        table = Table(title=f"ì„±ëŠ¥ ê¸°ì¤€ì„ : {baseline.name} v{baseline.version}")
        table.add_column("ë©”íŠ¸ë¦­", style="cyan")
        table.add_column("ê°’", style="magenta")

        table.add_row("ìƒì„± ì‹œê°„", baseline.timestamp.strftime("%Y-%m-%d %H:%M:%S"))
        if baseline.description:
            table.add_row("ì„¤ëª…", baseline.description)

        table.add_row("", "")  # êµ¬ë¶„ì„ 

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics_display = {
            "requests_per_second": ("ì´ˆë‹¹ ìš”ì²­ ìˆ˜", "RPS"),
            "avg_response_time": ("í‰ê·  ì‘ë‹µì‹œê°„", "ì´ˆ"),
            "p95_response_time": ("95th ë°±ë¶„ìœ„ìˆ˜", "ì´ˆ"),
            "p99_response_time": ("99th ë°±ë¶„ìœ„ìˆ˜", "ì´ˆ"),
            "error_rate": ("ì—ëŸ¬ìœ¨", "%"),
            "throughput_bytes_per_sec": ("ì²˜ë¦¬ëŸ‰", "bytes/s"),
            "peak_memory_mb": ("ìµœëŒ€ ë©”ëª¨ë¦¬", "MB"),
            "avg_memory_mb": ("í‰ê·  ë©”ëª¨ë¦¬", "MB"),
            "peak_cpu_percent": ("ìµœëŒ€ CPU", "%"),
            "avg_cpu_percent": ("í‰ê·  CPU", "%")
        }

        for key, (label, unit) in metrics_display.items():
            if key in baseline.metrics:
                value = baseline.metrics[key]
                if key.endswith("_time"):
                    table.add_row(label, f"{value:.3f} {unit}")
                elif key.endswith("_rate") or key.endswith("_percent"):
                    table.add_row(label, f"{value:.2f} {unit}")
                elif key == "throughput_bytes_per_sec":
                    table.add_row(label, f"{value/1024:.2f} KB/s")
                else:
                    table.add_row(label, f"{value:.2f} {unit}")

        self.console.print(table)

    def list_baselines(self) -> List[BenchmarkBaseline]:
        """ì €ì¥ëœ ê¸°ì¤€ì„  ëª©ë¡ ë°˜í™˜"""
        return list(self.baselines.values())

    def get_baseline(self, name: str) -> Optional[BenchmarkBaseline]:
        """íŠ¹ì • ê¸°ì¤€ì„  ë°˜í™˜"""
        return self.baselines.get(name)

    def delete_baseline(self, name: str, version: Optional[str] = None):
        """ê¸°ì¤€ì„  ì‚­ì œ"""
        if name not in self.baselines:
            self.console.print(f"[red]ê¸°ì¤€ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {name}[/red]")
            return

        baseline = self.baselines[name]

        # íŒŒì¼ ì‚­ì œ
        if version:
            filename = self.baseline_dir / f"{name}_{version}.json"
        else:
            filename = self.baseline_dir / f"{name}_{baseline.version}.json"

        if filename.exists():
            filename.unlink()

        # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        del self.baselines[name]

        self.console.print(f"[green]ê¸°ì¤€ì„ ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤: {name}[/green]")

    def add_regression_test(self, config: RegressionTestConfig):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì • ì¶”ê°€"""
        self.regression_configs[config.name] = config
        self.console.print(f"[green]íšŒê·€ í…ŒìŠ¤íŠ¸ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤: {config.name}[/green]")

    def remove_regression_test(self, name: str):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì • ì œê±°"""
        if name in self.regression_configs:
            del self.regression_configs[name]
            self.console.print(f"[green]íšŒê·€ í…ŒìŠ¤íŠ¸ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤: {name}[/green]")
        else:
            self.console.print(f"[red]íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {name}[/red]")

    async def run_regression_test(self, test_name: str) -> RegressionTestResult:
        """ë‹¨ì¼ íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if test_name not in self.regression_configs:
            raise ValueError(f"íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_name}")

        config = self.regression_configs[test_name]

        # ê¸°ì¤€ì„  í™•ì¸
        if test_name not in self.baselines:
            raise ValueError(f"ê¸°ì¤€ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_name}")

        baseline = self.baselines[test_name]

        self.console.print(f"[blue]íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘: {test_name}[/blue]")

        # í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •
        current_result = await self.performance_monitor.run_benchmark(
            name=f"{test_name}_regression_test",
            url=config.url,
            method=config.method,
            concurrent_requests=config.concurrency,
            total_requests=config.requests,
            timeout=config.timeout,
            headers=config.headers,
            data=config.data
        )

        # ì„±ëŠ¥ ë³€í™” ë¶„ì„
        performance_changes = self._analyze_performance_changes(baseline, current_result)

        # íšŒê·€ ê°ì§€
        regression_detected, severity = self._detect_regression(
            performance_changes,
            config.acceptable_regression_percent,
            config.critical_regression_percent
        )

        result = RegressionTestResult(
            config_name=test_name,
            baseline=baseline,
            current_result=current_result,
            regression_detected=regression_detected,
            regression_severity=severity,
            performance_changes=performance_changes
        )

        # ê²°ê³¼ ì¶œë ¥
        self._display_regression_result(result)

        return result

    async def run_all_regression_tests(self) -> List[RegressionTestResult]:
        """ëª¨ë“  íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        if not self.regression_configs:
            self.console.print("[yellow]ì‹¤í–‰í•  íšŒê·€ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return []

        self.console.print(f"[blue]{len(self.regression_configs)}ê°œì˜ íšŒê·€ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.[/blue]")

        results = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            task = progress.add_task("íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", total=len(self.regression_configs))

            for test_name in self.regression_configs:
                try:
                    result = await self.run_regression_test(test_name)
                    results.append(result)
                except Exception as e:
                    self.console.print(f"[red]íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ {test_name}: {e}[/red]")

                progress.advance(task)

        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self._display_regression_summary(results)

        return results

    def _analyze_performance_changes(
        self,
        baseline: BenchmarkBaseline,
        current: BenchmarkResult
    ) -> Dict[str, Dict[str, float]]:
        """ì„±ëŠ¥ ë³€í™” ë¶„ì„"""
        changes = {}

        # í˜„ì¬ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        current_metrics = {
            "requests_per_second": current.requests_per_second,
            "avg_response_time": current.avg_response_time,
            "p95_response_time": current.percentiles.get(95, 0),
            "p99_response_time": current.percentiles.get(99, 0),
            "error_rate": current.error_rate,
            "throughput_bytes_per_sec": current.throughput_bytes_per_sec,
            "peak_memory_mb": current.memory_usage.get('peak', 0),
            "avg_memory_mb": current.memory_usage.get('average', 0),
            "peak_cpu_percent": current.cpu_usage.get('peak', 0),
            "avg_cpu_percent": current.cpu_usage.get('average', 0)
        }

        # ê° ë©”íŠ¸ë¦­ë³„ ë³€í™”ìœ¨ ê³„ì‚°
        for metric_name in baseline.metrics:
            if metric_name in current_metrics:
                baseline_value = baseline.metrics[metric_name]
                current_value = current_metrics[metric_name]

                if baseline_value > 0:
                    change_percent = ((current_value - baseline_value) / baseline_value) * 100
                else:
                    change_percent = 0

                changes[metric_name] = {
                    "baseline": baseline_value,
                    "current": current_value,
                    "change_percent": change_percent
                }

        return changes

    def _detect_regression(
        self,
        changes: Dict[str, Dict[str, float]],
        acceptable_threshold: float,
        critical_threshold: float
    ) -> Tuple[bool, str]:
        """íšŒê·€ ê°ì§€"""
        # ì¤‘ìš”í•œ ë©”íŠ¸ë¦­ë“¤ê³¼ ê·¸ë“¤ì˜ ë°©í–¥ì„± (True = ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, False = ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        important_metrics = {
            "requests_per_second": True,
            "avg_response_time": False,
            "p95_response_time": False,
            "error_rate": False
        }

        max_regression = 0
        regression_detected = False
        severity = "none"

        for metric, is_higher_better in important_metrics.items():
            if metric not in changes:
                continue

            change_percent = changes[metric]["change_percent"]

            # íšŒê·€ íŒë‹¨ (ì„±ëŠ¥ì´ ë‚˜ë¹ ì§„ ê²½ìš°)
            if is_higher_better:
                # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ (RPS ë“±)
                regression_percent = -change_percent if change_percent < 0 else 0
            else:
                # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­ (ì‘ë‹µì‹œê°„, ì—ëŸ¬ìœ¨ ë“±)
                regression_percent = change_percent if change_percent > 0 else 0

            max_regression = max(max_regression, regression_percent)

        if max_regression >= critical_threshold:
            regression_detected = True
            severity = "critical"
        elif max_regression >= acceptable_threshold:
            regression_detected = True
            severity = "acceptable"

        return regression_detected, severity

    def _display_regression_result(self, result: RegressionTestResult):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        # ê²°ê³¼ ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ
        if result.regression_severity == "critical":
            status_color = "red"
            status_icon = "ğŸš¨"
            status_text = "ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜"
        elif result.regression_severity == "acceptable":
            status_color = "yellow"
            status_icon = "âš ï¸"
            status_text = "ì„±ëŠ¥ ì €í•˜ ê°ì§€"
        else:
            status_color = "green"
            status_icon = "âœ…"
            status_text = "ì„±ëŠ¥ ì •ìƒ"

        self.console.print(f"\n[{status_color}]{status_icon} {result.config_name}: {status_text}[/{status_color}]")

        # ì„±ëŠ¥ ë³€í™” í…Œì´ë¸”
        table = Table(title=f"ì„±ëŠ¥ ë³€í™”: {result.config_name}")
        table.add_column("ë©”íŠ¸ë¦­", style="cyan")
        table.add_column("ê¸°ì¤€ì„ ", justify="right")
        table.add_column("í˜„ì¬", justify="right")
        table.add_column("ë³€í™”", justify="right")

        for metric, change_data in result.performance_changes.items():
            baseline_val = change_data["baseline"]
            current_val = change_data["current"]
            change_percent = change_data["change_percent"]

            # ë³€í™”ìœ¨ ìƒ‰ìƒ
            if abs(change_percent) < 5:
                change_color = "white"
            elif change_percent > 0:
                if metric in ["requests_per_second", "throughput_bytes_per_sec"]:
                    change_color = "green"  # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                else:
                    change_color = "red"    # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
            else:
                if metric in ["requests_per_second", "throughput_bytes_per_sec"]:
                    change_color = "red"    # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­
                else:
                    change_color = "green"  # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ë©”íŠ¸ë¦­

            # ê°’ í¬ë§·íŒ…
            if metric.endswith("_time"):
                baseline_str = f"{baseline_val:.3f}s"
                current_str = f"{current_val:.3f}s"
            elif metric.endswith("_rate") or metric.endswith("_percent"):
                baseline_str = f"{baseline_val:.2f}%"
                current_str = f"{current_val:.2f}%"
            elif metric == "throughput_bytes_per_sec":
                baseline_str = f"{baseline_val/1024:.2f} KB/s"
                current_str = f"{current_val/1024:.2f} KB/s"
            else:
                baseline_str = f"{baseline_val:.2f}"
                current_str = f"{current_val:.2f}"

            change_str = f"[{change_color}]{change_percent:+.1f}%[/{change_color}]"

            table.add_row(
                metric.replace("_", " ").title(),
                baseline_str,
                current_str,
                change_str
            )

        self.console.print(table)

    def _display_regression_summary(self, results: List[RegressionTestResult]):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ì „ì²´ ê²°ê³¼ ìš”ì•½"""
        if not results:
            return

        # í†µê³„ ê³„ì‚°
        total_tests = len(results)
        critical_regressions = sum(1 for r in results if r.regression_severity == "critical")
        acceptable_regressions = sum(1 for r in results if r.regression_severity == "acceptable")
        passed_tests = total_tests - critical_regressions - acceptable_regressions

        # ìš”ì•½ í…Œì´ë¸”
        summary_table = Table(title="íšŒê·€ í…ŒìŠ¤íŠ¸ ìš”ì•½")
        summary_table.add_column("ìƒíƒœ", style="cyan")
        summary_table.add_column("ê°œìˆ˜", justify="right")
        summary_table.add_column("ë¹„ìœ¨", justify="right")

        summary_table.add_row(
            "[green]âœ… í†µê³¼[/green]",
            str(passed_tests),
            f"{(passed_tests/total_tests*100):.1f}%"
        )
        summary_table.add_row(
            "[yellow]âš ï¸ ì„±ëŠ¥ ì €í•˜[/yellow]",
            str(acceptable_regressions),
            f"{(acceptable_regressions/total_tests*100):.1f}%"
        )
        summary_table.add_row(
            "[red]ğŸš¨ ì‹¬ê°í•œ ì €í•˜[/red]",
            str(critical_regressions),
            f"{(critical_regressions/total_tests*100):.1f}%"
        )

        self.console.print(summary_table)

        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ëª©ë¡
        if critical_regressions > 0 or acceptable_regressions > 0:
            failed_table = Table(title="ì„±ëŠ¥ ì €í•˜ ê°ì§€ëœ í…ŒìŠ¤íŠ¸")
            failed_table.add_column("í…ŒìŠ¤íŠ¸", style="cyan")
            failed_table.add_column("ì‹¬ê°ë„", justify="center")
            failed_table.add_column("ì£¼ìš” ë¬¸ì œ", style="red")

            for result in results:
                if result.regression_detected:
                    # ê°€ì¥ í° íšŒê·€ë¥¼ ì°¾ê¸°
                    max_regression = 0
                    worst_metric = ""

                    for metric, change_data in result.performance_changes.items():
                        change_percent = abs(change_data["change_percent"])
                        if change_percent > max_regression:
                            max_regression = change_percent
                            worst_metric = metric

                    severity_display = {
                        "critical": "[red]ğŸš¨ ì‹¬ê°[/red]",
                        "acceptable": "[yellow]âš ï¸ ì£¼ì˜[/yellow]"
                    }

                    failed_table.add_row(
                        result.config_name,
                        severity_display[result.regression_severity],
                        f"{worst_metric}: {max_regression:.1f}% ì €í•˜"
                    )

            self.console.print(failed_table)

    def export_regression_results(self, results: List[RegressionTestResult], filename: str):
        """íšŒê·€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        data = {
            "export_timestamp": datetime.now().isoformat(),
            "total_tests": len(results),
            "results": []
        }

        for result in results:
            data["results"].append({
                "config_name": result.config_name,
                "timestamp": result.timestamp.isoformat(),
                "regression_detected": result.regression_detected,
                "regression_severity": result.regression_severity,
                "baseline": {
                    "name": result.baseline.name,
                    "version": result.baseline.version,
                    "timestamp": result.baseline.timestamp.isoformat(),
                    "metrics": result.baseline.metrics
                },
                "current_result": {
                    "requests_per_second": result.current_result.requests_per_second,
                    "avg_response_time": result.current_result.avg_response_time,
                    "error_rate": result.current_result.error_rate,
                    "percentiles": result.current_result.percentiles
                },
                "performance_changes": result.performance_changes
            })

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        self.console.print(f"[green]íšŒê·€ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")


# CLI í†µí•©ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜ë“¤
async def create_performance_baseline(
    name: str,
    version: str,
    url: str,
    requests: int = 200,
    concurrency: int = 10,
    baseline_dir: str = ".benchmarks"
) -> BenchmarkBaseline:
    """ì„±ëŠ¥ ê¸°ì¤€ì„  ìƒì„± (CLIìš©)"""
    manager = BenchmarkManager(baseline_dir)
    return manager.create_baseline(name, version, url, requests=requests, concurrency=concurrency)


async def run_performance_regression_test(
    config_file: str,
    baseline_dir: str = ".benchmarks"
) -> List[RegressionTestResult]:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (CLIìš©)"""
    manager = BenchmarkManager(baseline_dir)

    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open(config_file, 'r', encoding='utf-8') as f:
        config_data = json.load(f)

    # íšŒê·€ í…ŒìŠ¤íŠ¸ ì„¤ì • ì¶”ê°€
    for test_config in config_data.get('regression_tests', []):
        config = RegressionTestConfig(**test_config)
        manager.add_regression_test(config)

    # ëª¨ë“  íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    return await manager.run_all_regression_tests()

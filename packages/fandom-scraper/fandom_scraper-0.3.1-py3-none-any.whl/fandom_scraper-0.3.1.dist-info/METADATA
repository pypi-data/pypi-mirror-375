Metadata-Version: 2.3
Name: fandom-scraper
Version: 0.3.1
Summary: A simple AI (span-marker) powered fandom scraper
Author: AnthonyP57
Author-email: AnthonyP57 <antonipawlowicz123@gmail.com>
Requires-Dist: fandom-py==0.2.1
Requires-Dist: span-marker==1.7.0
Requires-Dist: transformers==4.47.0
Requires-Dist: torch==2.5.1
Requires-Dist: tokenizers==0.21.0
Requires-Dist: aiohttp==3.11.9
Requires-Dist: aiohappyeyeballs==2.4.4
Requires-Dist: aiosignal==1.3.1
Requires-Dist: beautifulsoup4==4.13.5
Requires-Dist: regex==2024.11.6
Requires-Dist: tqdm==4.67.1
Requires-Dist: numpy==2.1.3
Requires-Dist: pandas==2.2.3
Requires-Dist: scikit-learn==1.6.0
Requires-Dist: scipy==1.14.1
Requires-Dist: packaging==24.2
Requires-Dist: pyyaml==6.0.2
Requires-Dist: attrs==24.2.0
Requires-Dist: filelock==3.13.1
Requires-Dist: fsspec==2024.9.0
Requires-Dist: networkx==3.3
Requires-Dist: propcache==0.2.1
Requires-Dist: psutil==6.1.0
Requires-Dist: pyarrow==18.1.0
Requires-Dist: requests==2.32.3
Requires-Dist: typing-extensions==4.12.2
Requires-Dist: tzdata==2024.2
Requires-Dist: urllib3==2.2.3
Requires-Dist: yarl==1.18.3
Requires-Python: >=3.11
Description-Content-Type: text/markdown

# Fandom Scraper
A simple AI (span marker) powered fandom scraper.

> [!NOTE]  
> This package is a part of the [Cirilla project](https://github.com/AnthonyP57/Cirilla---a-LLM-made-on-a-budget)

> [!IMPORTANT]  
> In order to use the package an nvidia gpu is required.
## Installation
```bash
# (recommended)
uv add fandom-scraper

# or
pip install fandom-scraper
```
## Usage
The usage is very simple, the function requires path with so-called seeds to start scraping e.g. `examples/witcher_json/witcher_1.json`
```json
[
    "Geralt of Rivia", "Triss Merigold", "Vesemir", "Leo", "Lambert", 
    "Eskel", "Alvin", "Shani", "Zoltan Chivay", "Dandelion (Jaskier)", 
    "King Foltest", "Adda the White",

    "Jacques de Aldersberg", "Azar Javed", "Professor (leader of Salamandra)", 
    ...
]
```
and later uses sugesions provided by an Named Entity Recognition (NER) model. The script saves the scraped pages and instructions into respective folders.
```python
from fandom_scraper import scrape_fandom
in_path = Path("./examples/witcher_json")
out_path = Path("./examples/async_fandom")
instruct_path = Path("./examples/async_fandom_instruct")

scrape_fandom(in_path, out_path, instruct_path)
```
See `examples/async_fandom/` and `examples/async_fandom_instruct/` for more examples.
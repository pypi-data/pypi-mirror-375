Metadata-Version: 2.4
Name: crawlo
Version: 1.1.3
Summary: Crawlo æ˜¯ä¸€æ¬¾åŸºäºå¼‚æ­¥IOçš„é«˜æ€§èƒ½Pythonçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼æŠ“å–ã€‚
Home-page: https://github.com/crawl-coder/Crawlo.git
Author: crawl-coder
Author-email: crawlo@qq.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: aiohttp>=3.12.14
Requires-Dist: aiomysql>=0.2.0
Requires-Dist: aioredis>=2.0.1
Requires-Dist: asyncmy>=0.2.10
Requires-Dist: cssselect>=1.2.0
Requires-Dist: dateparser>=1.2.2
Requires-Dist: httpx[http2]>=0.27.0
Requires-Dist: curl-cffi>=0.13.0
Requires-Dist: lxml>=5.2.1
Requires-Dist: motor>=3.7.0
Requires-Dist: parsel>=1.9.1
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pymongo>=4.11
Requires-Dist: PyMySQL>=1.1.1
Requires-Dist: python-dateutil>=2.9.0.post0
Requires-Dist: redis>=6.2.0
Requires-Dist: requests>=2.32.4
Requires-Dist: six>=1.17.0
Requires-Dist: ujson>=5.9.0
Requires-Dist: urllib3>=2.5.0
Requires-Dist: w3lib>=2.1.2
Requires-Dist: rich>=14.1.0
Requires-Dist: astor>=0.8.1
Requires-Dist: watchdog>=6.0.0
Provides-Extra: render
Requires-Dist: webdriver-manager>=4.0.0; extra == "render"
Requires-Dist: playwright; extra == "render"
Requires-Dist: selenium>=3.141.0; extra == "render"
Provides-Extra: all
Requires-Dist: bitarray>=1.5.3; extra == "all"
Requires-Dist: PyExecJS>=1.5.1; extra == "all"
Requires-Dist: pymongo>=3.10.1; extra == "all"
Requires-Dist: redis-py-cluster>=2.1.0; extra == "all"
Requires-Dist: webdriver-manager>=4.0.0; extra == "all"
Requires-Dist: playwright; extra == "all"
Requires-Dist: selenium>=3.141.0; extra == "all"

# ğŸ•·ï¸ Crawlo - æ™ºèƒ½å¼‚æ­¥çˆ¬è™«æ¡†æ¶

> ä¸€ä¸ªç°ä»£åŒ–ã€é«˜æ€§èƒ½çš„ Python å¼‚æ­¥çˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼æ¨¡å¼ï¼Œå¼€ç®±å³ç”¨ã€‚

ğŸš€ **æ ¸å¿ƒç‰¹è‰²**ï¼šé»˜è®¤å•æœºæ¨¡å¼ï¼Œä¸€é”®åˆ†å¸ƒå¼ï¼Œé…ç½®ä¼˜é›…ï¼Œæ‰©å±•çµæ´»ã€‚

---

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ è¿è¡Œæ¨¡å¼
- **å•æœºæ¨¡å¼**ï¼ˆé»˜è®¤ï¼‰ï¼šé›¶é…ç½®å¯åŠ¨ï¼Œé€‚åˆå¼€å‘å’Œä¸­å°è§„æ¨¡çˆ¬å–
- **åˆ†å¸ƒå¼æ¨¡å¼**ï¼šRedis é˜Ÿåˆ—ï¼Œå¤šèŠ‚ç‚¹ååŒï¼Œé€‚åˆå¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ
- **è‡ªåŠ¨æ¨¡å¼**ï¼šæ™ºèƒ½æ£€æµ‹ Redis å¯ç”¨æ€§ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³è¿è¡Œæ–¹å¼

### ğŸ› ï¸ å¼€å‘å‹å¥½
- âœ… **å‘½ä»¤è¡Œé©±åŠ¨**ï¼š`crawlo startproject`ã€`crawlo genspider`ã€`crawlo run`
- âœ… **è‡ªåŠ¨å‘ç°çˆ¬è™«**ï¼šæ— éœ€æ‰‹åŠ¨æ³¨å†Œï¼Œè‡ªåŠ¨åŠ è½½ `spiders/` æ¨¡å—
- âœ… **æ™ºèƒ½é…ç½®ç³»ç»Ÿ**ï¼šé…ç½®å·¥å‚ + é“¾å¼è°ƒç”¨ + é¢„è®¾é…ç½®
- âœ… **çµæ´»è¿è¡Œå‚æ•°**ï¼š`--env`ã€`--concurrency`ã€`--debug`ã€`--distributed`

### âš¡ é«˜æ€§èƒ½æ¶æ„
- âœ… **å¼‚æ­¥æ ¸å¿ƒ**ï¼šåŸºäº `asyncio` å®ç°é«˜å¹¶å‘æŠ“å–
- âœ… **å¤šä¸‹è½½å™¨æ”¯æŒ**ï¼šaiohttpã€httpxã€curl-cffiï¼ˆæµè§ˆå™¨æŒ‡çº¹ï¼‰
- âœ… **æ™ºèƒ½ä¸­é—´ä»¶**ï¼šè¯·æ±‚å»é‡ã€å»¶è¿Ÿæ§åˆ¶ã€é‡è¯•æœºåˆ¶ã€ä»£ç†æ”¯æŒ
- âœ… **åˆ†å¸ƒå¼å»é‡**ï¼šRedis åˆ†å¸ƒå¼å»é‡ï¼Œé¿å…é‡å¤çˆ¬å–

### ğŸ“Š ç›‘æ§ä¸ç®¡ç†
- âœ… **å®æ—¶ç»Ÿè®¡**ï¼šçˆ¬å–è¿›åº¦ã€æˆåŠŸç‡ã€é”™è¯¯ç»Ÿè®¡
- âœ… **æ—¥å¿—ç³»ç»Ÿ**ï¼šç»“æ„åŒ–æ—¥å¿—è¾“å‡ºï¼Œæ”¯æŒæ–‡ä»¶å’Œæ§åˆ¶å°
- âœ… **å¥åº·æ£€æŸ¥**ï¼š`crawlo check` éªŒè¯çˆ¬è™«å®šä¹‰æ˜¯å¦åˆè§„
- âœ… **æ€§èƒ½åˆ†æ**ï¼š`crawlo stats` æŸ¥çœ‹å†å²è¿è¡ŒæŒ‡æ ‡

---

## ğŸŒ è¯­è¨€é€‰æ‹© / Language

- [ä¸­æ–‡æ–‡æ¡£ (é»˜è®¤)](#ä¸­æ–‡æ–‡æ¡£)
- [English Documentation](#english-documentation)

---

## ğŸ“š ä¸­æ–‡æ–‡æ¡£

è¯¦ç»†çš„æ¡†æ¶æ–‡æ¡£ç°å·²å¯ç”¨ï¼Œæä¾›ä¸­è‹±æ–‡åŒè¯­ç‰ˆæœ¬ï¼Œé»˜è®¤ä½¿ç”¨ä¸­æ–‡ï¼š

### å¿«é€Ÿå…¥é—¨
- [å¿«é€Ÿå…¥é—¨æŒ‡å—](docs/quick_start_guide_zh.md) - å¿«é€Ÿä¸Šæ‰‹ Crawlo
- [æ¡†æ¶å®Œæ•´æ–‡æ¡£](docs/crawlo_framework_documentation_zh.md) - æ¡†æ¶æ‰€æœ‰ç‰¹æ€§çš„ç»¼åˆæŒ‡å—
- [API å‚è€ƒ](docs/api_reference_zh.md) - æ‰€æœ‰ç±»å’Œæ–¹æ³•çš„è¯¦ç»†æ–‡æ¡£

### é«˜çº§ä¸»é¢˜
- [åˆ†å¸ƒå¼çˆ¬å–æ•™ç¨‹](docs/distributed_crawling_tutorial_zh.md) - åˆ†å¸ƒå¼çˆ¬å–çš„å®Œæ•´æŒ‡å—
- [é…ç½®æœ€ä½³å®è·µ](docs/configuration_best_practices_zh.md) - é…ç½® Crawlo é¡¹ç›®çš„æŒ‡å—
- [å»é‡ç®¡é“æŒ‡å—](docs/deduplication_pipelines_guide.md) - æ‰€æœ‰å»é‡ç®¡é“çš„è¯¦ç»†æŒ‡å—
- [å»é‡é…ç½®è¯´æ˜](docs/deduplication_configuration_zh.md) - å¦‚ä½•ä¸ºä¸åŒæ¨¡å¼é…ç½®å»é‡
- [ç¤ºä¾‹é¡¹ç›®](examples/) - çœŸå®é¡¹ç›®çš„å®Œæ•´ç¤ºä¾‹

---

## ğŸ“š English Documentation

Comprehensive framework documentation is now available in both Chinese and English, with Chinese as the default:

### Getting Started
- [Quick Start Guide](docs/quick_start_guide.md) - Get up and running with Crawlo quickly
- [Framework Documentation](docs/crawlo_framework_documentation.md) - Comprehensive guide to all framework features
- [API Reference](docs/api_reference.md) - Detailed documentation of all classes and methods

### Advanced Topics
- [Distributed Crawling Tutorial](docs/distributed_crawling_tutorial.md) - Complete guide to setting up distributed crawling
- [Configuration Best Practices](docs/configuration_best_practices.md) - Guidelines for configuring Crawlo projects
- [Deduplication Pipelines Guide](docs/deduplication_pipelines_guide.md) - Detailed guide to all deduplication pipelines
- [Deduplication Configuration](docs/deduplication_configuration.md) - How to configure deduplication for different modes

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…æ¡†æ¶

```bash
# ä»æºç å®‰è£…ï¼ˆæ¨èï¼‰
git clone https://github.com/crawl-coder/Crawlo.git
cd crawlo
pip install -e .

# æˆ–ç›´æ¥å®‰è£…ï¼ˆå¼€å‘ä¸­ï¼‰
pip install crawlo
```

### 2. åˆ›å»ºé¡¹ç›®

```bash
# åˆ›å»ºæ–°é¡¹ç›®
crawlo startproject myproject
cd myproject

# é¡¹ç›®ç»“æ„
# myproject/
# â”œâ”€â”€ crawlo.cfg          # é¡¹ç›®é…ç½®
# â”œâ”€â”€ myproject/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ settings.py     # è®¾ç½®æ–‡ä»¶
# â”‚   â”œâ”€â”€ items.py        # æ•°æ®é¡¹å®šä¹‰
# â”‚   â””â”€â”€ spiders/        # çˆ¬è™«ç›®å½•
# â””â”€â”€ run.py              # è¿è¡Œè„šæœ¬
```

### 3. ç”Ÿæˆçˆ¬è™«

```bash
# ç”Ÿæˆçˆ¬è™«æ¨¡æ¿
crawlo genspider example example.com
```

ç”Ÿæˆçš„çˆ¬è™«ä»£ç ï¼š

```python
from crawlo import Spider, Request
from myproject.items import ExampleItem

class ExampleSpider(Spider):
    name = "example"
    allowed_domains = ["example.com"]
    start_urls = ["https://example.com"]
    
    def parse(self, response):
        # æå–æ•°æ®
        item = ExampleItem()
        item['title'] = response.css('title::text').get()
        item['url'] = response.url
        yield item
        
        # è·Ÿè¿›é“¾æ¥
        for link in response.css('a::attr(href)').getall():
            yield Request(url=response.urljoin(link), callback=self.parse)
```

### 4. è¿è¡Œçˆ¬è™«

```bash
# ğŸ  å•æœºæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
python run.py example

# ğŸŒ åˆ†å¸ƒå¼æ¨¡å¼
python run.py example --distributed

# ğŸ› ï¸ å¼€å‘ç¯å¢ƒ
python run.py example --env development --debug

# âš¡ è‡ªå®šä¹‰å¹¶å‘
python run.py example --concurrency 20 --delay 0.5

# ğŸ”„ ä½¿ç”¨é¢„è®¾é…ç½®
python run.py example --env production
```

---

## ğŸ›ï¸ é…ç½®ç³»ç»Ÿ

### ä¼ ç»Ÿé…ç½®æ–¹å¼

```python
# settings.py
PROJECT_NAME = 'myproject'
CONCURRENCY = 16
DOWNLOAD_DELAY = 1.0
QUEUE_TYPE = 'memory'  # å•æœºæ¨¡å¼
# QUEUE_TYPE = 'redis'   # åˆ†å¸ƒå¼æ¨¡å¼
```

### ğŸ†• æ™ºèƒ½é…ç½®å·¥å‚

```python
from crawlo.config import CrawloConfig

# å•æœºæ¨¡å¼
config = CrawloConfig.standalone().set_concurrency(16)

# åˆ†å¸ƒå¼æ¨¡å¼
config = CrawloConfig.distributed(redis_host='192.168.1.100')

# é¢„è®¾é…ç½®
config = CrawloConfig.presets().production()

# é“¾å¼è°ƒç”¨
config = (CrawloConfig.standalone()
    .set_concurrency(20)
    .set_delay(1.5)
    .enable_debug()
    .enable_mysql())

# ç¯å¢ƒå˜é‡é…ç½®
config = CrawloConfig.from_env()
```

### ğŸ¯ é¢„è®¾é…ç½®

| é…ç½® | é€‚ç”¨åœºæ™¯ | ç‰¹ç‚¹ |
|------|----------|------|
| `development()` | å¼€å‘è°ƒè¯• | ä½å¹¶å‘ã€è¯¦ç»†æ—¥å¿—ã€è°ƒè¯•å‹å¥½ |
| `production()` | ç”Ÿäº§ç¯å¢ƒ | é«˜æ€§èƒ½ã€è‡ªåŠ¨æ¨¡å¼ã€ç¨³å®šå¯é  |
| `large_scale()` | å¤§è§„æ¨¡çˆ¬å– | åˆ†å¸ƒå¼ã€å†…å­˜ä¼˜åŒ–ã€æ‰¹å¤„ç† |
| `gentle()` | æ¸©å’Œæ¨¡å¼ | ä½è´Ÿè½½ã€å¯¹ç›®æ ‡æœåŠ¡å™¨å‹å¥½ |

---

## ğŸŒ åˆ†å¸ƒå¼æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  èŠ‚ç‚¹ A     â”‚    â”‚  èŠ‚ç‚¹ B     â”‚    â”‚  èŠ‚ç‚¹ N     â”‚
â”‚ (çˆ¬è™«å®ä¾‹)   â”‚    â”‚ (çˆ¬è™«å®ä¾‹)   â”‚    â”‚ (çˆ¬è™«å®ä¾‹)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚     Redis é›†ç¾¤        â”‚
              â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
              â”‚ â”‚ ä»»åŠ¡é˜Ÿåˆ— (Queue)    â”‚â”‚
              â”‚ â”‚ å»é‡é›†åˆ (Filter)   â”‚â”‚
              â”‚ â”‚ ç»Ÿè®¡ç›‘æ§ (Stats)    â”‚â”‚
              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    å…±äº«æ•°æ®å­˜å‚¨       â”‚
              â”‚   MySQL / MongoDB    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åˆ†å¸ƒå¼ç‰¹æ€§

- **ğŸ”„ è‡ªåŠ¨è´Ÿè½½å‡è¡¡**ï¼šèŠ‚ç‚¹é—´è‡ªåŠ¨åˆ†é…ä»»åŠ¡
- **ğŸ›¡ï¸ åˆ†å¸ƒå¼å»é‡**ï¼šé¿å…é‡å¤çˆ¬å–
- **ğŸ“ˆ æ°´å¹³æ‰©å±•**ï¼šåŠ¨æ€å¢å‡èŠ‚ç‚¹
- **ğŸ”§ æ•…éšœæ¢å¤**ï¼šèŠ‚ç‚¹æ•…éšœä¸å½±å“æ•´ä½“è¿è¡Œ

---

## ğŸ› ï¸ å‘½ä»¤è¡Œå·¥å…·

| å‘½ä»¤ | åŠŸèƒ½ | ç¤ºä¾‹ |
|------|------|------|
| `startproject` | åˆ›å»ºæ–°é¡¹ç›® | `crawlo startproject myproject` |
| `genspider` | ç”Ÿæˆçˆ¬è™« | `crawlo genspider news news.com` |
| `list` | åˆ—å‡ºæ‰€æœ‰çˆ¬è™« | `crawlo list` |
| `check` | æ£€æŸ¥çˆ¬è™«åˆè§„æ€§ | `crawlo check` |
| `run` | è¿è¡Œçˆ¬è™« | `crawlo run news --distributed` |
| `stats` | æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯ | `crawlo stats news` |

---

## ğŸ“– å®Œæ•´ç¤ºä¾‹

æˆ‘ä»¬æä¾›äº†åŸºäºçœŸå®é¡¹ç›®çš„å®Œæ•´ç¤ºä¾‹ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ï¼š

### ğŸ  å•æœºç‰ˆç¤ºä¾‹

```bash
# è¿›å…¥å•æœºç‰ˆç¤ºä¾‹
cd examples/telecom_licenses_standalone

# é›¶é…ç½®è¿è¡Œï¼ˆä½¿ç”¨é»˜è®¤ httpx ä¸‹è½½å™¨ï¼‰
python run.py telecom_device

# å¼€å‘ç¯å¢ƒé…ç½®
python run.py telecom_device --env development --concurrency 4

# è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰
python run.py telecom_device --debug

# è‡ªå®šä¹‰ä¸‹è½½å™¨ï¼ˆåœ¨é¡¹ç›® settings.py ä¸­é…ç½®ï¼‰
# DOWNLOADER_TYPE = 'aiohttp'    # é«˜æ€§èƒ½ä¸‹è½½å™¨
# DOWNLOADER_TYPE = 'curl_cffi'  # æµè§ˆå™¨æŒ‡çº¹æ¨¡æ‹Ÿ
```

**ç‰¹ç‚¹**ï¼š
- âœ… é›¶é…ç½®å¯åŠ¨ï¼Œå¼€ç®±å³ç”¨
- âœ… å†…å­˜é˜Ÿåˆ—ï¼Œé€Ÿåº¦å¿«
- âœ… é€‚åˆå¼€å‘è°ƒè¯•å’Œä¸­å°è§„æ¨¡çˆ¬å–

### ğŸŒ åˆ†å¸ƒå¼ç¤ºä¾‹

```bash
# è¿›å…¥åˆ†å¸ƒå¼ç¤ºä¾‹
cd examples/telecom_licenses_distributed

# å¯åŠ¨ Redis
redis-server

# å¯åŠ¨åˆ†å¸ƒå¼çˆ¬è™«ï¼ˆä½¿ç”¨é»˜è®¤ aiohttp ä¸‹è½½å™¨ï¼‰
python run.py telecom_device --distributed

# é«˜å¹¶å‘åˆ†å¸ƒå¼æ¨¡å¼
python run.py telecom_device --distributed --concurrency 30

# å¤šèŠ‚ç‚¹éƒ¨ç½²
# æœºå™¨A (RedisæœåŠ¡å™¨)
python run.py telecom_device

# æœºå™¨B
python run.py telecom_device --redis-host 192.168.1.100 --concurrency 16

# æœºå™¨C
python run.py telecom_device --redis-host 192.168.1.100 --concurrency 24

# ç¯å¢ƒå˜é‡é…ç½®
NODE_ID=node-1 REDIS_HOST=192.168.1.100 python run.py telecom_device --distributed
```

**ç‰¹ç‚¹**ï¼š
- âœ… å¤šèŠ‚ç‚¹ååŒï¼Œé«˜å¹¶å‘
- âœ… Redis é˜Ÿåˆ—å’Œå»é‡
- âœ… é€‚åˆå¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ

### ğŸ“š è¯¦ç»†æ•™ç¨‹

- **[å•æœºç‰ˆç¤ºä¾‹è¯´æ˜](examples/telecom_licenses_standalone/)**ï¼šä»åˆ›å»ºåˆ°è¿è¡Œçš„å®Œæ•´æµç¨‹
- **[åˆ†å¸ƒå¼ç‰ˆç¤ºä¾‹è¯´æ˜](examples/telecom_licenses_distributed/)**ï¼šåˆ†å¸ƒå¼æ¶æ„å’Œéƒ¨ç½²æ–¹æ¡ˆ
- **[examples/README.md](examples/README.md)**ï¼šå®Œæ•´ç¤ºä¾‹è¯´æ˜

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯å¯¹æ¯”

| ç‰¹æ€§ | å•æœºç‰ˆ | åˆ†å¸ƒå¼ç‰ˆ |
|-----|--------|----------|
| **é…ç½®å¤æ‚åº¦** | é›¶é…ç½® | éœ€è¦ Redis |
| **å¤–éƒ¨ä¾èµ–** | æ—  | Redis + æ•°æ®åº“ |
| **å¹¶å‘èƒ½åŠ›** | ä¸­ç­‰ | é«˜ |
| **æ‰©å±•æ€§** | æœ‰é™ | æ°´å¹³æ‰©å±• |
| **é€‚ç”¨åœºæ™¯** | å¼€å‘æµ‹è¯•ã€ä¸­å°è§„æ¨¡ | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡ |
| **å­¦ä¹ éš¾åº¦** | ç®€å• | ä¸­ç­‰ |

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å¤šä¸‹è½½å™¨æ”¯æŒ

```python
# æ–¹å¼1: ä½¿ç”¨ç®€åŒ–åç§°ï¼ˆæ¨èï¼‰
DOWNLOADER_TYPE = 'aiohttp'    # é«˜æ€§èƒ½é»˜è®¤é€‰æ‹©
DOWNLOADER_TYPE = 'httpx'      # HTTP/2 æ”¯æŒ
DOWNLOADER_TYPE = 'curl_cffi'  # æµè§ˆå™¨æŒ‡çº¹æ¨¡æ‹Ÿ

# æ–¹å¼2: å®Œæ•´ç±»è·¯å¾„ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
DOWNLOADER = "crawlo.downloader.aiohttp_downloader.AioHttpDownloader"
DOWNLOADER = "crawlo.downloader.httpx_downloader.HttpXDownloader"
DOWNLOADER = "crawlo.downloader.cffi_downloader.CurlCffiDownloader"

# æ–¹å¼3: åœ¨ Spider ä¸­åŠ¨æ€é€‰æ‹©
class MySpider(Spider):
    custom_settings = {
        'DOWNLOADER_TYPE': 'curl_cffi',  # éœ€è¦æµè§ˆå™¨æŒ‡çº¹æ—¶
        'CURL_BROWSER_TYPE': 'chrome136'
    }

# ä¸‹è½½å™¨ç‰¹å®šé…ç½®
CURL_BROWSER_TYPE = "chrome136"         # curl-cffi æ¨¡æ‹Ÿæµè§ˆå™¨
HTTPX_HTTP2 = True                      # httpx å¯ç”¨ HTTP/2
CONNECTION_POOL_LIMIT_PER_HOST = 20     # è¿æ¥æ± ä¼˜åŒ–
```

### æ™ºèƒ½ä¸­é—´ä»¶

```python
MIDDLEWARES = [
    'crawlo.middleware.request_ignore.RequestIgnoreMiddleware',   # è¯·æ±‚è¿‡æ»¤
    'crawlo.middleware.download_delay.DownloadDelayMiddleware',   # å»¶è¿Ÿæ§åˆ¶
    'crawlo.middleware.default_header.DefaultHeaderMiddleware',   # é»˜è®¤è¯·æ±‚å¤´
    'crawlo.middleware.proxy.ProxyMiddleware',                    # ä»£ç†æ”¯æŒ
    'crawlo.middleware.retry.RetryMiddleware',                    # é‡è¯•æœºåˆ¶
    'crawlo.middleware.response_code.ResponseCodeMiddleware',     # çŠ¶æ€ç å¤„ç†
]
```

### æ•°æ®ç®¡é“

```python
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',          # æ§åˆ¶å°è¾“å‡º
    'crawlo.pipelines.json_pipeline.JsonPipeline',               # JSON æ–‡ä»¶ï¼ˆé€è¡Œï¼‰
    'crawlo.pipelines.json_pipeline.JsonLinesPipeline',          # JSON Lines æ ¼å¼
    'crawlo.pipelines.json_pipeline.JsonArrayPipeline',          # JSON æ•°ç»„æ ¼å¼
    'crawlo.pipelines.csv_pipeline.CsvPipeline',                 # CSV æ–‡ä»¶
    'crawlo.pipelines.csv_pipeline.CsvDictPipeline',             # CSV å­—å…¸æ ¼å¼
    'crawlo.pipelines.csv_pipeline.CsvBatchPipeline',            # CSV æ‰¹é‡å†™å…¥
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',      # MySQL æ•°æ®åº“ï¼ˆæ¨èï¼‰
    'crawlo.pipelines.mysql_pipeline.AiomysqlMySQLPipeline',     # MySQL æ•°æ®åº“ï¼ˆå¤‡é€‰ï¼‰
    'crawlo.pipelines.mongo_pipeline.MongoPipeline',             # MongoDB
    'crawlo.pipelines.mongo_pipeline.MongoPoolPipeline',         # MongoDB è¿æ¥æ± ç‰ˆæœ¬
]
```

### æ™ºèƒ½å»é‡é…ç½®

Crawlo æ¡†æ¶æ ¹æ®è¿è¡Œæ¨¡å¼è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å»é‡ç®¡é“ï¼š

- **å•æœºæ¨¡å¼**ï¼šé»˜è®¤ä½¿ç”¨å†…å­˜å»é‡ç®¡é“ ([MemoryDedupPipeline](file://d:/dowell/projects/Crawlo/crawlo/pipelines/memory_dedup_pipeline.py#L25-L115))
- **åˆ†å¸ƒå¼æ¨¡å¼**ï¼šé»˜è®¤ä½¿ç”¨ Redis å»é‡ç®¡é“ ([RedisDedupPipeline](file://d:/dowell/projects/Crawlo/crawlo/pipelines/redis_dedup_pipeline.py#L33-L162))

ç”¨æˆ·ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šå…¶ä»–å»é‡ç®¡é“ï¼š

```python
# settings.py
ITEM_PIPELINES = {
    'crawlo.pipelines.BloomDedupPipeline': 100,  # ä½¿ç”¨Bloom Filterå»é‡
    'crawlo.pipelines.ConsolePipeline': 300,
}
```

æ›´å¤šå»é‡é…ç½®ä¿¡æ¯è¯·å‚é˜…ï¼š
- [å»é‡ç®¡é“æŒ‡å—](docs/deduplication_pipelines_guide.md)
- [å»é‡é…ç½®è¯´æ˜](docs/deduplication_configuration_zh.md)

---

## ğŸ“Š ç›‘æ§ä¸è¿ç»´

### å®æ—¶ç›‘æ§

```bash
# æŸ¥çœ‹è¿è¡Œç»Ÿè®¡
crawlo stats

# æŸ¥çœ‹ç‰¹å®šçˆ¬è™«
crawlo stats my_spider

# Redis é˜Ÿåˆ—ç›‘æ§
redis-cli llen crawlo:requests
redis-cli scard crawlo:fingerprint
```

### æ—¥å¿—ç³»ç»Ÿ

```python
# æ—¥å¿—é…ç½®
LOG_LEVEL = 'INFO'
LOG_FILE = 'logs/crawlo.log'
LOG_FORMAT = '%(asctime)s - [%(name)s] - %(levelname)s: %(message)s'
```

### æ€§èƒ½è°ƒä¼˜

```python
# å¹¶å‘æ§åˆ¶
CONCURRENCY = 16                    # å¹¶å‘è¯·æ±‚æ•°
DOWNLOAD_DELAY = 1.0               # ä¸‹è½½å»¶è¿Ÿ
CONNECTION_POOL_LIMIT = 100        # å…¨å±€è¿æ¥æ± å¤§å°
CONNECTION_POOL_LIMIT_PER_HOST = 30 # æ¯ä¸ªä¸»æœºè¿æ¥æ•°

# é‡è¯•ç­–ç•¥
MAX_RETRY_TIMES = 3                # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_HTTP_CODES = [500, 502, 503] # é‡è¯•çŠ¶æ€ç 

# ç»Ÿè®¡å’Œç›‘æ§ï¼ˆæ–°å¢ï¼‰
DOWNLOADER_STATS = True            # å¯ç”¨ä¸‹è½½å™¨ç»Ÿè®¡
DOWNLOAD_STATS = True              # è®°å½•ä¸‹è½½æ—¶é—´å’Œå¤§å°
DOWNLOADER_HEALTH_CHECK = True     # ä¸‹è½½å™¨å¥åº·æ£€æŸ¥
REQUEST_STATS_ENABLED = True       # è¯·æ±‚ç»Ÿè®¡
```

---

## ğŸš€ æœ€ä½³å®è·µ

### 1. å¼€å‘é˜¶æ®µ
```bash
# ä½¿ç”¨å¼€å‘é…ç½®ï¼Œä½å¹¶å‘ï¼Œè¯¦ç»†æ—¥å¿—
python run.py my_spider --env development --debug
```

### 2. æµ‹è¯•é˜¶æ®µ
```bash
# å¹²è¿è¡Œæ¨¡å¼ï¼ŒéªŒè¯é€»è¾‘
python run.py my_spider --dry-run
```

### 3. ç”Ÿäº§ç¯å¢ƒ
```bash
# ä½¿ç”¨ç”Ÿäº§é…ç½®æˆ–åˆ†å¸ƒå¼æ¨¡å¼
python run.py my_spider --env production
python run.py my_spider --distributed --concurrency 50
```

### 4. å¤§è§„æ¨¡çˆ¬å–
```bash
# ä½¿ç”¨å¤§è§„æ¨¡é…ç½®ï¼Œå¯ç”¨åˆ†å¸ƒå¼
python run.py my_spider --env large-scale
```

### 5. ä¸‹è½½å™¨é€‰æ‹©æœ€ä½³å®è·µ
```python
# å¼€å‘/æµ‹è¯•ç¯å¢ƒ - ä½¿ç”¨ httpxï¼ˆç¨³å®šã€å…¼å®¹æ€§å¥½ï¼‰
DOWNLOADER_TYPE = 'httpx'

# ç”Ÿäº§ç¯å¢ƒ - ä½¿ç”¨ aiohttpï¼ˆé«˜æ€§èƒ½ï¼‰
DOWNLOADER_TYPE = 'aiohttp'

# åçˆ¬è™«åœºæ™¯ - ä½¿ç”¨ curl_cffiï¼ˆæµè§ˆå™¨æŒ‡çº¹ï¼‰
DOWNLOADER_TYPE = 'curl_cffi'
CURL_BROWSER_TYPE = 'chrome136'
```

---

## ğŸ’¡ æ ¸å¿ƒä¼˜åŠ¿

### ğŸ¯ å¼€ç®±å³ç”¨
- **é›¶é…ç½®å¯åŠ¨**ï¼šé»˜è®¤å•æœºæ¨¡å¼ï¼Œæ— éœ€å¤æ‚é…ç½®
- **æ™ºèƒ½æ£€æµ‹**ï¼šè‡ªåŠ¨å‘ç°çˆ¬è™«ï¼Œæ™ºèƒ½é€‰æ‹©è¿è¡Œæ¨¡å¼
- **é¢„è®¾é…ç½®**ï¼šå†…ç½®å¤šç§åœºæ™¯çš„æœ€ä½³å®è·µé…ç½®

### ğŸ”§ çµæ´»é…ç½®
- **é…ç½®å·¥å‚**ï¼šé“¾å¼è°ƒç”¨ï¼Œä»£ç å³é…ç½®
- **å¤šä¸‹è½½å™¨æ”¯æŒ**ï¼šç®€åŒ–é…ç½®ï¼Œæ”¯æŒ aiohttpã€httpxã€curl_cffi
- **ç¯å¢ƒå˜é‡**ï¼šæ”¯æŒå®¹å™¨åŒ–éƒ¨ç½²
- **å¤šç§æ¨¡å¼**ï¼šå•æœºã€åˆ†å¸ƒå¼ã€è‡ªåŠ¨æ¨¡å¼

### âš¡ é«˜æ€§èƒ½
- **å¼‚æ­¥æ¶æ„**ï¼šåŸºäº asyncio çš„é«˜å¹¶å‘è®¾è®¡
- **å¤šä¸‹è½½å™¨**ï¼šaiohttpã€httpxã€curl_cffi çµæ´»é€‰æ‹©
- **æ™ºèƒ½å»é‡**ï¼šå†…å­˜/Redis åˆ†å¸ƒå¼å»é‡
- **è´Ÿè½½å‡è¡¡**ï¼šå¤šèŠ‚ç‚¹è‡ªåŠ¨ä»»åŠ¡åˆ†é…
- **æ€§èƒ½ç›‘æ§**ï¼šå®æ—¶ç»Ÿè®¡å’Œå¥åº·æ£€æŸ¥

### ğŸ›¡ï¸ ç”Ÿäº§å°±ç»ª
- **å®¹é”™æœºåˆ¶**ï¼šèŠ‚ç‚¹æ•…éšœè‡ªåŠ¨æ¢å¤
- **ç›‘æ§ç³»ç»Ÿ**ï¼šå®Œå–„çš„ç»Ÿè®¡å’Œç›‘æ§
- **æ‰©å±•èƒ½åŠ›**ï¼šæ°´å¹³æ‰©å±•ï¼ŒæŒ‰éœ€å¢å‡èŠ‚ç‚¹

---

## ğŸ†š ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | Crawlo | Scrapy | å…¶ä»–æ¡†æ¶ |
|------|--------|--------|---------|
| **å­¦ä¹ æ›²çº¿** | ç®€å• | ä¸­ç­‰ | å¤æ‚ |
| **é…ç½®æ–¹å¼** | æ™ºèƒ½é…ç½®å·¥å‚ | ä¼ ç»Ÿé…ç½® | æ‰‹åŠ¨é…ç½® |
| **åˆ†å¸ƒå¼** | ä¸€é”®åˆ‡æ¢ | éœ€è¦ Scrapyd | å¤æ‚ |
| **é»˜è®¤æ¨¡å¼** | å•æœºé›¶é…ç½® | å•æœº | å„å¼‚ |
| **è¿è¡Œæ–¹å¼** | å¤šç§çµæ´»é€‰é¡¹ | å‘½ä»¤è¡Œ | å„å¼‚ |
| **ç°ä»£åŒ–** | ç°ä»£ Python | ä¼ ç»Ÿ | å„å¼‚ |

---

## ğŸ“ æ”¯æŒä¸è´¡çŒ®

### ğŸ› é—®é¢˜åé¦ˆ
- **GitHub Issues**ï¼š[æäº¤é—®é¢˜](https://github.com/yourname/crawlo/issues)
- **æ–‡æ¡£**ï¼šæŸ¥çœ‹ [examples/README.md](examples/README.md) è·å–æ›´å¤šç¤ºä¾‹

### ğŸ¤ å‚ä¸è´¡çŒ®
- **Fork é¡¹ç›®**ï¼šæ¬¢è¿æäº¤ Pull Request
- **æ”¹è¿›æ–‡æ¡£**ï¼šå¸®åŠ©å®Œå–„æ–‡æ¡£å’Œç¤ºä¾‹
- **åˆ†äº«ç»éªŒ**ï¼šåˆ†äº«ä½¿ç”¨ç»éªŒå’Œæœ€ä½³å®è·µ

### ğŸ“‹ å¼€å‘è·¯çº¿å›¾
- [ ] å›¾å½¢åŒ–ç®¡ç†ç•Œé¢
- [ ] æ›´å¤šæ•°æ®å­˜å‚¨æ”¯æŒ
- [ ] äº‘åŸç”Ÿéƒ¨ç½²æ–¹æ¡ˆ
- [ ] æ™ºèƒ½åçˆ¬è™«å¯¹æŠ—
- [ ] å¯è§†åŒ–ç›‘æ§é¢æ¿

---

## ğŸ“„ è®¸å¯è¯

MIT License - è‡ªç”±ä½¿ç”¨ï¼Œå•†ä¸šå‹å¥½

---

**ğŸ‰ ç«‹å³å¼€å§‹æ‚¨çš„çˆ¬è™«ä¹‹æ—…ï¼**

```bash
git clone https://github.com/yourname/crawlo.git
cd crawlo
pip install -e .
crawlo startproject my_first_spider
```

#!/usr/bin/env python3
"""
nseekfs_first_look.py - A curious AI engineer's first exploration of NSeekFS

I heard about this new vector search library called NSeekFS that claims to be 
built in Rust for performance. As someone working with embeddings daily, 
I'm always skeptical of new libraries until I test them myself.

Let me put it through its paces and see how it compares to what I'm used to.
"""

import time
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Any

def setup_test_data():
    """Generate some realistic test data similar to what I use at work"""
    print("Setting up test data...")
    
    # Simulate document embeddings (like I'd get from OpenAI/Cohere)
    np.random.seed(42)  # For reproducible results
    
    # Mix of different "document types" with different characteristics
    n_docs = 10000
    dims = 384  # Common embedding dimension
    
    # Create some structure in the data (like real embeddings would have)
    embeddings = []
    
    # Technology documents (cluster 1)
    tech_center = np.random.randn(dims) * 0.5
    tech_docs = np.random.randn(3000, dims) * 0.3 + tech_center
    
    # Business documents (cluster 2) 
    biz_center = np.random.randn(dims) * 0.5
    biz_docs = np.random.randn(3000, dims) * 0.3 + biz_center
    
    # Random/mixed documents
    mixed_docs = np.random.randn(4000, dims) * 0.8
    
    embeddings = np.vstack([tech_docs, biz_docs, mixed_docs]).astype(np.float32)
    
    # Normalize (like real embeddings)
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    
    print(f"Created {len(embeddings)} embeddings with {dims} dimensions")
    print(f"Data shape: {embeddings.shape}")
    
    return embeddings

def test_basic_functionality():
    """Basic smoke test - does it work at all?"""
    print("\n" + "="*60)
    print("BASIC FUNCTIONALITY TEST")
    print("="*60)
    
    try:
        import nseekfs
        print("‚úì Successfully imported nseekfs")
    except ImportError as e:
        print(f"‚úó Failed to import: {e}")
        return False
    
    # Try the most basic case
    simple_vectors = np.array([
        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    ], dtype=np.float32)
    
    query = np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)
    
    try:
        index = nseekfs.from_embeddings(simple_vectors, normalized=True)
        results = index.query(query, top_k=2)
        
        print(f"‚úì Basic query returned {len(results)} results")
        print(f"  Top result: idx={results[0]['idx']}, score={results[0]['score']:.6f}")
        
        # Sanity check - should find perfect match
        if results[0]['idx'] == 0 and abs(results[0]['score'] - 1.0) < 1e-6:
            print("‚úì Sanity check passed - found perfect match")
            return True
        else:
            print("‚úó Sanity check failed - unexpected result")
            return False
            
    except Exception as e:
        print(f"‚úó Basic test failed: {e}")
        return False

def compare_with_numpy_baseline(embeddings: np.ndarray):
    """Compare against my usual numpy approach"""
    print("\n" + "="*60)
    print("ACCURACY COMPARISON vs NumPy Baseline")
    print("="*60)
    
    # My usual approach with numpy
    def numpy_search(vectors, query, k):
        similarities = np.dot(vectors, query)
        top_indices = np.argsort(-similarities)[:k]
        return [(idx, similarities[idx]) for idx in top_indices]
    
    # Test with a few random queries
    n_test_queries = 10
    query_indices = np.random.choice(len(embeddings), n_test_queries, replace=False)
    
    import nseekfs
    index = nseekfs.from_embeddings(embeddings, normalized=True)
    
    all_matches = []
    for i, query_idx in enumerate(query_indices):
        query = embeddings[query_idx]
        
        # Ground truth with numpy
        numpy_results = numpy_search(embeddings, query, 10)
        numpy_top10 = [r[0] for r in numpy_results]
        
        # NSeekFS results
        nseek_results = index.query(query, top_k=10)
        nseek_top10 = [r['idx'] for r in nseek_results]
        
        # Check if top-1 matches
        top1_match = numpy_top10[0] == nseek_top10[0]
        all_matches.append(top1_match)
        
        # Check recall@10
        recall = len(set(numpy_top10) & set(nseek_top10)) / 10
        
        print(f"Query {i+1}: Top-1 match = {top1_match}, Recall@10 = {recall:.3f}")
    
    accuracy = sum(all_matches) / len(all_matches)
    print(f"\nOverall Top-1 Accuracy: {accuracy:.1%}")
    
    if accuracy >= 0.95:
        print("‚úì Accuracy looks good!")
        return True
    else:
        print("‚úó Accuracy concerns - might have bugs")
        return False

def benchmark_performance(embeddings: np.ndarray):
    """See how fast this thing actually is"""
    print("\n" + "="*60)
    print("PERFORMANCE BENCHMARK")
    print("="*60)
    
    import nseekfs
    
    # Build index
    print("Building index...")
    build_start = time.time()
    index = nseekfs.from_embeddings(embeddings, normalized=True)
    build_time = time.time() - build_start
    print(f"Index build time: {build_time:.2f}s ({len(embeddings)/build_time:.0f} vectors/sec)")
    
    # Single query performance
    query = embeddings[0]  # Use first vector as query
    
    # Warmup
    index.query(query, top_k=10)
    
    # Time multiple queries
    n_queries = 100
    query_times = []
    
    for i in range(n_queries):
        query_idx = np.random.randint(0, len(embeddings))
        test_query = embeddings[query_idx]
        
        start = time.perf_counter()
        results = index.query(test_query, top_k=10)
        end = time.perf_counter()
        
        query_times.append((end - start) * 1000)  # Convert to ms
    
    avg_time = np.mean(query_times)
    p95_time = np.percentile(query_times, 95)
    qps = 1000 / avg_time  # queries per second
    
    print(f"Single query performance (n={n_queries}):")
    print(f"  Average: {avg_time:.2f}ms")
    print(f"  95th percentile: {p95_time:.2f}ms") 
    print(f"  Throughput: {qps:.0f} queries/second")
    
    # Compare with naive numpy approach
    print("\nComparing with naive numpy approach...")
    numpy_times = []
    
    for i in range(min(20, n_queries)):  # Fewer iterations since numpy is slower
        query_idx = np.random.randint(0, len(embeddings))
        test_query = embeddings[query_idx]
        
        start = time.perf_counter()
        similarities = np.dot(embeddings, test_query)
        top_indices = np.argsort(-similarities)[:10]
        end = time.perf_counter()
        
        numpy_times.append((end - start) * 1000)
    
    numpy_avg = np.mean(numpy_times)
    speedup = numpy_avg / avg_time
    
    print(f"NumPy baseline: {numpy_avg:.2f}ms")
    print(f"Speedup: {speedup:.1f}x faster than numpy")
    
    return {
        'build_time': build_time,
        'avg_query_time_ms': avg_time,
        'p95_query_time_ms': p95_time,
        'qps': qps,
        'speedup_vs_numpy': speedup
    }

def test_batch_queries(embeddings: np.ndarray):
    """Test batch query functionality"""
    print("\n" + "="*60)
    print("BATCH QUERY TEST")
    print("="*60)
    
    import nseekfs
    index = nseekfs.from_embeddings(embeddings, normalized=True)
    
    # Test different batch sizes
    batch_sizes = [1, 5, 10, 25, 50]
    
    for batch_size in batch_sizes:
        # Generate random queries
        query_indices = np.random.choice(len(embeddings), batch_size, replace=False)
        batch_queries = embeddings[query_indices]
        
        start = time.perf_counter()
        batch_results = index.query_batch(batch_queries, top_k=10)
        end = time.perf_counter()
        
        batch_time = (end - start) * 1000
        per_query_time = batch_time / batch_size
        
        print(f"Batch size {batch_size:2d}: {batch_time:6.2f}ms total, {per_query_time:5.2f}ms/query")

def stress_test(embeddings: np.ndarray):
    """Push it hard and see if it breaks"""
    print("\n" + "="*60)
    print("STRESS TEST")
    print("="*60)
    
    import nseekfs
    index = nseekfs.from_embeddings(embeddings, normalized=True)
    
    # Test edge cases
    test_cases = [
        ("Normal case", embeddings[0], 10),
        ("k=1", embeddings[0], 1),
        ("Large k", embeddings[0], 100),
        ("k > dataset size", embeddings[0], len(embeddings) + 100),
        ("Zero vector", np.zeros(embeddings.shape[1], dtype=np.float32), 10),
    ]
    
    for test_name, query, k in test_cases:
        try:
            results = index.query(query, top_k=k)
            print(f"‚úì {test_name}: returned {len(results)} results")
        except Exception as e:
            print(f"‚úó {test_name}: failed with {e}")

def main():
    print("NSeekFS First Look - An AI Engineer's Perspective")
    print("=" * 60)
    print("I'm testing this new Rust-based vector search library.")
    print("Let me see if it's worth switching from my current tools...\n")
    
    # Setup
    embeddings = setup_test_data()
    
    # Run tests
    basic_ok = test_basic_functionality()
    if not basic_ok:
        print("Basic functionality failed. Stopping here.")
        return
    
    accuracy_ok = compare_with_numpy_baseline(embeddings)
    perf_stats = benchmark_performance(embeddings)
    test_batch_queries(embeddings)
    stress_test(embeddings)
    
    # Final verdict
    print("\n" + "="*60)
    print("FINAL VERDICT")
    print("="*60)
    
    if accuracy_ok:
        if perf_stats['speedup_vs_numpy'] > 3:
            verdict = "IMPRESSIVE"
            icon = "üéâ"
        elif perf_stats['speedup_vs_numpy'] > 1.5:
            verdict = "SOLID"
            icon = "üëç"
        else:
            verdict = "ADEQUATE"
            icon = "ü§î"
        
        print(f"{icon} {verdict}! This library shows promise:")
        print(f"   ‚úì Perfect accuracy matches my numpy baseline")
        print(f"   ‚úì {perf_stats['speedup_vs_numpy']:.1f}x faster than naive approach")
        print(f"   ‚úì {perf_stats['qps']:.0f} queries/second throughput")
        print(f"   ‚úì Very fast build time: {perf_stats['build_time']:.2f}s for 10K vectors")
        print(f"   ‚úì Sub-millisecond latency: {perf_stats['avg_query_time_ms']:.2f}ms avg")
        
        if perf_stats['speedup_vs_numpy'] < 2:
            print(f"\n   Note: Modest speedup is normal for small datasets.")
            print(f"         Real gains typically show with larger datasets or when")
            print(f"         you need the reliability and type safety of Rust.")
        
        print(f"\nFor production consideration:")
        print(f"  + 100% exact results (crucial for precision-critical apps)")
        print(f"  + Rust reliability (no segfaults, memory safety)")
        print(f"  + Simple API (easy integration)")
        print(f"  + Good performance characteristics")
        
    else:
        print("üö® ACCURACY ISSUES DETECTED:")
        print("   ‚úó Results don't match numpy baseline")
        print("   ‚úó Cannot recommend for production use")
        print("\nNeed to investigate accuracy problems before considering adoption.")
    
    print(f"\nKey metrics for the team:")
    print(f"  - Query latency: {perf_stats['avg_query_time_ms']:.1f}ms avg, {perf_stats['p95_query_time_ms']:.1f}ms P95")
    print(f"  - Throughput: {perf_stats['qps']:.0f} QPS")
    print(f"  - Index build: {perf_stats['build_time']:.2f}s for 10K vectors")
    print(f"  - Memory efficiency: Rust-based, should be lean")
    print(f"  - Accuracy: {'100%' if accuracy_ok else 'Issues detected'}")

if __name__ == "__main__":
    main()
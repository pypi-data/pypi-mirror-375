import unittest

import pandas as pd

from template_log_parser.log_functions import (
    process_log,
    event_type_column,
    event_data_column,
)

from test.resources import built_in_log_file_types

from test.resources import logger

# Eight possible combinations between column functions, merge events, and drop columns for each type
config_combinations = {
    built_in: {
        "column functions; merge events; drop columns": [
            built_in.column_functions,
            built_in.merge_events,
            True,
        ],
        "column functions; no merge events; drop columns": [
            built_in.column_functions,
            None,
            True,
        ],
        "column functions; merge events; no drop columns": [
            built_in.column_functions,
            built_in.merge_events,
            False,
        ],
        "column functions; no merge events; no drop columns": [
            built_in.column_functions,
            None,
            False,
        ],
        "no column functions; merge events; no drop columns": [
            None,
            built_in.merge_events,
            False,
        ],
        "no column functions; merge events; drop columns": [
            None,
            built_in.merge_events,
            True,
        ],
        "no column functions; no merge events; drop columns": [
            None,
            None,
            True,
        ],
        "no column functions; no merge events; no drop columns": [
            None,
            None,
            False,
        ],
    }
    for built_in in built_in_log_file_types
}


class TestProcessLog(unittest.TestCase):
    """Defines a class to test the process_log function"""

    def test_dict_format_true(self):
        """Test function to determine that instances where dict_format=True contain the correct keys (event_types) and accounts for all log file lines"""

        logger.info("Checking process_log() where dict_format=True\n")

        for built_in, configurations in config_combinations.items():
            logger.info(f"----------{built_in.name}---------\n")

            with open(built_in.sample_log_file, "r") as raw_log:
                lines_in_log_file = len(raw_log.readlines())
                logger.info(f"Total lines in log file: {lines_in_log_file}")

            for configuration, (col_funcs, merges, drop_cols) in configurations.items():
                logger.info(f"##### Configuration: {configuration} #####\n")

                output = process_log(
                    file=built_in.sample_log_file,
                    templates=built_in.templates,
                    additional_column_functions=col_funcs,
                    merge_dictionary=merges,
                    drop_columns=drop_cols,
                    dict_format=True,
                )

                expected_keys = sorted(
                    list(set([value[1] for value in built_in.templates]))
                )

                logger.debug(f"Base keys in template dictionary ({len(expected_keys)}): {expected_keys}")


                # KEYS
                if merges:  # Merge events
                    new_merge_events = [key for key in merges]
                    logger.debug(f"New keys generated by merge_events ({len(new_merge_events)}): {new_merge_events}")

                    keys_to_delete = []

                    # The values of the merge_event dict is a list of event_types that were merged (and deleted)
                    for deleted_column_list in merges.values():
                        keys_to_delete.extend(deleted_column_list)

                    logger.debug(f"Keys to be dropped ({len(keys_to_delete)}): {keys_to_delete}")

                    expected_keys = [
                        column
                        for column in expected_keys
                        if column not in keys_to_delete
                    ]
                    expected_keys.extend(new_merge_events)

                expected_keys = sorted(list(set(expected_keys)))
                logger.debug(f"Expected keys {len(expected_keys)}: {expected_keys}")


                actual_keys = sorted(list(output.keys()))

                logger.debug(f"Actual keys {len(actual_keys)}: {actual_keys}")

                self.assertEqual(expected_keys, actual_keys, f"{built_in.name}: {expected_keys} does not match {actual_keys}")
                logger.info("---MATCHING KEYS OK---\n")

                # COLUMNS PER MINI DF
                # Running parse on individual templates against template dictionary in a comprehension
                # to return {'event_type': [column1, column2,...], ...}
                # Every event_type now has a list of columns associated with it, adding 'event_type' at the end of list
                expected_columns_by_template = {
                    value[1]: list(value[0].parse(value[0].format).named)
                    + [event_type_column]
                    for value in built_in.templates
                }

                expected_columns_by_event_type = {}

                logger.info("Generating expected columns by event type\n")
                for event_type in expected_keys:

                    logger.info(f"Event type: {event_type}")
                    expected_columns = []
                    columns_to_add = []
                    columns_to_drop = []
                    standard_drop_columns = [event_data_column]

                    if merges is None:
                        logger.info("No merge Events")
                        expected_columns = expected_columns_by_template[event_type]

                    elif merges:
                        if event_type in merges.keys():
                            merged_events = sorted(list(set(merges[event_type])))

                            logger.debug(
                                f"Events to be merged ({len(merged_events)}): {merged_events}"
                            )

                            for event in merged_events:
                                individual_event_columns = sorted(
                                    expected_columns_by_template[event]
                                )
                                logger.debug(
                                    f'Columns associated with "{event}": {individual_event_columns}'
                                )
                                expected_columns.extend(individual_event_columns)
                        else:
                            expected_columns = expected_columns_by_template[event_type]
                            logger.debug(f"{event_type} not in merge events")


                    logger.debug(
                        f"Initial expected columns ({len(expected_columns)}): {expected_columns}"
                    )

                    if col_funcs:
                        logger.info("Checking column functions\n")
                        drops = sorted(list(col_funcs.keys()))
                        columns_to_drop.extend(drops)

                        logger.debug(f"Adding ({len(drops)}) {drops} to columns_to_drop")

                        for original_column, function_attributes in col_funcs.items():
                            if original_column in expected_columns:
                                to_add = function_attributes[1]
                                if type(to_add) is str:
                                    columns_to_add.append(to_add)
                                    logger.debug(f'Adding "{to_add}" to columns_to_add')
                                elif type(to_add) is list:
                                    columns_to_add.extend(to_add)
                                    logger.debug(
                                        f"Adding ({len(to_add)}) {to_add} to columns_to_add"
                                    )

                    elif col_funcs is None:
                        logger.info(f"No column functions for {event_type}")

                    expected_columns.extend(columns_to_add)
                    logger.debug(f"Adding columns: {columns_to_add}")

                    if drop_cols is True:
                        expected_columns = [
                            column
                            for column in expected_columns
                            if column not in columns_to_drop
                        ]
                        logger.debug(f"Dropping columns: {columns_to_drop}")
                        expected_columns = [
                            column
                            for column in expected_columns
                            if column not in standard_drop_columns
                        ]
                        logger.debug(f"Dropping columns: {standard_drop_columns}")

                    elif drop_cols is False:
                        expected_columns.extend(standard_drop_columns)
                        logger.debug(
                            f"Drop columns is False. Adding standard drop columns: {standard_drop_columns}"
                        )

                    expected_columns = sorted(list(set(expected_columns)))
                    expected_columns_by_event_type[event_type] = expected_columns

                    logger.debug(
                        f"Final expected columns ({len(expected_columns)}): {expected_columns}"
                    )

                total_log_lines_accounted_for = 0

                for event_type, df in output.items():

                    logger.info(f"Event type: {event_type}")
                    self.assertIsInstance(event_type, str, f"Event type ({event_type}) is not a string?")
                    self.assertIsInstance(df, pd.DataFrame, f"Value for key {event_type} is not a dataframe")
                    total_log_lines_accounted_for += df.shape[0]
                    actual_columns = sorted(df.columns.tolist())
                    logger.debug(f"Actual columns {len(actual_columns)}: {actual_columns}")

                    expected_columns = expected_columns_by_event_type[event_type]
                    logger.debug(f"Expected columns {len(expected_columns)}: {expected_columns}")
                    self.assertEqual(expected_columns, actual_columns, f"{built_in.name}: Mismatch between expected: {expected_columns} and actual {actual_columns}")
                    logger.info("---ALL EXPECTED COLUMNS ARE PRESENT---\n")

                self.assertEqual(lines_in_log_file, total_log_lines_accounted_for, f"{built_in}: Expected {lines_in_log_file} lines, found {total_log_lines_accounted_for}")

                logger.info("---ALL LINES IN LOG FILE ARE ACCOUNTED FOR---")
            logger.info(f"----------{built_in.name}: OK----------\n\n")

    def test_dict_format_false(self):
        """Test function to determine that instances where dict_format=False contain the correct columns in the main df"""

        logger.info("Checking process_log() where dict_format=False\n")

        for built_in, configurations in config_combinations.items():
            logger.info(f"----------{built_in.name}---------\n")

            with open(built_in.sample_log_file, "r") as raw_log:
                lines_in_log_file = len(raw_log.readlines())
                logger.info(f"Total Lines in log file: {lines_in_log_file}\n")

            for configuration, (col_funcs, merges, drop_cols) in configurations.items():
                logger.info(f"##### Configuration: {configuration} #####\n")
                output = process_log(
                    file=built_in.sample_log_file,
                    templates=built_in.templates,
                    additional_column_functions=col_funcs,
                    merge_dictionary=merges,
                    drop_columns=drop_cols,
                    dict_format=False,
                )

                columns_accounted_for_by_templates = [
                    list(value[0].parse(value[0].format).named)
                    for value in built_in.templates
                ]

                # Remove duplicates, unpack lists, this is a list of all possible columns
                columns_accounted_for_by_templates = list(
                    set(
                        [
                            column
                            for column_list in columns_accounted_for_by_templates
                            for column in column_list
                        ]
                        + [event_type_column]
                    )
                )

                standard_drop_columns = [event_data_column]
                columns_to_add = []
                columns_to_delete = []

                # New column or columns created by functions, add/extend accordingly
                # Second item in the function_info list will be either a string or list for new column(s)
                if col_funcs:
                    for old_column, function_info in col_funcs.items():
                        columns_to_delete.append(old_column)
                        if type(function_info[1]) is str:
                            columns_to_add.append(function_info[1])
                        if type(function_info[1]) is list:
                            columns_to_add.extend(function_info[1])

                if drop_cols is True:
                    columns_to_delete = columns_to_delete + standard_drop_columns

                elif drop_cols is False:
                    columns_to_add.extend(standard_drop_columns)
                    # If drop columns is False, re-define columns to delete as en empty list
                    columns_to_delete = []

                columns_accounted_for_by_templates.extend(columns_to_add)

                # Remove duplicates once again just to be safe and sort
                expected_columns = sorted(
                    list(
                        set(
                            [
                                column
                                for column in columns_accounted_for_by_templates
                                if column not in columns_to_delete
                            ]
                        )
                    )
                )

                actual_columns = sorted(output.columns.tolist())
                logger.debug(f"Actual columns {len(actual_columns)}: {actual_columns}")

                logger.debug(f"Expected columns {len(expected_columns)}: {expected_columns}")
                self.assertEqual(expected_columns, actual_columns, f"{built_in.name}: Mismatch between expected: {expected_columns} and actual: {actual_columns}")
                logger.info("---ALL EXPECTED COLUMNS ARE PRESENT---")

                total_log_lines_accounted_for = output.shape[0]

                logger.debug(f"{built_in.name}: Expecting {lines_in_log_file} lines, found {total_log_lines_accounted_for}")

                self.assertEqual(lines_in_log_file, total_log_lines_accounted_for, f"{built_in.name}: Expected {lines_in_log_file} lines, found {total_log_lines_accounted_for}")

                logger.info("---ALL LINES IN LOG FILE ARE ACCOUNTED FOR---")

    def test_dict_format_false_no_matches(self):
        """Check to ensure an empty df is return in the event that match/eliminate criteria produces no entries"""
        logger.info(f"---Checking dict format no matches---")
        for built_in in built_in_log_file_types:
            logger.info(f"Checking {built_in.name}")
            df_output = process_log(built_in.sample_log_file, templates=built_in.templates, match=["THIS STRING SHOULD NOT BE PRESENT"], dict_format=False)
            self.assertIsInstance(df_output, pd.DataFrame)
            self.assertEqual(0, df_output.shape[0])
            logger.info("Correctly produces empty df")
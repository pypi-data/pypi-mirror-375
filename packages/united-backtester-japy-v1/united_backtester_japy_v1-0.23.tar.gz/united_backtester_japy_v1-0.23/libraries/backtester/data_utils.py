# hamoni/test/utils/data_utils.py
import pandas as pd
import pytz
from pathlib import Path
from typing import List, Optional, Union, Tuple, Dict, Any
import os
from urllib.request import urlretrieve
import zipfile
import csv
from binance.client import Client
import itertools
from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import logging
import gc
from .config import FUTURES_DATA_DIR, SPOT_DATA_DIR, PROJECT_ROOT

import asyncio
import aiohttp
import tempfile
from io import BytesIO

import re

from tqdm import tqdm

# ìƒìˆ˜ ì •ì˜
MAX_WORKERS = 10
BATCH_SIZE = 10
DEFAULT_INTERVAL = '60m'
COLUMNS = [
    'open_time', 'open', 'high', 'low', 'close', 'volume',
    'close_time', 'quote_volume', 'count',
    'taker_buy_volume', 'taker_buy_quote_volume', 'ignore'
]
    
def _norm_interval_for_binance(interval: str) -> str:
    """ìš°ë¦¬ ë‚´ë¶€ í‘œê¸° â†’ Binance í‘œê¸°(1h, 4h, 1d ...)"""
    mapping = {
        '60m': '1h', '240m': '4h', '360m': '6h',
        '720m': '12h', '1440m': '1d',
        '1m': '1m', '5m': '5m', '15m': '15m',
        '30m': '30m', '1h': '1h', '4h': '4h', '6h': '6h',
        '12h': '12h', '1d': '1d', '1w': '1w'
    }
    return mapping.get(interval, interval)

def get_data(
    symbol: str,
    interval: str = DEFAULT_INTERVAL,
    start: Optional[str] = None,
    end: Optional[str] = None,
    data_type: str = 'futures'
) -> pd.DataFrame:
    """OHLCV ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        symbol: ìì‚° ì‹¬ë³¼ (e.g. ``"BTCUSDT"``).
        interval: ì‹œê°„ ê°„ê²© (e.g. ``"60m"``).
        start: ì‹œì‘ ë‚ ì§œ (e.g. ``"2024-01-01"``).
        end: ì¢…ë£Œ ë‚ ì§œ (e.g. ``"2025-01-01"``).
        data_type: ë°ì´í„° íƒ€ì… (e.g. ``"futures"`` ë˜ëŠ” ``"spot"``).
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ OHLCV ë°ì´í„°
    
    Raises:
        FileNotFoundError: ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        DataProcessingError: ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ
    """
    try:
        data_dir = FUTURES_DATA_DIR if data_type == 'futures' else SPOT_DATA_DIR
        tz = pytz.timezone('Asia/Seoul')
        data_path = Path(data_dir[interval]) / f'{symbol}.csv'
        
        if not data_path.exists():
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        
        df = pd.read_csv(data_path)
        df['datetime'] = pd.to_datetime(df['datetime'])
        df.set_index('datetime', inplace=True)
        df.index = df.index.tz_localize(pytz.UTC).tz_convert(tz)
        
        if start is not None:
            df = df[start:]
        if end is not None:
            df = df[:end]
        
        return df
    
    except Exception as e:
        raise DataProcessingError(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    

def generate_data_for_backtest(
    symbols: List[str],
    start_YY_MM: Union[str, datetime],
    end_YY_MM: Union[str, datetime],
    intervals: List[str],
    data_types: List[str]
) -> None:
    """ë°±í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì£¼ì˜: ë³‘ë ¬ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ Jupyter Notebookì—ì„œ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”.

    Args:
        symbols: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ ['BTCUSDT', 'ETHUSDT', ...]
        start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM)
        intervals: ì‹œê°„ ê°„ê²© ë¦¬ìŠ¤íŠ¸ ì‹œê°„ ê°„ê²© ë¦¬ìŠ¤íŠ¸ ['1m', '5m', '15m','60m','240m','360m','720m','1440m','1w']
        data_types: ë°ì´í„° íƒ€ì… ë¦¬ìŠ¤íŠ¸ ['futures', 'spot']
    """
    
    setup_logging()
    # start_YY_MM í¬ë©§ì— ì•ˆë§ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
    if not isinstance(start_YY_MM, str) or not isinstance(end_YY_MM, str):
        raise ValueError("start_YY_MM ë˜ëŠ” end_YY_MMëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    if not re.match(r'^\d{4}-\d{2}$', start_YY_MM) or not re.match(r'^\d{4}-\d{2}$', end_YY_MM):
        raise ValueError("start_YY_MM ë˜ëŠ” end_YY_MMëŠ” YYYY-MM í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    # YYYY-MM íŒŒì‹±
    if isinstance(start_YY_MM, str):
        start_date = datetime.strptime(start_YY_MM, '%Y-%m')
    else:
        start_date = datetime(start_YY_MM.year, start_YY_MM.month, 1)

    if isinstance(end_YY_MM, str):
        end_date = datetime.strptime(end_YY_MM, '%Y-%m')
    else:
        end_date = datetime(end_YY_MM.year, end_YY_MM.month, 1)

    total_tasks = len(data_types) * len(intervals) * len(symbols)

    print(f"ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print(f"   - ì‹¬ë³¼ ìˆ˜: {len(symbols)}")
    print(f"   - ì‹œê°„ ê°„ê²©: {intervals}")
    print(f"   - ë°ì´í„° íƒ€ì…: {data_types}")
    print(f"   - ì´ ì‘ì—… ìˆ˜: {total_tasks}")
    print(f"   - ê¸°ê°„: {start_date.strftime('%Y-%m')} ~ {end_date.strftime('%Y-%m')}")
    print("=" * 60)

    completed_tasks = 0

    for data_type_idx, data_type in enumerate(data_types):
        print(f"\nğŸ”„ ë°ì´í„° íƒ€ì…: {data_type} ({data_type_idx + 1}/{len(data_types)})")

        data_type_dir = Path(PROJECT_ROOT) / f'data_{data_type}'
        data_type_dir.mkdir(exist_ok=True)

        for interval_idx, interval in enumerate(intervals):
            print(f"  â° ì‹œê°„ ê°„ê²©: {interval} ({interval_idx + 1}/{len(intervals)})")

            # ë‚´ë¶€ ë””ë ‰í† ë¦¬ ì´ë¦„ì€ ê¸°ì¡´ ê·œì¹™ ìœ ì§€
            str_interval = (f"{interval}" if 'm' in interval else
                            f'{int(interval.replace("h",""))*60}m' if 'h' in interval
                            else f'{int(interval.replace("d",""))*1440}m')

            base_dir = Path(f"{data_type_dir}/data_{str_interval}")
            base_dir.mkdir(exist_ok=True)

            with tqdm(total=len(symbols),
                      desc=f"    ğŸ’° {data_type}-{interval} ì²˜ë¦¬ ì¤‘",
                      unit="ì‹¬ë³¼",
                      leave=False) as pbar:

                for i in range(0, len(symbols), BATCH_SIZE):
                    symbol_batch = symbols[i:i + BATCH_SIZE]

                    with ProcessPoolExecutor(max_workers=BATCH_SIZE) as executor:
                        args = [
                            (symbol, interval, base_dir, start_date, end_date, data_type)
                            for symbol in symbol_batch
                        ]
                        list(executor.map(process_symbol, args))

                    pbar.update(len(symbol_batch))
                    completed_tasks += len(symbol_batch)

            gc.collect()

    print("\n" + "=" * 60)
    print(f"âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (ì´ {completed_tasks}ê°œ ì‘ì—…)")
    print(f"ğŸ“ ë°ì´í„°ê°€ ì €ì¥ëœ ìœ„ì¹˜: {PROJECT_ROOT}")


def make_unofficial_interval(
    symbols: List[str],
    resample_minutes: int,
    data_type: str
) -> None:
    """ë¹„ê³µì‹ ì‹œê°„ ê°„ê²©ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        symbols: ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
        resample_minutes: ë¦¬ìƒ˜í”Œë§í•  ë¶„ ë‹¨ìœ„ 45ë¶„ ->45, 2ì‹œê°„ -> 120
        data_type: ë°ì´í„° íƒ€ì…, [data_futures,data_spot]
    """
    def resampler(df: pd.DataFrame, resample_minutes: int = 45) -> pd.DataFrame:
        """ë°ì´í„°í”„ë ˆì„ì„ ë¦¬ìƒ˜í”Œë§í•©ë‹ˆë‹¤."""
        if not isinstance(df.index, pd.DatetimeIndex):
            df.set_index('datetime', inplace=True)
        
        return df.resample(f'{resample_minutes}min').agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        })
    
    for symbol in symbols:
        input_path = Path(f'{data_type}/data_5m/{symbol}.csv')
        df_5m = pd.read_csv(input_path, index_col=0, parse_dates=True)
        
        new_df = resampler(df_5m, resample_minutes=resample_minutes)
        
        output_dir = Path(f'{data_type}/data_{resample_minutes}m')
        output_dir.mkdir(exist_ok=True)
        
        output_path = output_dir / f'{symbol}.csv'
        new_df.to_csv(output_path)
        
# ë‚´ë¶€ í•¨ìˆ˜

class DataProcessingError(Exception):
    """ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤"""
    pass

def setup_logging() -> None:
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def get_last_timestamp(file_path: Path) -> Optional[datetime]:
    """íŒŒì¼ì—ì„œ ë§ˆì§€ë§‰ ë°ì´í„°ì˜ timestampë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not file_path.exists() or file_path.stat().st_size == 0:
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # íŒŒì¼ ëì—ì„œë¶€í„° ì—­ë°©í–¥ìœ¼ë¡œ ì½ê¸°
            f.seek(0, os.SEEK_END)
            file_size = f.tell()
            
            if file_size == 0:
                return None
            
            # ë§ˆì§€ë§‰ 1KB ì •ë„ë§Œ ì½ì–´ì„œ ì²˜ë¦¬ (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¶©ë¶„)
            read_size = min(1024, file_size)
            f.seek(file_size - read_size)
            
            # ì²« ë²ˆì§¸ ë¶ˆì™„ì „í•œ ì¤„ ê±´ë„ˆë›°ê¸° (ì¤‘ê°„ì—ì„œ ì‹œì‘í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
            if file_size > read_size:
                f.readline()
            
            # ë‚˜ë¨¸ì§€ ì¤„ë“¤ ì½ê¸°
            lines = f.readlines()
            
            # ë§ˆì§€ë§‰ë¶€í„° ìœ íš¨í•œ ë°ì´í„° ì¤„ ì°¾ê¸°
            for line in reversed(lines):
                line = line.strip()
                
                if not line or line.startswith('datetime,'):
                    continue
                
                try:
                    dt_str = line.split(',')[0]
                    return pd.to_datetime(dt_str)
                except (IndexError, ValueError):
                    continue
            
            return None
            
    except Exception as e:
        logging.error(f"íƒ€ì„ìŠ¤íƒ¬í”„ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
        return None


def generate_monthly_urls(
    symbol: str,
    start_ym: datetime,   # ë°˜ë“œì‹œ month-startë©´ ë” ì¢‹ì§€ë§Œ, ê·¸ëƒ¥ YYYY-MM í˜•íƒœë©´ OK
    end_ym: datetime,
    interval: str,
    data_type: str = 'futures'
) -> List[str]:
    """
    Binance Vision monthly klines:
    - futures: https://data.binance.vision/data/futures/um/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{YYYY}-{MM}.zip
    - spot   : https://data.binance.vision/data/spot/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{YYYY}-{MM}.zip
    """
    base_url = ("https://data.binance.vision/data/spot/monthly/klines"
                if data_type == 'spot'
                else "https://data.binance.vision/data/futures/um/monthly/klines")

    iv = _norm_interval_for_binance(interval)

    # start_ym, end_ymì´ day í¬í•¨ì´ì–´ë„ month ë²”ìœ„ë¡œ ì •ê·œí™”
    cur = datetime(start_ym.year, start_ym.month, 1)
    end = datetime(end_ym.year, end_ym.month, 1)

    urls: List[str] = []
    while cur <= end:
        urls.append(f"{base_url}/{symbol}/{iv}/{symbol}-{iv}-{cur.year}-{cur.month:02d}.zip")
        # ë‹¤ìŒ ë‹¬
        if cur.month == 12:
            cur = datetime(cur.year + 1, 1, 1)
        else:
            cur = datetime(cur.year, cur.month + 1, 1)
    return urls


# ê³¼ê±° ë°ì´í„°ì¤‘ ë¹ ì§„ ë¶€ë¶„ë“¤ì´ ê°„í˜¹ìˆì–´ ì œì™¸
def generate_daily_urls(
    symbol: str,
    start_date: datetime,
    end_date: datetime,
    interval: str,
    data_type: str = 'futures'
) -> List[str]:
    """ì¼ë³„ ë°ì´í„° URLì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        symbol: ê±°ë˜ ì‹¬ë³¼
        start_date: ì‹œì‘ ë‚ ì§œ
        end_date: ì¢…ë£Œ ë‚ ì§œ
        interval: ì‹œê°„ ê°„ê²©
        data_type: ë°ì´í„° íƒ€ì…
        
    Returns:
        List[str]: URL ë¦¬ìŠ¤íŠ¸
    """
    base_url = ("https://data.binance.vision/data/spot/daily/klines" 
                if data_type == 'spot' 
                else "https://data.binance.vision/data/futures/um/daily/klines")
    
    urls = []
    if interval == '60m':
        interval = '1h'
    if interval == '240m':
        interval = '4h'
    if interval =='360m':
        interval = '6h'
    if interval =='720m':
        interval = '12h'
    if interval =='1440m':
        interval = '1d'
    current_date = start_date
    while current_date <= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        urls.append(f"{base_url}/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip")
        current_date += timedelta(days=1)
    return urls

def has_header(file_path: Path) -> bool:
    """CSV íŒŒì¼ì˜ í—¤ë” ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        with open(file_path, 'r') as f:
            sniffer = csv.Sniffer()
            sample = ''.join(list(itertools.islice(f, 10)))
            return sniffer.has_header(sample)
    except Exception:
        return False

def download_single_file(args: Tuple[str, Path]) -> bool:
    """ë‹¨ì¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì••ì¶•ì„ í•´ì œí•©ë‹ˆë‹¤.
    
    Args:
        args: (url, ì¶”ì¶œ ë””ë ‰í† ë¦¬) íŠœí”Œ
        
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    url, extract_dir = args
    try:
        filename = url.split('/')[-1]
        file_path = extract_dir / filename
        
        urlretrieve(url, str(file_path))
        
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        file_path.unlink()
        return True
    except Exception as e:
        # logging.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {url}: {e}")
        return False

async def _fetch_and_unzip(session, url: str, extract_dir: Path, retries: int = 3, chunk_size: int = 1<<15) -> bool:
    backoff = 0.5
    for attempt in range(retries):
        try:
            async with session.get(url) as resp:
                if resp.status == 404:
                    return False  # ì—†ëŠ” ë‚ ì§œ/íŒŒì¼
                resp.raise_for_status()

                # í° íŒŒì¼ ëŒ€ë¹„: ë””ìŠ¤í¬ ì„ì‹œíŒŒì¼ì— ìŠ¤íŠ¸ë¦¬ë° ì €ì¥ í›„ unzip
                with tempfile.NamedTemporaryFile(delete=False) as tmp:
                    tmp_path = Path(tmp.name)
                    async for chunk in resp.content.iter_chunked(chunk_size):
                        tmp.write(chunk)

                try:
                    with zipfile.ZipFile(tmp_path, 'r') as zf:
                        zf.extractall(extract_dir)
                finally:
                    try:
                        tmp_path.unlink(missing_ok=True)
                    except Exception:
                        pass
                return True

        except Exception:
            if attempt == retries - 1:
                return False
            await asyncio.sleep(backoff)
            backoff *= 2


async def _download_and_unzip_async(urls: List[str], extract_dir: Path, concurrency: int = 64, per_host_limit: int = 16, timeout_s: int = 30) -> None:
    if not urls:
        return
    extract_dir.mkdir(parents=True, exist_ok=True)

    timeout = aiohttp.ClientTimeout(total=None, sock_connect=timeout_s, sock_read=timeout_s)
    connector = aiohttp.TCPConnector(limit=concurrency, limit_per_host=per_host_limit, ttl_dns_cache=300)
    sem = asyncio.Semaphore(concurrency)

    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
        async def worker(u):
            async with sem:
                return await _fetch_and_unzip(session, u, extract_dir)

        # ì‹¤í–‰ (ì‹¤íŒ¨ True/False ë¬´ì‹œí•˜ê³  ë³‘í•© ë‹¨ê³„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê±°ë¦„)
        await asyncio.gather(*(worker(u) for u in urls))


def download_and_unzip(urls: List[str], extract_dir: Path, concurrency: Optional[int] = None) -> None:
    """
    ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜ ìœ ì§€. ë‚´ë¶€ì—ì„œ asyncio ì‹¤í–‰.
    concurrency ë¯¸ì§€ì • ì‹œ URL ê°œìˆ˜/íŒŒì¼ í¬ê¸° ê°ì•ˆí•´ ë™ì  ì„¤ì •.
    """
    if not urls:
        return
    # ì‘ì€ íŒŒì¼(ì¼ë´‰Â·4H ë“±)ì€ ë™ì‹œì„± ë†’ê²Œ, í° íŒŒì¼ì€ ë‚®ê²Œ
    c = concurrency or max(16, min(64, len(urls)))
    try:
        asyncio.run(_download_and_unzip_async(urls, extract_dir, concurrency=c))
    except RuntimeError:
        # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ëŒì•„ê°€ëŠ” í™˜ê²½(Jupyter/ì„œë¸Œë£¨í”„) ë³´í˜¸
        loop = asyncio.get_event_loop()
        loop.run_until_complete(_download_and_unzip_async(urls, extract_dir, concurrency=c))

def process_single_file(file_path: Path) -> Optional[pd.DataFrame]:
    """ë‹¨ì¼ CSV íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        if has_header(file_path):
            df = pd.read_csv(file_path)
        else:
            df = pd.read_csv(file_path, header=None, names=COLUMNS)
        return df
    except Exception as e:
        logging.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
        return None
    finally:
        gc.collect()

def merge_data_files(
    extract_dir: Path,
    output_filename: Path,
    existing_df: Optional[pd.DataFrame] = None  # í˜¸í™˜ì„±ë§Œ ìœ ì§€, ë‚´ë¶€ì—ì„œ ì‚¬ìš© ì•ˆ í•¨
) -> None:
    """temp ë””ë ‰í† ë¦¬ì˜ ì¼ì CSVë“¤ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë³‘í•©(ìŠ¤íŠ¸ë¦¬ë° append)."""
    def _date_key(p: Path) -> tuple:
        # íŒŒì¼ëª…: SYMBOL-INTERVAL-YYYY-MM-DD.csv â†’ ì •ë ¬ ì•ˆì •ì„± í™•ë³´
        stem = p.stem
        date_str = stem.rsplit('-', 1)[-1]  # YYYY-MM-DD
        try:
            d = datetime.strptime(date_str, '%Y-%m-%d')
        except Exception:
            d = datetime.min
        return (d.year, d.month, d.day)

    # ê¸°ì¡´ íŒŒì¼ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë¹ ë¥´ê²Œ ì½ì–´ì™€ì„œ(naive) ì¤‘ë³µ ë°©ì§€
    last_ts = get_last_timestamp(output_filename)

    # temp ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ë“¤(í•˜ë£¨ ë‹¨ìœ„) ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬
    csv_files = sorted(extract_dir.glob('*.csv'), key=_date_key)

    # ê²°ê³¼ íŒŒì¼ì— í—¤ë”ë¥¼ ì“¸ì§€ ê²°ì •
    header_needed = (not output_filename.exists()) or (output_filename.stat().st_size == 0)

    # í•œ ë²ˆë§Œ ì—´ê³  ê³„ì† append (I/O ì˜¤ë²„í—¤ë“œ ê°ì†Œ)
    with open(output_filename, 'a', newline='') as fout:
        for file in csv_files:
            df_raw = process_single_file(file)
            if df_raw is None or df_raw.empty:
                continue

            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì“°ê¸° (ì—´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰)
            # (process_single_fileê°€ ì´ë¯¸ names=COLUMNSë¡œ ë³´ì¥í•œë‹¤ë©´ ì•„ë˜ ì„ íƒì€ ì—†ì–´ë„ OK)
            try:
                df_raw = df_raw[['open_time', 'open', 'high', 'low', 'close', 'volume']]
            except KeyError:
                pass

            # ìµœì¢… í¬ë§·ìœ¼ë¡œ ì¦‰ì‹œ ë³€í™˜ (naive datetime, ì •ë ¬)
            # â”” tz-awareë¡œ ë§Œë“¤ì§€ ì•ŠìŒ(naive ì €ì¥ â†’ +00:00 ì•ˆ ë¶™ìŒ)
            processed = pd.DataFrame({
                'datetime': pd.to_datetime(df_raw['open_time'], unit='ms'),  # naive
                'open':  df_raw['open'],
                'high':  df_raw['high'],
                'low':   df_raw['low'],
                'close': df_raw['close'],
                'volume':df_raw['volume']
            })

            processed.set_index('datetime', inplace=True)
            processed.sort_index(inplace=True)

            # ê°™ì€ chunk ë‚´ ì¤‘ë³µ ì¸ë±ìŠ¤ ì œê±° (ë„¤íŠ¸ì›Œí¬/íŒŒì¼ ê²½ê³„ ì´ìƒì¹˜ ëŒ€ë¹„)
            if processed.index.has_duplicates:
                processed = processed[~processed.index.duplicated(keep='last')]

            # ê¸°ì¡´ íŒŒì¼ì˜ ë§ˆì§€ë§‰ ì‹œì  ì´í›„ë§Œ append (íŒŒì¼ ê°„ ì¤‘ë³µ ë°©ì§€)
            if last_ts is not None:
                processed = processed[processed.index > last_ts]
                if processed.empty:
                    # ì´ë²ˆ íŒŒì¼ ë‚´ìš© ì „ë¶€ê°€ ì´ë¯¸ ë°˜ì˜ë¨
                    try:
                        file.unlink()
                    except Exception:
                        pass
                    continue

            # í˜¹ì‹œ tz-awareë©´ ì¶œë ¥ ì „ì— naiveë¡œ ê°•ì œ (ì•ˆì „ì¥ì¹˜)
            if getattr(processed.index, 'tz', None) is not None:
                processed.index = processed.index.tz_localize(None)

            # ğŸ‘‰ ìŠ¤íŠ¸ë¦¬ë° append: ì¦‰ì‹œ ë””ìŠ¤í¬ë¡œ
            processed.to_csv(
                fout,
                header=header_needed,
                index_label='datetime',
                date_format="%Y-%m-%d %H:%M:%S"   # +00:00 ì—†ëŠ” ê³ ì • í¬ë§·
            )
            header_needed = False

            # ë‹¤ìŒ íŒŒì¼ í•„í„°ë§ì„ ìœ„í•´ last_ts ê°±ì‹ 
            last_ts = processed.index[-1]

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del df_raw, processed
            gc.collect()

    # temp íŒŒì¼ ì •ë¦¬
    for file in csv_files:
        try:
            file.unlink()
        except Exception as e:
            logging.error(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜ {file}: {e}")
            
def _date_key(p: Path) -> tuple:
    """
    íŒŒì¼ëª… ëì´ YYYY-MM.csv ë˜ëŠ” YYYY-MM-DD.csv ëª¨ë‘ ì²˜ë¦¬.
    ì •ë ¬ ì•ˆì •ì„±ì„ ìœ„í•´ (year, month, day) íŠœí”Œì„ ë°˜í™˜. dayê°€ ì—†ìœ¼ë©´ 1ë¡œ ë‘ .
    """
    stem = p.stem  # e.g., SYMBOL-1d-2024-07 or SYMBOL-1d-2024-07-15
    tail = stem.rsplit('-', 1)[-1]  # 'YYYY-MM' or 'YYYY-MM-DD'
    # ì¼/ì›” í¬ë§· ëª¨ë‘ ì‹œë„
    for fmt in ("%Y-%m-%d", "%Y-%m"):
        try:
            d = datetime.strptime(tail, fmt)
            if fmt == "%Y-%m":
                return (d.year, d.month, 1)
            return (d.year, d.month, d.day)
        except ValueError:
            continue
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë§¨ ë’¤ì—ì„œ ë‘ ì¡°ê°ì„ ì¡ì•„ë³´ëŠ” ë³´í˜¸ ë¡œì§
    parts = tail.split('-')
    try:
        y, m = int(parts[0]), int(parts[1])
        d = int(parts[2]) if len(parts) > 2 else 1
        return (y, m, d)
    except Exception:
        # ìµœí›„ì˜ ë³´í˜¸: ì•„ì£¼ ì•ìª½ìœ¼ë¡œ ê°€ë„ë¡
        return (1, 1, 1)

def process_symbol(args: Tuple[str, str, Path, datetime, datetime, str]) -> bool:
    """
    ë‹¨ì¼ ì‹¬ë³¼ ì²˜ë¦¬ (ì›”ë³„ ZIP ì‚¬ìš©).
    args: (symbol, interval, base_dir, start_date, end_date, data_type)
    - start_date/end_date: YYYY-MM ê¸°ë°˜ì´ì–´ë„ datetimeì´ë©´ OK. monthë§Œ ì“°ì„.
    """
    symbol, interval, base_dir, start_date, end_date, data_type = args
    try:
        logging.info(f"ì²˜ë¦¬ ì¤‘: {symbol} (ê°„ê²©: {interval})")
        output_filename = base_dir / f'{symbol}.csv'
        temp_dir = base_dir / f'temp_dir_{symbol}'

        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ts ê¸°ì¤€ "ê·¸ ë‹¬ë¶€í„°" ë‹¤ì‹œ ë°›ê¸° (ê°™ì€ ë‹¬ ì¼ë¶€ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        last_ts = get_last_timestamp(output_filename)
        if last_ts is not None:
            # í•´ë‹¹ ì›” 1ì¼ë¡œ ë‚´ë¦¼
            start_date = datetime(last_ts.year, last_ts.month, 1)
            logging.info(f"{symbol}: {start_date:%Y-%m}ë¶€í„° ì¬ê°œ")
            # ìµœì‹  ì—¬ë¶€ ì²´í¬ëŠ” merge ë‹¨ê³„ì—ì„œ ìë™ í•„í„°ë§(> last_ts)ìœ¼ë¡œ ë³´ì¥ë˜ë¯€ë¡œ ë”°ë¡œ early return ì•ˆ í•¨
        else:
            # ì „ë‹¬ë°›ì€ start_dateë¥¼ ì›”ì´ˆë¡œ ì •ê·œí™”
            start_date = datetime(start_date.year, start_date.month, 1)

        # end_dateë„ ì›”ì´ˆë¡œ ì •ê·œí™”
        end_date = datetime(end_date.year, end_date.month, 1)

        temp_dir.mkdir(parents=True, exist_ok=True)

        # âœ… ì›”ë³„ ZIP URL ìƒì„±
        urls = generate_monthly_urls(symbol, start_date, end_date, interval, data_type)
        if urls:
            # ì›” ZIPì€ í¬ë¯€ë¡œ ë™ì‹œì„± ë„ˆë¬´ ë†’ì´ì§€ ì•ŠëŠ” ê²Œ ìœ ë¦¬(ë””í´íŠ¸ ë‚´ë¶€ ì •ì±… ì‚¬ìš©)
            download_and_unzip(urls, temp_dir)
            merge_data_files(temp_dir, output_filename, existing_df=None)
            logging.info(f"{symbol} ì²˜ë¦¬ ì™„ë£Œ")

        # temp ì •ë¦¬
        if temp_dir.exists():
            for file in temp_dir.iterdir():
                try:
                    file.unlink()
                except Exception:
                    pass
            try:
                temp_dir.rmdir()
            except Exception:
                pass

        return True
    except Exception as e:
        logging.error(f"ì‹¬ë³¼ ì²˜ë¦¬ ì˜¤ë¥˜ {symbol}: {e}")
        return False
    finally:
        gc.collect()


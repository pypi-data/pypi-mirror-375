"""add FT search to sqlite3

Revision ID: d8e4b5427f24
Revises: 155970a8159a
Create Date: 2025-03-03 11:19:57.603218

"""
from typing import Sequence, Union
from sqlalchemy.sql import text
from alembic import op
import sqlalchemy as sa

import logging, regex as re, json
logger = logging.getLogger(__name__)

# revision identifiers, used by Alembic.
revision: str = 'd8e4b5427f24'
down_revision: Union[str, None] = '155970a8159a'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    """
    This migration consists of two parts ::
    - rebuilding of the search helper to include all the attributes of the dataset
    - add a full-text search table to the database
    """
    session = sa.orm.Session(bind=op.get_bind())

    # Step 1: Rebuilding the search helper
    rebuild_search_helper(session)
    
    # Step 2: Add the full-text search table    
    result = session.execute(
        text("SELECT name FROM sqlite_master WHERE type='table' AND name='datasets_fts';")
    ).fetchone()
    
    # Only create the table and triggers if it doesn't exist
    if not result:
        # Execute each statement separately
        session.execute(text('''
        CREATE VIRTUAL TABLE datasets_fts USING fts5(
            search_helper,
            content='datasets',
            content_rowid='id',
            tokenize='porter unicode61'
        );
        '''))
        
        session.execute(text('''
        CREATE TRIGGER datasets_ai AFTER INSERT ON datasets BEGIN
            INSERT INTO datasets_fts(rowid, search_helper)
            VALUES (new.id, new.search_helper);
        END;
        '''))
        
        session.execute(text('''
        CREATE TRIGGER datasets_au AFTER UPDATE ON datasets BEGIN
            INSERT INTO datasets_fts(datasets_fts, rowid, search_helper)
            VALUES('delete', old.id, old.search_helper);
            INSERT INTO datasets_fts(rowid, search_helper)
            VALUES (new.id, new.search_helper);
        END;
        '''))
        
        session.execute(text('''
        CREATE TRIGGER datasets_ad AFTER DELETE ON datasets BEGIN
            INSERT INTO datasets_fts(datasets_fts, rowid, search_helper)
            VALUES('delete', old.id, old.search_helper);
        END;
        '''))
        
        # Rebuild the FTS table
        session.execute(text("INSERT INTO datasets_fts(datasets_fts) VALUES('rebuild');"))
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('datasets_fts')
    # remove triggers
    op.execute(text('DROP TRIGGER datasets_ai;'))
    op.execute(text('DROP TRIGGER datasets_au;'))
    op.execute(text('DROP TRIGGER datasets_ad;'))
    # ### end Alembic commands ###
    pass

BLOCK_SIZE = 10000

def build_search_helper(row) -> str:
    """
    row is a SQLAlchemy row object with:
      row.id
      row.description
      row.notes
      row.keywords
      row.all_files (JSON array)
    """
    search_helper = ""
    if row.name is not None:
        search_helper += f"{row.name} "
    
    if row.description is not None:
        search_helper += f"{row.description} "
    if row.notes is not None:
        search_helper += f"{row.notes} "

    if row.keywords:
        try:
            keywords = json.loads(row.keywords)
            if keywords:
                search_helper += " ".join(keywords) + " "
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse keywords for dataset {row.id}")

    if row.all_attributes:
        try:
            attributes = json.loads(row.all_attributes)
            for attr_info in attributes:
                if attr_info:
                    attr_key = attr_info.get('key', '')
                    attr_value = attr_info.get('value', '')

                    if attr_key:
                        search_helper += f"{attr_key} "
                    if attr_value:
                        search_helper += f"{attr_value} "
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse attributes for dataset {row.id}")
    
    file_names = set()
    if row.all_files:
        try:
            files = json.loads(row.all_files)
            for file_info in files:
                filename = file_info.get('filename', '')
                name = file_info.get('name', '')
                if filename:
                    file_names.add(filename)
                    if name and not filename.startswith(name):
                        file_names.add(name)
        except json.JSONDecodeError:
            logger.warning(f"Failed to parse files for dataset {row.id}")

    if file_names:
        search_helper += " ".join(file_names)

    return search_helper.strip()       

def clean_for_search_helper(text : str) -> str:
    """
    Replace non-alphanumeric characters with spaces to create a clean string (to match postgres' to_tsvector)
    
    Parameters:
        text (str): The input text to clean
        
    Returns:
        str: Text with non-alphanumeric characters replaced by spaces
    """
    if text is None:
        return ""
    
    cleaned_word = re.sub(r'[^\p{L}\d]', ' ', text)
    cleaned_word = re.sub(r'\s+', ' ', cleaned_word)
    cleaned_word = cleaned_word.strip()
    
    return cleaned_word

def rebuild_search_helper(session):
    result = session.execute(text("SELECT count(*) FROM datasets")).scalar()

    n_blocks = (result + BLOCK_SIZE - 1) // BLOCK_SIZE
    logger.info("Rebuilding search helper...")
    print("Rebuilding search helper...")
    for i, offset in enumerate(range(0, result, BLOCK_SIZE)):
        query = f"""
                WITH dataset_attributes AS (
                    SELECT 
                        l.dataset_id,
                        json_group_array(json_object('key', a.key, 'value', a.value)) AS attributes
                    FROM ds_attr_link l
                    JOIN dataset_attr a ON a.id = l.dataset_attr_id
                    GROUP BY l.dataset_id
                ),
                dataset_files AS (
                    SELECT 
                        dataset_id,
                        json_group_array(json_object('name', name, 'filename', filename)) AS files
                    FROM files
                    GROUP BY dataset_id
                )
                SELECT
                    d.id,
                    d.name,
                    d.description,
                    d.notes,
                    d.keywords,
                    a.attributes AS all_attributes,
                    f.files AS all_files
                FROM datasets d
                LEFT JOIN dataset_attributes a ON d.id = a.dataset_id
                LEFT JOIN dataset_files f ON d.id = f.dataset_id
                ORDER BY d.id
                limit {BLOCK_SIZE} offset {offset};
                """
        stmt = text(query)
        rows = session.execute(stmt).fetchall()

        search_helpers = []
        for row in rows:
            search_helper = clean_for_search_helper(build_search_helper(row))
            search_helpers.append({"id": row.id, "search_helper": search_helper})

        update_stmt = text("""
            UPDATE datasets
            SET search_helper = :search_helper
            WHERE id = :id;
        """)
        session.execute(update_stmt, search_helpers)
        session.commit()

        # Print progress in whole percentages
        progress = int(((i + 1) / n_blocks) * 100)
        print(f"Completed {progress}% of datasets.")
        logger.info(f"Completed {progress}% of datasets.")
    logger.info("Rebuilding search helper complete.")
    print("Rebuilding search helper complete.")

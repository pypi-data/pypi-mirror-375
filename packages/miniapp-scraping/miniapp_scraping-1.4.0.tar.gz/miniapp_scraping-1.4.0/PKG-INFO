Metadata-Version: 2.4
Name: miniapp-scraping
Version: 1.4.0
Summary: Lightweight web scraping tool with email extraction
Home-page: https://github.com/TECHFUND/miniapp-scraping
Author: TECHFUND Development Team
Author-email: dev@techfund.jp
Project-URL: Bug Tracker, https://github.com/TECHFUND/miniapp-scraping/issues
Project-URL: Documentation, https://github.com/TECHFUND/miniapp-scraping#readme
Project-URL: Source Code, https://github.com/TECHFUND/miniapp-scraping
Project-URL: Changelog, https://github.com/TECHFUND/miniapp-scraping/blob/main/CHANGELOG.md
Keywords: scraping,miniapp,press-release,email-extraction,web-scraping,data-extraction,journalism,media,automation
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Information Technology
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Internet :: WWW/HTTP :: Indexing/Search
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing :: Markup :: HTML
Classifier: Topic :: Communications :: Email
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests>=2.25.1
Requires-Dist: beautifulsoup4>=4.9.3
Requires-Dist: lxml>=4.6.3
Provides-Extra: dev
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: pytest>=7.0.0; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Miniapp Scraping v1.4.0

A lightweight, efficient Python tool for web scraping press releases with advanced email extraction capabilities.

## ğŸ¯ Features v1.4.0

### âœ¨ Core Functionality
- **ğŸ“§ Email Extraction**: Advanced email address detection from press releases
- **ğŸ” Contact Information**: Extract phone numbers, FAX, and company details  
- **ğŸš€ Pure Python**: Uses only requests + BeautifulSoup4 (no external APIs)
- **âš¡ Lightweight**: Minimal dependencies, fast execution
- **ğŸ› ï¸ CLI Interface**: Easy command-line integration

### ğŸ† Proven Technology
- **âœ… Real Web Scraping**: Tested on actual PRTimes pages
- **ğŸ“§ Email Success Rate**: Validated email extraction patterns
- **ğŸ Pure Python**: No browser automation or heavy frameworks
- **ğŸ’¾ JSON Output**: Structured data format for easy integration

## Version Information
- **Version**: 1.4.0 - PyPI Release
- **Released**: September 2025
- **Language**: Python 3.8+
- **Dependencies**: 3 core packages only

## ğŸš€ Quick Start

### Installation
```bash
pip install miniapp-scraping
```

### Command Line Usage
```bash
# Basic scraping
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html

# Verbose output with custom filename
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html -v -o my_data.json
```

### Python Integration
```python
from miniapp_scraping import MiniappScraper

# Initialize scraper
scraper = MiniappScraper()

# Scrape a press release
url = "https://prtimes.jp/main/html/rd/p/000000001.000000001.html"
result = scraper.scrape(url)

# Access extracted data
print(f"Title: {result['title']}")
print(f"Company: {result['company']}")
print(f"Emails found: {result['emails']}")

# Save to JSON
scraper.save_json(result, "output.json")
```

## ğŸ“Š Data Structure

### Extracted Information
```json
{
  "url": "https://prtimes.jp/...",
  "title": "Press Release Title",
  "company": "Company Name Ltd.",
  "date": "2025å¹´9æœˆ10æ—¥",
  "content": "Full press release content...",
  "emails": ["contact@company.com", "info@company.com"],
  "scraped_at": "2025-09-10T12:00:00",
  "version": "1.4.0"
}
```

## ğŸ”§ Advanced Usage

### CLI Options
```bash
# Help
miniapp-scraping --help

# Verbose mode
miniapp-scraping URL --verbose

# Custom output file
miniapp-scraping URL --output custom_name.json
```

### Python API
```python
from miniapp_scraping import MiniappScraper

scraper = MiniappScraper()

# Scrape multiple URLs
urls = [
    "https://prtimes.jp/main/html/rd/p/000000001.000000001.html",
    "https://prtimes.jp/main/html/rd/p/000000002.000000001.html"
]

results = []
for url in urls:
    data = scraper.scrape(url)
    if data:
        results.append(data)

# Batch save
for i, result in enumerate(results):
    scraper.save_json(result, f"release_{i+1}.json")
```

## ğŸ“ Project Structure

```
miniapp_scraping/
â”œâ”€â”€ __init__.py       # Package initialization
â”œâ”€â”€ scraper.py        # Core scraping functionality  
â””â”€â”€ cli.py           # Command-line interface
```

## ğŸ§ª Tested Email Patterns

### Supported Formats
- **Context emails**: ãŠå•ã„åˆã‚ã›ï¼šcontact@example.com
- **Standard format**: info@company.co.jp
- **Special purpose**: press@, support@, inquiry@
- **HTML entities**: Encoded email addresses

### Validation
- âœ… RFC-compliant email format checking
- âœ… Duplicate removal
- âœ… Priority-based extraction (contact info first)

## ğŸ¯ Use Cases

### Business Applications
- **Media Monitoring**: Track company announcements
- **Lead Generation**: Extract contact information for outreach
- **Market Research**: Analyze industry press releases
- **Journalism**: Gather information for news stories

### Technical Integration
- **Data Pipelines**: Integrate with existing workflows
- **Automation**: Schedule regular scraping tasks
- **Analysis**: Feed data into business intelligence tools
- **Archives**: Build press release databases

## âš™ï¸ System Requirements

- **Python**: 3.8 or higher
- **OS**: Windows, macOS, Linux
- **Internet**: Required for scraping
- **Memory**: Minimal (< 50MB typical usage)

## ğŸ”„ Changelog

### v1.4.0 (September 2025) - PyPI Release
- **ğŸ¯ PyPI Publication**: Official package release
- **ğŸ“§ Enhanced Email Extraction**: Multi-pattern detection
- **ğŸ› ï¸ CLI Interface**: Command-line tool included
- **âš¡ Performance**: Optimized for speed and efficiency
- **ğŸ“ Documentation**: Comprehensive English documentation

### Previous Versions
- v1.3.0: Email extraction enhancement
- v1.2.1: JSONL database integration
- v1.1.0: Structured data output

## ğŸ› ï¸ Development

### Local Development
```bash
# Clone repository
git clone https://github.com/TECHFUND/miniapp-scraping
cd miniapp-scraping

# Install in development mode
pip install -e .

# Run tests
python -m pytest
```

### Building Package
```bash
# Build distribution
python setup.py sdist bdist_wheel

# Upload to PyPI (maintainers only)
twine upload dist/*
```

## ğŸ“ Support & Contributing

- **GitHub Issues**: [Report bugs or request features](https://github.com/TECHFUND/miniapp-scraping/issues)
- **Documentation**: [Full documentation](https://github.com/TECHFUND/miniapp-scraping#readme)
- **Contributing**: Pull requests welcome

## ğŸ“‹ License

MIT License - see [LICENSE](LICENSE) file for details.

## âš ï¸ Legal Notice

This tool is designed for legitimate research and business purposes. Please respect website terms of service and robots.txt when using this scraper.

---

**âœ¨ v1.4.0 Highlights**: Production-ready PyPI package with enhanced email extraction and CLI interface for seamless integration into any workflow.

# Miniapp Scraping v1.4.0

ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã‚’åŠ¹ç‡çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®æŠ½å‡ºã«ç‰¹åŒ–ã—ãŸè»½é‡Pythonãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## ğŸ¯ ç‰¹å¾´ v1.4.0

### âœ¨ ä¸»è¦æ©Ÿèƒ½
- **ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹æŠ½å‡º**: ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã‹ã‚‰é«˜ç²¾åº¦ã§ãƒ¡ãƒ¼ãƒ«æŠ½å‡º
- **ğŸ” é€£çµ¡å…ˆæƒ…å ±**: é›»è©±ç•ªå·ãƒ»FAXãƒ»ä¼æ¥­è©³ç´°ã‚‚è‡ªå‹•å–å¾—
- **ğŸš€ ç´”ç²‹Python**: requests + BeautifulSoup4ã®ã¿ä½¿ç”¨
- **âš¡ è»½é‡è¨­è¨ˆ**: æœ€å°é™ã®ä¾å­˜é–¢ä¿‚ã€é«˜é€Ÿå®Ÿè¡Œ
- **ğŸ› ï¸ CLIå¯¾å¿œ**: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³çµ±åˆãŒç°¡å˜

### ğŸ† å®Ÿè¨¼æ¸ˆã¿æŠ€è¡“
- **âœ… ãƒªã‚¢ãƒ«Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**: å®Ÿéš›ã®PRTimesãƒšãƒ¼ã‚¸ã§ãƒ†ã‚¹ãƒˆæ¸ˆã¿
- **ğŸ“§ ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºç²¾åº¦**: æ¤œè¨¼æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
- **ğŸ Pure Python**: ãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–ã‚„é‡ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä¸è¦
- **ğŸ’¾ JSONå‡ºåŠ›**: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§ç°¡å˜çµ±åˆ

## ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
- **Version**: 1.4.0 - PyPIå…¬é–‹ç‰ˆ
- **Released**: 2025å¹´9æœˆ
- **Language**: Python 3.8+
- **Dependencies**: æ ¸ã¨ãªã‚‹3ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã¿

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
pip install miniapp-scraping
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ä½¿ç”¨
```bash
# åŸºæœ¬çš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html

# è©³ç´°å‡ºåŠ›ã§ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚¡ã‚¤ãƒ«åæŒ‡å®š
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html -v -o my_data.json
```

### Pythonçµ±åˆ
```python
from miniapp_scraping import MiniappScraper

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼åˆæœŸåŒ–
scraper = MiniappScraper()

# ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹å–å¾—
url = "https://prtimes.jp/main/html/rd/p/000000001.000000001.html"
result = scraper.scrape(url)

# æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹
print(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['title']}")
print(f"ä¼šç¤¾å: {result['company']}")
print(f"ãƒ¡ãƒ¼ãƒ«ç™ºè¦‹æ•°: {result['emails']}")

# JSONä¿å­˜
scraper.save_json(result, "output.json")
```

## ğŸ”§ é«˜åº¦ãªä½¿ç”¨æ–¹æ³•

### CLIã‚ªãƒ—ã‚·ãƒ§ãƒ³
```bash
# ãƒ˜ãƒ«ãƒ—
miniapp-scraping --help

# è©³ç´°ãƒ¢ãƒ¼ãƒ‰
miniapp-scraping URL --verbose

# ã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
miniapp-scraping URL --output custom_name.json
```

## ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»è²¢çŒ®

- **GitHub Issues**: [ãƒã‚°ãƒ¬ãƒãƒ¼ãƒˆã‚„æ©Ÿèƒ½ãƒªã‚¯ã‚¨ã‚¹ãƒˆ](https://github.com/TECHFUND/miniapp-scraping/issues)
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: [å®Œå…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://github.com/TECHFUND/miniapp-scraping#readme)
- **è²¢çŒ®**: ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ­“è¿

---

**âœ¨ v1.4.0ã®ç‰¹å¾´**: ãƒ¡ãƒ¼ãƒ«æŠ½å‡ºã¨CLIã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å¼·åŒ–ã—ãŸæœ¬æ ¼çš„ãªPyPIãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã€ã‚ã‚‰ã‚†ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¸ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆãŒå¯èƒ½ã€‚

# Miniapp Scraping v1.4.0

ä¸€ä¸ªè½»é‡çº§ã€é«˜æ•ˆçš„Pythonå·¥å…·ï¼Œç”¨äºæŠ“å–æ–°é—»ç¨¿å¹¶æå–ç”µå­é‚®ä»¶åœ°å€ã€‚

## ğŸ¯ ç‰¹æ€§ v1.4.0

### âœ¨ æ ¸å¿ƒåŠŸèƒ½
- **ğŸ“§ ç”µå­é‚®ä»¶æå–**: ä»æ–°é—»ç¨¿ä¸­é«˜ç²¾åº¦æå–ç”µå­é‚®ä»¶
- **ğŸ” è”ç³»ä¿¡æ¯**: è‡ªåŠ¨è·å–ç”µè¯å·ç ã€ä¼ çœŸã€å…¬å¸è¯¦æƒ…
- **ğŸš€ çº¯Python**: ä»…ä½¿ç”¨requests + BeautifulSoup4
- **âš¡ è½»é‡è®¾è®¡**: æœ€å°ä¾èµ–å…³ç³»ï¼Œå¿«é€Ÿæ‰§è¡Œ
- **ğŸ› ï¸ CLIæ”¯æŒ**: è½»æ¾å‘½ä»¤è¡Œé›†æˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
pip install miniapp-scraping
```

### å‘½ä»¤è¡Œä½¿ç”¨
```bash
# åŸºæœ¬æŠ“å–
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html

# è¯¦ç»†è¾“å‡ºè‡ªå®šä¹‰æ–‡ä»¶å
miniapp-scraping https://prtimes.jp/main/html/rd/p/000000001.000000001.html -v -o my_data.json
```

---

**âœ¨ v1.4.0äº®ç‚¹**: ç”Ÿäº§å°±ç»ªçš„PyPIåŒ…ï¼Œå¢å¼ºçš„ç”µå­é‚®ä»¶æå–å’ŒCLIç•Œé¢ï¼Œå¯æ— ç¼é›†æˆåˆ°ä»»ä½•å·¥ä½œæµç¨‹ä¸­ã€‚

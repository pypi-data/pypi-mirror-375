"""
SKADA (Statistical Knowledge Adaptation for Domain Adaptation) ëª¨ë“ˆ
ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ í†µê³„ì  ì§€ì‹ ì ì‘ ê¸°ë²•ë“¤ê³¼ êµìœ¡ìš© ì œì¶œ ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
from xgboost import XGBClassifier
import warnings
import os
import json
import pickle
import re
import requests
import base64
from datetime import datetime
from . import __version__
from pathlib import Path
from typing import List, Dict, Tuple, Type, Callable, Any, Union, NamedTuple
from functools import wraps, partial
from sys import stderr
import inspect
from inspect import signature, Signature

# ê²½ê³  ì¶œë ¥ í•¨ìˆ˜
warn = partial(print, file=stderr)

# API ì„¤ì •
SKADA_API_URL = "https://skada.quest/api/miniskada"
DEFAULT_TIMEOUT = 30

# Matplotlib í•œê¸€ ì„¤ì •
import seaborn as sns
import matplotlib
import matplotlib.figure as figure
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
matplotlib.rcParams['axes.unicode_minus'] = False
try:
    # macOS/Linuxìš© í•œê¸€ í°íŠ¸ ê²½ë¡œë“¤
    font_paths = [
        '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',  # Ubuntu
        '/System/Library/Fonts/AppleGothic.ttf',  # macOS
        '/System/Library/Fonts/Helvetica.ttc',  # macOS fallback
    ]
    
    font_path = None
    for path in font_paths:
        if os.path.exists(path):
            font_path = path
            break
    
    if font_path:
        font_name = fm.FontProperties(fname=font_path, size=10).get_name()
        plt.rc('font', family=font_name)
    else:
        # ì‹œìŠ¤í…œ ê¸°ë³¸ í•œê¸€ í°íŠ¸ ì‚¬ìš©
        plt.rcParams['font.family'] = ['DejaVu Sans', 'Malgun Gothic', 'AppleGothic', 'NanumGothic']
except Exception:
    # í°íŠ¸ ì„¤ì • ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì„¤ì • ìœ ì§€
    pass


# =============================================================================
# API ì œì¶œ ì‹œìŠ¤í…œ
# =============================================================================

class SubmissionError(Exception):
    """ì œì¶œ ê´€ë ¨ ì˜¤ë¥˜"""
    pass


def serialize_data(data: Any) -> Dict[str, Any]:
    """
    ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        data: ì§ë ¬í™”í•  ë°ì´í„°
        
    Returns:
        Dict containing serialized data with type information
    """
    if isinstance(data, np.ndarray):
        return {
            "type": "numpy",
            "content": base64.b64encode(data.tobytes()).decode('utf-8'),
            "dtype": str(data.dtype),
            "shape": data.shape
        }
    elif isinstance(data, pd.DataFrame):
        return {
            "type": "dataframe", 
            "content": data.to_json(orient='records'),
            "columns": list(data.columns),
            "index": list(data.index)
        }
    elif isinstance(data, pd.Series):
        return {
            "type": "series",
            "content": data.to_json(),
            "name": data.name,
            "index": list(data.index)
        }
    elif isinstance(data, (list, tuple)):
        return {
            "type": "list" if isinstance(data, list) else "tuple",
            "content": [serialize_data(item)["content"] if isinstance(item, (np.ndarray, pd.DataFrame, pd.Series)) 
                       else item for item in data]
        }
    elif isinstance(data, dict):
        return {
            "type": "dict",
            "content": {k: serialize_data(v)["content"] if isinstance(v, (np.ndarray, pd.DataFrame, pd.Series)) 
                       else v for k, v in data.items()}
        }
    elif callable(data):
        # í•¨ìˆ˜ì˜ ê²½ìš° pickleë¡œ ì§ë ¬í™”
        return {
            "type": "function",
            "content": base64.b64encode(pickle.dumps(data)).decode('utf-8'),
            "name": getattr(data, '__name__', 'anonymous_function')
        }
    else:
        # ê¸°ë³¸ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ íƒ€ì…ë“¤ (int, float, str, bool, None)
        return {
            "type": "primitive",
            "content": data
        }


def submit_to_api(problem_id: str, answer_data: Any, email: str = None, 
                 session_id: str = None, metadata: Dict = None) -> Dict[str, Any]:
    """
    ë‹µì•ˆì„ SKADA API ì„œë²„ë¡œ ì œì¶œ
    
    Args:
        problem_id: ë¬¸ì œ ID (ì˜ˆ: "Q4_1", "Q5_2")
        answer_data: ì œì¶œí•  ë‹µì•ˆ ë°ì´í„°
        email: í•™ìƒ ì´ë©”ì¼ ì£¼ì†Œ
        session_id: ì„¸ì…˜ ID
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
    Returns:
        ì„œë²„ ì‘ë‹µ ë°ì´í„°
        
    Raises:
        SubmissionError: ì œì¶œ ì‹¤íŒ¨ ì‹œ
    """
    # ì´ë©”ì¼ê³¼ ì„¸ì…˜ ID í™•ì¸
    if email is None:
        email = os.environ.get('SKADA_EMAIL')
        if not email:
            raise SubmissionError("ì´ë©”ì¼ ì£¼ì†Œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SKADA.instance(email='your@email.com')ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    if session_id is None:
        session_id = os.environ.get('SKADA_SESSION_ID', f'session_{email.replace("@", "_").replace(".", "_")}_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    
    # ë°ì´í„° ì§ë ¬í™”
    serialized_data = serialize_data(answer_data)
    
    # ì œì¶œ í˜ì´ë¡œë“œ êµ¬ì„±
    payload = {
        "email": email,
        "session_id": session_id,
        "problem_id": problem_id,
        "answer_data": serialized_data,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "metadata": metadata or {}
    }
    
    try:
        # API í˜¸ì¶œ
        response = requests.post(
            f"{SKADA_API_URL}/submit",
            json=payload,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "mySUNI-SKADA/2.0.3"
            },
            timeout=DEFAULT_TIMEOUT
        )
        
        # ì‘ë‹µ ì²˜ë¦¬
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… {problem_id} ë‹µì•ˆ ì œì¶œ ì„±ê³µ!")
            if result.get('score') is not None:
                print(f"ğŸ“Š ì ìˆ˜: {result['score']}")
            if result.get('feedback'):
                print(f"ğŸ’¬ í”¼ë“œë°±: {result['feedback']}")
            return result
        else:
            error_msg = f"ì œì¶œ ì‹¤íŒ¨ (HTTP {response.status_code})"
            try:
                error_detail = response.json().get('error', response.text)
                error_msg += f": {error_detail}"
            except:
                error_msg += f": {response.text}"
            raise SubmissionError(error_msg)
            
    except requests.exceptions.Timeout:
        raise SubmissionError(f"ì œì¶œ ì‹œê°„ ì´ˆê³¼ ({DEFAULT_TIMEOUT}ì´ˆ)")
    except requests.exceptions.ConnectionError:
        raise SubmissionError("ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except requests.exceptions.RequestException as e:
        raise SubmissionError(f"ì œì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def submit_with_fallback(problem_id: str, answer_data: Any, files: List[str], 
                        submit_root: str, skada_instance=None, **kwargs) -> Dict[str, Any]:
    """
    API ì œì¶œì„ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ë¡œì»¬ íŒŒì¼ë¡œ ë°±ì—… ì €ì¥
    
    Args:
        problem_id: ë¬¸ì œ ID
        answer_data: ë‹µì•ˆ ë°ì´í„°
        files: ì €ì¥í•  íŒŒì¼ ëª©ë¡ (ë°±ì—…ìš©)
        submit_root: ë¡œì»¬ ì €ì¥ ë””ë ‰í† ë¦¬
        skada_instance: SKADA ì¸ìŠ¤í„´ìŠ¤ (ì´ë©”ì¼/ì„¸ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°ìš©)
        **kwargs: submit_to_apiì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
        
    Returns:
        ì œì¶œ ê²°ê³¼
    """
    try:
        # SKADA ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì´ë©”ì¼ê³¼ ì„¸ì…˜ ID ê°€ì ¸ì˜¤ê¸°
        if skada_instance and hasattr(skada_instance, 'email') and skada_instance.email:
            kwargs.setdefault('email', skada_instance.email)
            kwargs.setdefault('session_id', skada_instance.session_id)
        
        # API ì œì¶œ ì‹œë„
        return submit_to_api(problem_id, answer_data, **kwargs)
    except SubmissionError as e:
        warn(f"âš ï¸  API ì œì¶œ ì‹¤íŒ¨: {e}")
        warn("ğŸ“ ë¡œì»¬ íŒŒì¼ë¡œ ë°±ì—… ì €ì¥í•©ë‹ˆë‹¤...")
        
        # ë¡œì»¬ ë°±ì—… ì €ì¥
        os.makedirs(submit_root, exist_ok=True)
        
        if len(files) == 1:
            file_path = os.path.join(submit_root, files[0])
            
            if files[0].endswith('.json'):
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(answer_data, f, ensure_ascii=False, indent=2, default=str)
            elif files[0].endswith('.npy'):
                np.save(file_path, answer_data)
            elif files[0].endswith('.pkl'):
                with open(file_path, 'wb') as f:
                    pickle.dump(answer_data, f)
            elif files[0].endswith('.txt'):
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(str(answer_data))
        
        print(f"ğŸ’¾ ë¡œì»¬ ë°±ì—… ì™„ë£Œ: {submit_root}")
        return {"status": "backup_saved", "location": submit_root}


class DomainAdapter(BaseEstimator, TransformerMixin):
    """
    ë„ë©”ì¸ ì ì‘ì„ ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤
    """
    
    def __init__(self, method='coral', n_components=None):
        """
        Parameters:
        -----------
        method : str, default='coral'
            ì ì‘ ë°©ë²• ('coral', 'mmd', 'dann')
        n_components : int, optional
            ì°¨ì› ì¶•ì†Œí•  ì»´í¬ë„ŒíŠ¸ ìˆ˜
        """
        self.method = method
        self.n_components = n_components
        self.scaler_source = StandardScaler()
        self.scaler_target = StandardScaler()
        self.pca = None
        
    def fit(self, X_source, X_target):
        """
        ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°ë¡œ ì ì‘ê¸°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        X_source : array-like, shape (n_source_samples, n_features)
            ì†ŒìŠ¤ ë„ë©”ì¸ ë°ì´í„°
        X_target : array-like, shape (n_target_samples, n_features)
            íƒ€ê²Ÿ ë„ë©”ì¸ ë°ì´í„°
        """
        X_source = np.array(X_source)
        X_target = np.array(X_target)
        
        # ë°ì´í„° ì •ê·œí™”
        self.scaler_source.fit(X_source)
        self.scaler_target.fit(X_target)
        
        X_source_scaled = self.scaler_source.transform(X_source)
        X_target_scaled = self.scaler_target.transform(X_target)
        
        # ì°¨ì› ì¶•ì†Œ (ì„ íƒì )
        if self.n_components:
            self.pca = PCA(n_components=self.n_components)
            X_combined = np.vstack([X_source_scaled, X_target_scaled])
            self.pca.fit(X_combined)
        
        # ë„ë©”ì¸ ì ì‘ ë°©ë²•ë³„ í•™ìŠµ
        if self.method == 'coral':
            self._fit_coral(X_source_scaled, X_target_scaled)
        elif self.method == 'mmd':
            self._fit_mmd(X_source_scaled, X_target_scaled)
        else:
            warnings.warn(f"Method '{self.method}' not implemented. Using identity transformation.")
        
        return self
    
    def transform(self, X, domain='source'):
        """
        ë°ì´í„°ë¥¼ ì ì‘ëœ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Parameters:
        -----------
        X : array-like, shape (n_samples, n_features)
            ë³€í™˜í•  ë°ì´í„°
        domain : str, default='source'
            ë°ì´í„°ì˜ ë„ë©”ì¸ ('source' ë˜ëŠ” 'target')
        
        Returns:
        --------
        X_transformed : array-like, shape (n_samples, n_features_out)
            ë³€í™˜ëœ ë°ì´í„°
        """
        X = np.array(X)
        
        # ì •ê·œí™”
        if domain == 'source':
            X_scaled = self.scaler_source.transform(X)
        else:
            X_scaled = self.scaler_target.transform(X)
        
        # ì°¨ì› ì¶•ì†Œ
        if self.pca:
            X_scaled = self.pca.transform(X_scaled)
        
        # ë„ë©”ì¸ ì ì‘ ë³€í™˜
        if self.method == 'coral' and hasattr(self, 'coral_transform_'):
            X_transformed = self._transform_coral(X_scaled, domain)
        else:
            X_transformed = X_scaled
        
        return X_transformed
    
    def _fit_coral(self, X_source, X_target):
        """CORAL (Correlation Alignment) ë°©ë²•ìœ¼ë¡œ í•™ìŠµ"""
        # ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
        cov_source = np.cov(X_source.T) + np.eye(X_source.shape[1]) * 1e-6
        cov_target = np.cov(X_target.T) + np.eye(X_target.shape[1]) * 1e-6
        
        # CORAL ë³€í™˜ í–‰ë ¬ ê³„ì‚°
        try:
            # Cholesky ë¶„í•´ë¥¼ ì‚¬ìš©í•œ ì•ˆì •ì ì¸ ê³„ì‚°
            L_source = np.linalg.cholesky(cov_source)
            L_target = np.linalg.cholesky(cov_target)
            
            self.coral_transform_ = np.linalg.solve(L_source, L_target)
        except np.linalg.LinAlgError:
            # Cholesky ë¶„í•´ ì‹¤íŒ¨ì‹œ SVD ì‚¬ìš©
            U_s, S_s, Vt_s = np.linalg.svd(cov_source)
            U_t, S_t, Vt_t = np.linalg.svd(cov_target)
            
            sqrt_source_inv = U_s @ np.diag(1.0 / np.sqrt(S_s + 1e-6)) @ Vt_s
            sqrt_target = U_t @ np.diag(np.sqrt(S_t)) @ Vt_t
            
            self.coral_transform_ = sqrt_source_inv @ sqrt_target
    
    def _transform_coral(self, X, domain):
        """CORAL ë³€í™˜ ì ìš©"""
        if domain == 'source':
            return X @ self.coral_transform_.T
        else:
            return X
    
    def _fit_mmd(self, X_source, X_target):
        """MMD (Maximum Mean Discrepancy) ë°©ë²•ìœ¼ë¡œ í•™ìŠµ"""
        # ê°„ë‹¨í•œ MMD ê¸°ë°˜ ë³€í™˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ìµœì í™” í•„ìš”)
        mean_source = np.mean(X_source, axis=0)
        mean_target = np.mean(X_target, axis=0)
        self.mmd_shift_ = mean_target - mean_source


class CORAL(DomainAdapter):
    """
    CORAL (Correlation Alignment) ë„ë©”ì¸ ì ì‘
    """
    
    def __init__(self, n_components=None):
        super().__init__(method='coral', n_components=n_components)


class MMD(DomainAdapter):
    """
    MMD (Maximum Mean Discrepancy) ë„ë©”ì¸ ì ì‘
    """
    
    def __init__(self, n_components=None):
        super().__init__(method='mmd', n_components=n_components)


def domain_adaptation_score(X_source, X_target, y_source, y_target, adapter=None):
    """
    ë„ë©”ì¸ ì ì‘ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    X_source : array-like
        ì†ŒìŠ¤ ë„ë©”ì¸ íŠ¹ì„±
    X_target : array-like
        íƒ€ê²Ÿ ë„ë©”ì¸ íŠ¹ì„±
    y_source : array-like
        ì†ŒìŠ¤ ë„ë©”ì¸ ë ˆì´ë¸”
    y_target : array-like
        íƒ€ê²Ÿ ë„ë©”ì¸ ë ˆì´ë¸”
    adapter : DomainAdapter, optional
        ì‚¬ìš©í•  ë„ë©”ì¸ ì ì‘ê¸°
    
    Returns:
    --------
    score : dict
        í‰ê°€ ì ìˆ˜ë“¤
    """
    from sklearn.metrics import accuracy_score, f1_score
    from sklearn.ensemble import RandomForestClassifier
    
    if adapter is None:
        adapter = CORAL()
    
    # ë„ë©”ì¸ ì ì‘ê¸° í•™ìŠµ
    adapter.fit(X_source, X_target)
    
    # ë°ì´í„° ë³€í™˜
    X_source_adapted = adapter.transform(X_source, domain='source')
    X_target_adapted = adapter.transform(X_target, domain='target')
    
    # ë¶„ë¥˜ê¸° í•™ìŠµ (ì†ŒìŠ¤ ë„ë©”ì¸)
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_source_adapted, y_source)
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ì—ì„œ ì˜ˆì¸¡
    y_pred = clf.predict(X_target_adapted)
    
    # ì„±ëŠ¥ í‰ê°€
    accuracy = accuracy_score(y_target, y_pred)
    f1 = f1_score(y_target, y_pred, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'adapter': adapter.__class__.__name__
    }


def compare_domain_adapters(X_source, X_target, y_source, y_target):
    """
    ì—¬ëŸ¬ ë„ë©”ì¸ ì ì‘ ë°©ë²•ì„ ë¹„êµí•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    X_source : array-like
        ì†ŒìŠ¤ ë„ë©”ì¸ íŠ¹ì„±
    X_target : array-like
        íƒ€ê²Ÿ ë„ë©”ì¸ íŠ¹ì„±
    y_source : array-like
        ì†ŒìŠ¤ ë„ë©”ì¸ ë ˆì´ë¸”
    y_target : array-like
        íƒ€ê²Ÿ ë„ë©”ì¸ ë ˆì´ë¸”
    
    Returns:
    --------
    results : pd.DataFrame
        ë¹„êµ ê²°ê³¼
    """
    adapters = [
        ('No Adaptation', None),
        ('CORAL', CORAL()),
        ('MMD', MMD()),
    ]
    
    results = []
    
    for name, adapter in adapters:
        if adapter is None:
            # ì ì‘ ì—†ì´ ì§ì ‘ í•™ìŠµ
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import accuracy_score, f1_score
            
            clf = RandomForestClassifier(n_estimators=100, random_state=42)
            clf.fit(X_source, y_source)
            y_pred = clf.predict(X_target)
            
            accuracy = accuracy_score(y_target, y_pred)
            f1 = f1_score(y_target, y_pred, average='weighted')
            
            results.append({
                'Method': name,
                'Accuracy': accuracy,
                'F1_Score': f1
            })
        else:
            score = domain_adaptation_score(X_source, X_target, y_source, y_target, adapter)
            results.append({
                'Method': name,
                'Accuracy': score['accuracy'],
                'F1_Score': score['f1_score']
            })
    
    return pd.DataFrame(results)


# í¸ì˜ í•¨ìˆ˜ë“¤
def load_sample_domain_data():
    """
    ìƒ˜í”Œ ë„ë©”ì¸ ì ì‘ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    np.random.seed(42)
    
    # ì†ŒìŠ¤ ë„ë©”ì¸ (ì •ê·œë¶„í¬)
    X_source = np.random.normal(0, 1, (200, 10))
    y_source = (X_source[:, 0] + X_source[:, 1] > 0).astype(int)
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ (ë¶„í¬ê°€ ì•½ê°„ ë‹¤ë¦„)
    X_target = np.random.normal(0.5, 1.2, (150, 10))
    y_target = (X_target[:, 0] + X_target[:, 1] > 0.5).astype(int)
    
    return X_source, X_target, y_source, y_target


def plot_domain_comparison(X_source, X_target, adapter=None, feature_indices=[0, 1]):
    """
    ë„ë©”ì¸ ì ì‘ ì „í›„ì˜ ë°ì´í„° ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    import matplotlib.pyplot as plt
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # ì›ë³¸ ë°ì´í„°
    axes[0].scatter(X_source[:, feature_indices[0]], X_source[:, feature_indices[1]], 
                   alpha=0.6, label='Source', color='blue')
    axes[0].scatter(X_target[:, feature_indices[0]], X_target[:, feature_indices[1]], 
                   alpha=0.6, label='Target', color='red')
    axes[0].set_title('Original Data')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ì ì‘ëœ ë°ì´í„°
    if adapter is not None:
        adapter.fit(X_source, X_target)
        X_source_adapted = adapter.transform(X_source, domain='source')
        X_target_adapted = adapter.transform(X_target, domain='target')
        
        axes[1].scatter(X_source_adapted[:, feature_indices[0]], X_source_adapted[:, feature_indices[1]], 
                       alpha=0.6, label='Source (Adapted)', color='blue')
        axes[1].scatter(X_target_adapted[:, feature_indices[0]], X_target_adapted[:, feature_indices[1]], 
                       alpha=0.6, label='Target (Adapted)', color='red')
        axes[1].set_title(f'After {adapter.__class__.__name__} Adaptation')
    else:
        axes[1].text(0.5, 0.5, 'No Adapter Provided', 
                    transform=axes[1].transAxes, ha='center', va='center')
        axes[1].set_title('No Adaptation')
    
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


# ===== ì œì¶œ ë° ìºì‹± ì‹œìŠ¤í…œ =====

def type_checker(obj, type_hint: Type) -> bool:
    """íƒ€ì… ì²´í¬ í•¨ìˆ˜"""
    try:
        if getattr(type_hint, "_name", None) is not None:
            if type_hint._name == "Any":
                return True
            elif type_hint._name == "List":
                return all(type_checker(sub, type_hint.__args__[0]) for sub in obj)
            elif type_hint._name == "Tuple":
                return all(type_checker(sub, sub_type) for sub, sub_type in zip(obj, type_hint.__args__))
            elif type_hint._name == "Callable":
                return callable(obj)
        else:
            return isinstance(obj, type_hint)
    except:
        pass
    return False


def submit(files: List[str], submit_root: str):
    """ì œì¶œ ë°ì½”ë ˆì´í„°"""
    os.makedirs(submit_root, exist_ok=True)

    for file in files:
        file_path = os.path.join(submit_root, file)
        if not os.path.isfile(file_path):
            with open(file_path, 'w'):
                pass

    def wrapping(func):
        sig = signature(func)
        type_map = {
            name: param.annotation for name, param in sig.parameters.items()
            if param.annotation is not Signature.empty
        }

        @wraps(func)
        def wrapper(self, *args, **kwargs):
            # íƒ€ì… ì²´í¬
            params = sig.bind_partial(self, *args, **kwargs)
            for name, annotation in type_map.items():
                assert name in params.arguments and type_checker(params.arguments[name], annotation), \
                       f"{name}ì„ {annotation} íƒ€ì…ìœ¼ë¡œ ì œì¶œ ë°”ëë‹ˆë‹¤."

            self.path = [Path(os.path.join(submit_root, file)) for file in files] 
            
            # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰
            result = func(self, *args, **kwargs)
            
            # ë¬¸ì œ ID ì¶”ì¶œ (í•¨ìˆ˜ëª…ì—ì„œ)
            problem_id = func.__name__.replace('_answer', '').replace('_', '_')
            
            # ë‹µì•ˆ ë°ì´í„° ê²°ì • (ì²« ë²ˆì§¸ ì¸ì ë˜ëŠ” ì—¬ëŸ¬ ì¸ìë¥¼ íŠœí”Œë¡œ)
            if len(args) == 1:
                answer_data = args[0]
            else:
                answer_data = args
            
            # API ì œì¶œ ì‹œë„ (ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—…)
            try:
                submission_result = submit_with_fallback(
                    problem_id=problem_id,
                    answer_data=answer_data,
                    files=files,
                    submit_root=submit_root,
                    skada_instance=self,  # SKADA ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬
                    metadata={
                        "function_name": func.__name__,
                        "args_count": len(args),
                        "file_names": files,
                        "client_version": __version__
                    }
                )
                
                if submission_result.get("status") == "backup_saved":
                    warn("ğŸ“ ë¡œì»¬ ë°±ì—… ì €ì¥ ì™„ë£Œ. ìµœì¢… ì œì¶œì„ ìœ„í•´ì„  ëŸ°ë°•ìŠ¤ ìƒë‹¨ì˜ ì œì¶œë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                
                return submission_result
            except Exception as e:
                warn(f"ì œì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                warn("ğŸ“ ë¡œì»¬ ë°±ì—…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ìµœì¢… ì œì¶œì„ ìœ„í•´ì„  ëŸ°ë°•ìŠ¤ ìƒë‹¨ì˜ ì œì¶œë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                return {"status": "error", "error": str(e)}
        
        return wrapper

    return wrapping


class Singleton:
    """ì‹±ê¸€í†¤ íŒ¨í„´ êµ¬í˜„"""
    
    def __init__(self) -> None:
        # ì‹±ê¸€í†¤ íŒ¨í„´ì„ ìœ„í•œ ì´ˆê¸°í™”
        pass


# ===== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ =====

def disp_confusion_matrix(X_train, y_train, X_test, y_test):
    """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
    model = XGBClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    conf_mat = confusion_matrix(y_test, y_pred)
    df_conf_mat = pd.DataFrame(conf_mat, columns=['Prediction_Fail', 'Prediction_Pass'], 
                              index=['Real_Fail', 'Real_Pass'])
    plt.figure(figsize=(10, 7))
    plt.title('Confusion Matrix for XGBoost Classifier')
    sns.heatmap(df_conf_mat, annot=True, fmt='g')
    plt.show()


def draw_histograms(variables: pd.DataFrame, n_rows: int, n_cols: int):
    """íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°"""
    fig = plt.figure(figsize=(20, 10))
    for i, var_name in enumerate(variables):
        ax = fig.add_subplot(n_rows, n_cols, i+1)
        variables.iloc[:, i].hist(bins=10, ax=ax)
        ax.set_title(var_name + " Distribution")
    fig.tight_layout()
    plt.show()


def train_and_evaluate_xgb_model(X_train, y_train, X_test, y_test):
    """XGBoost ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    model = XGBClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    positive_tp = np.sum((y_pred == 1) & (y_test == 1))
    positive_fp = np.sum((y_pred == 1) & (y_test != 1))
    positive_fn = np.sum((y_pred != 1) & (y_test == 1))

    negative_tp = np.sum((y_pred == 0) & (y_test == 0))
    negative_fp = np.sum((y_pred == 0) & (y_test != 0))
    negative_fn = np.sum((y_pred != 0) & (y_test == 0))

    positive_precision = positive_tp / (positive_tp + positive_fp)
    positive_recall = positive_tp / (positive_tp + positive_fn)

    negative_precision = negative_tp / (negative_tp + negative_fp)
    negative_recall = negative_tp / (negative_tp + negative_fn)

    positive_f1 = 2 * (positive_precision * positive_recall) / (positive_precision + positive_recall)
    negative_f1 = 2 * (negative_precision * negative_recall) / (negative_precision + negative_recall)

    macro_f1 = (positive_f1 + negative_f1) / 2
    
    total_tp = positive_tp + negative_tp
    total_fp = positive_fp + negative_fp
    total_fn = positive_fn + negative_fn
    micro_precision = total_tp / (total_tp + total_fp)
    micro_recall = total_tp / (total_tp + total_fn)
    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)
    
    print(f"Pass (Class 1): {(y_test == 1).sum()} Samples".title())
    print(f"TP: {positive_tp}")
    print(f"FP: {positive_fp}")
    print(f"FN: {positive_fn}")
    print("#" * 100)
    print(f"Fail (Class 0): {(y_test == 0).sum()} Samples".title())
    print(f"TP: {negative_tp}")
    print(f"FP: {negative_fp}")
    print(f"FN: {negative_fn}")
    print("#" * 100)
    print(f'Micro F1 Score: {micro_f1:.4f}')
    print(f'Macro F1 Score: {macro_f1:.4f}')
    
    return macro_f1


def check_Q2_2_answer(submit):
    """Q2_2 ë‹µì•ˆ ì²´í¬ í•¨ìˆ˜"""
    answer = ['s242', 's281', 's285', 's161', 's84', 's290', 's167', 's227', 's293', 's268', 's150', 's16', 's74', 's278', 's159', 's296', 's106', 's243', 's83', 's279', 's277', 's26', 's87', 's238', 's297', 's32', 's75', 's151', 's231', 's267', 's264', 's276', 's157', 's235', 's295', 's228', 's239', 's284', 's76', 's292', 's230', 's233', 's288', 's291', 's82', 's287', 's97', 's229', 's299', 's85', 's109', 's80', 's273', 's274', 's149', 's275', 's282', 's283', 's79', 's91', 's176', 's294', 's269', 's154', 's300', 's286', 's179', 's88', 's240', 's166', 's236', 's164', 's73', 's298']
    assert set(answer) == set(submit), "ì˜ëª» êµ¬í•˜ì˜€ìœ¼ë¯€ë¡œ ë‹¤ì‹œ í™•ì¸í•˜ì‹œì˜¤."
    print("í†µê³¼í•˜ì˜€ìœ¼ë¯€ë¡œ ì•„ë˜ ë‹µì•ˆ ì œì¶œì„ ìˆ˜í–‰í•˜ì‹œì˜¤.") 


# ===== SKADA ë©”ì¸ í´ë˜ìŠ¤ =====

class SKADA(Singleton):
    """SKADA ë©”ì¸ í´ë˜ìŠ¤ - ë¯¸ë‹ˆìŠ¤ì¹´ë‹¤ 1ë²ˆê³¼ 2ë²ˆ ë¬¸ì œ í†µí•©"""
    SUBMISSION = 'submit'
    
    def __init__(self, email: str = None, cache_root='.cache') -> None:
        super().__init__()
        self.cache_root = cache_root
        self.path: List[Path] = None
        self.email = email
        self.session_id = None
        
        # ì´ë©”ì¼ ì„¤ì • ë° ì„¸ì…˜ ID ìƒì„±
        if email:
            self.email = email
            self.session_id = f"session_{email.replace('@', '_').replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            print(f"ğŸ“§ SKADA ì„¸ì…˜ ì‹œì‘: {email}")
            print(f"ğŸ”‘ ì„¸ì…˜ ID: {self.session_id}")
        else:
            print("âš ï¸  ì´ë©”ì¼ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SKADA.instance(email='your@email.com')ìœ¼ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    @classmethod
    def instance(cls, email: str = None):
        """
        SKADA ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±/ë°˜í™˜
        
        Args:
            email: í•™ìƒ ì´ë©”ì¼ ì£¼ì†Œ (í•„ìˆ˜)
            
        Returns:
            SKADA ì¸ìŠ¤í„´ìŠ¤
        """
        if not hasattr(cls, '_instance') or cls._instance is None:
            if not email:
                # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì´ë©”ì¼ í™•ì¸
                email = os.environ.get('SKADA_EMAIL')
                if not email:
                    raise ValueError(
                        "ì´ë©”ì¼ ì£¼ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n"
                        "1. SKADA.instance(email='your@email.com')\n"
                        "2. os.environ['SKADA_EMAIL'] = 'your@email.com'"
                    )
            cls._instance = cls(email=email)
        elif email and cls._instance.email != email:
            # ë‹¤ë¥¸ ì´ë©”ì¼ë¡œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            cls._instance = cls(email=email)
        
        return cls._instance

    # ===== ë¯¸ë‹ˆìŠ¤ì¹´ë‹¤ 1ë²ˆ ë¬¸ì œë“¤ =====
    
    @submit(['data1.npy'], SUBMISSION)
    def Q1_answer(self, X_test):
        """Q1 ë‹µì•ˆ ì œì¶œ"""
        np.save(self.path[0], X_test)

    @submit(['data2-1.npy', 'data2-1_2.npy'], SUBMISSION)
    def Q2_1_answer(self, X_test, mu):
        """Q2-1 ë‹µì•ˆ ì œì¶œ"""
        np.save(self.path[0], X_test)
        np.save(self.path[1], mu)

    @submit(['data2-2.txt'], SUBMISSION)
    def Q2_2_answer(self, drop_cols):
        """Q2-2 ë‹µì•ˆ ì œì¶œ"""
        with open(self.path[0], "w") as f:
            f.write(" ".join(drop_cols))

    @submit(['result2-3.pkl', 'result2-3.npy'], SUBMISSION)
    def Q2_3_answer(self, function, result):
        """Q2-3 ë‹µì•ˆ ì œì¶œ"""
        with open(self.path[0], "wb") as f:
            pickle.dump(function, f)
        np.save(self.path[1], result)
            
    @submit(['data3-1.txt'], SUBMISSION)
    def Q3_1_answer(self, resampled_sm_X_train, resampled_sm_y_train, resampled_ada_X_train, resampled_ada_y_train):
        """Q3-1 ë‹µì•ˆ ì œì¶œ"""
        with open(self.path[0], 'w') as f:
            f.write(str(len(resampled_sm_y_train) + len(resampled_ada_y_train)))

    @submit(['best_hyper_parameters.json'], SUBMISSION)
    def Q3_2_answer(self, best_hyper_parameters):
        """Q3-2 ë‹µì•ˆ ì œì¶œ"""
        with open(self.path[0], "w") as f:
            json.dump(best_hyper_parameters, f)

    # ===== ë¯¸ë‹ˆìŠ¤ì¹´ë‹¤ 2ë²ˆ ë¬¸ì œë“¤ =====
    
    @submit(['Q4_1.json'], SUBMISSION)
    def Q4_1_answer(self, answer):
        """Q4-1 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = answer.shape == (7043, 21)
            b = '6840-RESVD' not in answer['customerID'].tolist()
            json.dump(int(a and b), fd)
            
    @submit(['Q4_2.json'], SUBMISSION)
    def Q4_2_answer(self, answer):
        """Q4-2 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = answer['TotalCharges'].dtype == 'float64'
            b = not answer['TotalCharges'].isnull().any()
            json.dump(int(a and b), fd) 

    @submit(['Q4_3.json'], SUBMISSION)
    def Q4_3_answer(self, answer, numerical_features):
        """Q4-3 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = answer.shape == (7043, 29)
            b = sum(answer[numerical_features].max().round() == 1.0) == 3
            c = sum(answer[numerical_features].min().round() == 0.0) == 3
            d = 'customerID' in answer.columns
            e = 'SeniorCitizen' in answer.columns
            json.dump(int(a and b and c and d and e), fd) 
            
    @submit(['Q5_1.json'], SUBMISSION)
    def Q5_1_answer(self, answer):
        """Q5-1 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = len(answer) == 27
            b = True
            for i in range(26):
                if i == 0:
                    b = answer[i][1] >= answer[i+1][1]
                else:
                    b = (answer[i][1] >= answer[i+1][1]) and b
            
            json.dump(int(a and b), fd)
    
    @submit(['Q5_2.json'], SUBMISSION)
    def Q5_2_answer(self, f_count, f):
        """Q5-2 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = len(f) == f_count == 11
            b = f[0] == 'Contract_Month-to-month'
            c = f[-1] == 'OnlineSecurity'
            json.dump(int(a and b and c), fd)
    
    @submit(['Q6_1.json'], SUBMISSION)
    def Q6_1_answer(self, answer_1, answer_2):
        """Q6-1 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = round(answer_1, 3) == 0.806
            b = round(answer_2, 3) == 0.99
            json.dump(int(a and b), fd)
    
    @submit(['Q6_2.json'], SUBMISSION)
    def Q6_2_answer(self, answer_1, answer_2):
        """Q6-2 ë‹µì•ˆ ì œì¶œ"""
        with self.path[0].open('w') as fd:
            a = len(answer_1) == 580
            b = len(answer_2) == 133
            json.dump(int(a and b), fd)


# ëª¨ë“ˆ ë ˆë²¨ ë³€ìˆ˜ë“¤
__all__ = [
    # ë„ë©”ì¸ ì ì‘ ê´€ë ¨
    'DomainAdapter',
    'CORAL', 
    'MMD',
    'domain_adaptation_score',
    'compare_domain_adapters',
    'load_sample_domain_data',
    'plot_domain_comparison',
    
    # SKADA ì‹œìŠ¤í…œ ê´€ë ¨
    'SKADA',
    'Singleton',
    'submit',
    'type_checker',
    
    # API ì œì¶œ ì‹œìŠ¤í…œ
    'SubmissionError',
    'serialize_data',
    'submit_to_api',
    'submit_with_fallback',
    
    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
    'disp_confusion_matrix',
    'draw_histograms',
    'train_and_evaluate_xgb_model',
    'check_Q2_2_answer',
]

Index: opencv_mcp_server/computer_vision.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nOpenCV MCP Server - Computer Vision\n\nThis module provides high-level computer vision and object detection tools using OpenCV.\nIt includes functionality for feature detection and matching, face detection,\nand object detection using pre-trained models.\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport os\nimport logging\nfrom typing import Optional, List, Dict, Any, Tuple, Union\n\n# Import utility functions from utils\nfrom .utils import get_image_info, save_and_display, get_timestamp\n\nlogger = logging.getLogger(\"opencv-mcp-server.computer_vision\")\n\n# Tool implementations\ndef detect_features_tool(\n    image_path: str, \n    method: str = \"sift\", \n    max_features: int = 500,\n    draw: bool = True,\n    color: Tuple[int, int, int] = (0, 255, 0)\n) -> Dict[str, Any]:\n    \"\"\"\n    Detect features in an image\n    \n    Args:\n        image_path: Path to the image file\n        method: Feature detection method ('sift', 'orb', 'brisk', 'akaze')\n        max_features: Maximum number of features to detect\n        draw: Whether to draw keypoints on the image\n        color: Color for drawing keypoints (BGR format)\n        \n    Returns:\n        Dict: Image with keypoints and feature information\n    \"\"\"\n    try:\n        # Read image from path\n        img = cv2.imread(image_path)\n        if img is None:\n            raise ValueError(f\"Failed to read image from path: {image_path}\")\n        \n        # Convert to grayscale if needed\n        if len(img.shape) == 3:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = img\n        \n        # Initialize the feature detector\n        if method.lower() == 'sift':\n            detector = cv2.SIFT_create(nfeatures=max_features)\n        elif method.lower() == 'orb':\n            detector = cv2.ORB_create(nfeatures=max_features)\n        elif method.lower() == 'brisk':\n            detector = cv2.BRISK_create()\n        elif method.lower() == 'akaze':\n            detector = cv2.AKAZE_create()\n        else:\n            raise ValueError(f\"Unsupported feature detection method: {method}\")\n        \n        # Detect keypoints and compute descriptors\n        keypoints, descriptors = detector.detectAndCompute(gray, None)\n        \n        # Draw keypoints if requested\n        if draw:\n            img_keypoints = cv2.drawKeypoints(\n                img, \n                keypoints, \n                None, \n                color=color, \n                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n            )\n        else:\n            img_keypoints = img.copy()\n        \n        # Prepare keypoint data\n        keypoint_data = []\n        for i, kp in enumerate(keypoints[:50]):  # Limit to 50 for response size\n            keypoint_data.append({\n                \"index\": i,\n                \"x\": float(kp.pt[0]),\n                \"y\": float(kp.pt[1]),\n                \"size\": float(kp.size),\n                \"angle\": float(kp.angle),\n                \"response\": float(kp.response),\n                \"octave\": int(kp.octave)\n            })\n        \n        # Save and display\n        new_path = save_and_display(img_keypoints, image_path, f\"features_{method}\")\n        \n        return {\n            \"keypoint_count\": len(keypoints),\n            \"keypoints\": keypoint_data,\n            \"method\": method,\n            \"info\": get_image_info(img_keypoints),\n            \"path\": new_path,\n            \"output_path\": new_path  # Return path for chaining operations\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error detecting features: {str(e)}\")\n        raise ValueError(f\"Failed to detect features: {str(e)}\")\n\ndef match_features_tool(\n    image1_path: str, \n    image2_path: str, \n    method: str = \"sift\",\n    matcher: str = \"bf\",\n    max_features: int = 500,\n    ratio_thresh: float = 0.75,\n    draw: bool = True\n) -> Dict[str, Any]:\n    \"\"\"\n    Match features between two images\n    \n    Args:\n        image1_path: Path to the first image file\n        image2_path: Path to the second image file\n        method: Feature detection method ('sift', 'orb', 'brisk', 'akaze')\n        matcher: Matcher type ('bf' for BFMatcher, 'flann' for FlannBasedMatcher)\n        max_features: Maximum number of features to detect\n        ratio_thresh: Ratio threshold for Lowe's ratio test\n        draw: Whether to draw matches\n        \n    Returns:\n        Dict: Image with matches and match information\n    \"\"\"\n    try:\n        # Read images from paths\n        img1 = cv2.imread(image1_path)\n        if img1 is None:\n            raise ValueError(f\"Failed to read image from path: {image1_path}\")\n            \n        img2 = cv2.imread(image2_path)\n        if img2 is None:\n            raise ValueError(f\"Failed to read image from path: {image2_path}\")\n        \n        # Convert to grayscale if needed\n        if len(img1.shape) == 3:\n            gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n        else:\n            gray1 = img1\n            \n        if len(img2.shape) == 3:\n            gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n        else:\n            gray2 = img2\n        \n        # Initialize the feature detector\n        if method.lower() == 'sift':\n            detector = cv2.SIFT_create(nfeatures=max_features)\n            is_binary = False\n        elif method.lower() == 'orb':\n            detector = cv2.ORB_create(nfeatures=max_features)\n            is_binary = True\n        elif method.lower() == 'brisk':\n            detector = cv2.BRISK_create()\n            is_binary = True\n        elif method.lower() == 'akaze':\n            detector = cv2.AKAZE_create()\n            is_binary = True\n        else:\n            raise ValueError(f\"Unsupported feature detection method: {method}\")\n        \n        # Detect keypoints and compute descriptors\n        keypoints1, descriptors1 = detector.detectAndCompute(gray1, None)\n        keypoints2, descriptors2 = detector.detectAndCompute(gray2, None)\n        \n        # Check if descriptors were found\n        if descriptors1 is None or descriptors2 is None or len(keypoints1) == 0 or len(keypoints2) == 0:\n            return {\n                \"match_count\": 0,\n                \"error\": \"No keypoints or descriptors found in one or both images\",\n                \"keypoint_count1\": len(keypoints1) if keypoints1 else 0,\n                \"keypoint_count2\": len(keypoints2) if keypoints2 else 0,\n            }\n        \n        # Create feature matcher\n        if matcher.lower() == 'bf':\n            if is_binary:\n                norm_type = cv2.NORM_HAMMING\n            else:\n                norm_type = cv2.NORM_L2\n                \n            match_obj = cv2.BFMatcher(norm_type, crossCheck=False)\n            \n        elif matcher.lower() == 'flann':\n            if is_binary:\n                # Special FLANN params for binary descriptors\n                flann_params = dict(\n                    algorithm=6,  # FLANN_INDEX_LSH\n                    table_number=6,\n                    key_size=12,\n                    multi_probe_level=1\n                )\n            else:\n                flann_params = dict(\n                    algorithm=1,  # FLANN_INDEX_KDTREE\n                    trees=5\n                )\n                \n            match_obj = cv2.FlannBasedMatcher(flann_params, {})\n        else:\n            raise ValueError(f\"Unsupported matcher type: {matcher}\")\n        \n        # Apply ratio test if not using crossCheck\n        matches = match_obj.knnMatch(descriptors1, descriptors2, k=2)\n        \n        # Apply ratio test\n        good_matches = []\n        for m, n in matches:\n            if m.distance < ratio_thresh * n.distance:\n                good_matches.append(m)\n        \n        # Collect match data\n        match_data = []\n        for i, m in enumerate(good_matches[:50]):  # Limit to 50 for response size\n            match_data.append({\n                \"index\": i,\n                \"queryIdx\": m.queryIdx,\n                \"trainIdx\": m.trainIdx,\n                \"distance\": float(m.distance),\n                \"image1_point\": (float(keypoints1[m.queryIdx].pt[0]), float(keypoints1[m.queryIdx].pt[1])),\n                \"image2_point\": (float(keypoints2[m.trainIdx].pt[0]), float(keypoints2[m.trainIdx].pt[1]))\n            })\n        \n        # Draw matches if requested\n        if draw and good_matches:\n            draw_params = dict(\n                matchColor=(0, 255, 0),\n                singlePointColor=(255, 0, 0),\n                flags=cv2.DrawMatchesFlags_DEFAULT\n            )\n            \n            # Only draw a limited number of matches\n            draw_matches = good_matches[:50] if len(good_matches) > 50 else good_matches\n            \n            img_matches = cv2.drawMatches(\n                img1, keypoints1, \n                img2, keypoints2, \n                draw_matches, None, \n                **draw_params\n            )\n        else:\n            # Create a side-by-side image\n            h1, w1 = img1.shape[:2]\n            h2, w2 = img2.shape[:2]\n            h = max(h1, h2)\n            img_matches = np.zeros((h, w1 + w2, 3), dtype=np.uint8)\n            img_matches[:h1, :w1] = img1 if len(img1.shape) == 3 else cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n            img_matches[:h2, w1:w1+w2] = img2 if len(img2.shape) == 3 else cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n        \n        # Generate a unique output path based on both input images\n        base_name1 = os.path.basename(image1_path)\n        base_name2 = os.path.basename(image2_path)\n        name_parts1 = os.path.splitext(base_name1)\n        directory = os.path.dirname(image1_path) or '.'\n        \n        # Save and display\n        result_path = save_and_display(img_matches, image1_path, f\"matches_{method}_{name_parts1[0]}_to_{os.path.splitext(base_name2)[0]}\")\n        \n        return {\n            \"match_count\": len(good_matches),\n            \"matches\": match_data,\n            \"keypoint_count1\": len(keypoints1),\n            \"keypoint_count2\": len(keypoints2),\n            \"match_parameters\": {\n                \"method\": method,\n                \"matcher\": matcher,\n                \"ratio_thresh\": ratio_thresh\n            },\n            \"info\": get_image_info(img_matches),\n            \"path\": result_path,\n            \"output_path\": result_path  # Return path for chaining operations\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error matching features: {str(e)}\")\n        raise ValueError(f\"Failed to match features: {str(e)}\")\n\ndef detect_faces_tool(\n    image_path: str, \n    method: str = \"haar\", \n    scale_factor: float = 1.3,\n    min_neighbors: int = 5,\n    min_size: Tuple[int, int] = (30, 30),\n    confidence_threshold: float = 0.5,\n    draw: bool = True,\n    color: Tuple[int, int, int] = (0, 255, 0),\n    thickness: int = 2\n) -> Dict[str, Any]:\n    \"\"\"\n    Detect faces in an image\n    \n    Args:\n        image_path: Path to the image file\n        method: Face detection method ('haar', 'dnn')\n        scale_factor: Scale factor for cascade classifier\n        min_neighbors: Minimum neighbors for cascade classifier\n        min_size: Minimum face size to detect\n        confidence_threshold: Confidence threshold for DNN method\n        draw: Whether to draw rectangles around faces\n        color: Color for drawing rectangles (BGR format)\n        thickness: Thickness of the rectangle\n        \n    Returns:\n        Dict: Image with faces and face information\n    \"\"\"\n    try:\n        # Read image from path\n        img = cv2.imread(image_path)\n        if img is None:\n            raise ValueError(f\"Failed to read image from path: {image_path}\")\n        \n        # Make a copy for drawing\n        img_copy = img.copy()\n        \n        # Get face cascade directory\n        cascade_path = cv2.data.haarcascades\n        haar_xml = os.path.join(cascade_path, 'haarcascade_frontalface_default.xml')\n        \n        # Check if cascade file exists\n        if not os.path.exists(haar_xml):\n            download_instructions = (\n                f\"Haar cascade file not found: {haar_xml}\\n\"\n                \"To download the files:\\n\"\n                \"1. Download from: https://github.com/opencv/opencv/tree/master/data/haarcascades\\n\"\n                \"2. Save to the OpenCV data directory at: {}\\n\"\n                \"   OR set the CV_HAAR_CASCADE_DIR environment variable to your preferred location\\n\"\n                \"3. Restart the application\".format(cascade_path)\n            )\n            logger.error(download_instructions)\n            raise FileNotFoundError(download_instructions)\n        \n        faces = []\n        \n        if method.lower() == 'haar':\n            # Load the face detector\n            face_cascade = cv2.CascadeClassifier(haar_xml)\n            \n            # Convert to grayscale\n            if len(img.shape) == 3:\n                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            else:\n                gray = img\n            \n            # Detect faces\n            faces_rect = face_cascade.detectMultiScale(\n                gray,\n                scaleFactor=scale_factor,\n                minNeighbors=min_neighbors,\n                minSize=min_size\n            )\n            \n            # Prepare face data\n            for i, (x, y, w, h) in enumerate(faces_rect):\n                faces.append({\n                    \"index\": i,\n                    \"x\": int(x),\n                    \"y\": int(y),\n                    \"width\": int(w),\n                    \"height\": int(h),\n                    \"confidence\": None  # Haar doesn't provide confidence\n                })\n                \n                # Draw rectangle if requested\n                if draw:\n                    cv2.rectangle(img_copy, (x, y), (x+w, y+h), color, thickness)\n            \n        elif method.lower() == 'dnn':\n            # Paths to the model files\n            # Note: You would need to download these files for actual use\n            model_dir = os.environ.get(\"OPENCV_DNN_MODELS_DIR\", \"models\")\n            if not os.path.exists(model_dir):\n                os.makedirs(model_dir, exist_ok=True)\n                \n            prototxt_path = os.path.join(model_dir, \"deploy.prototxt\")\n            model_path = os.path.join(model_dir, \"res10_300x300_ssd_iter_140000.caffemodel\")\n            \n            # Check if model files exist\n            model_files_exist = os.path.exists(prototxt_path) and os.path.exists(model_path)\n            \n            if not model_files_exist:\n                download_instructions = (\n                    f\"DNN model files not found at {model_dir}.\\n\"\n                    \"To download the required files:\\n\"\n                    \"1. Download the model files from: https://github.com/opencv/opencv_3rdparty/tree/dnn_samples_face_detector_20170830\\n\"\n                    \"   - deploy.prototxt\\n\"\n                    \"   - res10_300x300_ssd_iter_140000.caffemodel\\n\"\n                    \"2. Save them to: {}\\n\"\n                    \"   OR set the OPENCV_DNN_MODELS_DIR environment variable to your preferred directory\\n\"\n                    \"3. Restart the application\".format(model_dir)\n                )\n                logger.warning(download_instructions)\n                return {\n                    \"error\": \"DNN model files not found\",\n                    \"download_instructions\": download_instructions,\n                    \"face_count\": 0,\n                    \"faces\": []\n                }\n            \n            # Try to load the model\n            try:\n                face_net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n            except Exception as e:\n                logger.error(f\"Error loading DNN model: {str(e)}\")\n                return {\n                    \"error\": f\"Failed to load DNN model: {str(e)}\",\n                    \"info\": {\n                        \"prototxt_path\": prototxt_path,\n                        \"model_path\": model_path\n                    }\n                }\n            \n            # Get image dimensions\n            (h, w) = img.shape[:2]\n            \n            # Create a blob from the image\n            blob = cv2.dnn.blobFromImage(\n                cv2.resize(img, (300, 300)), \n                1.0, \n                (300, 300), \n                (104.0, 177.0, 123.0)\n            )\n            \n            # Pass the blob through the network\n            face_net.setInput(blob)\n            detections = face_net.forward()\n            \n            # Process detections\n            for i in range(detections.shape[2]):\n                confidence = detections[0, 0, i, 2]\n                \n                # Filter by confidence threshold\n                if confidence > confidence_threshold:\n                    # Get coordinates\n                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n                    (startX, startY, endX, endY) = box.astype(\"int\")\n                    \n                    # Ensure coordinates are within image bounds\n                    startX = max(0, startX)\n                    startY = max(0, startY)\n                    endX = min(w, endX)\n                    endY = min(h, endY)\n                    \n                    width = endX - startX\n                    height = endY - startY\n                    \n                    faces.append({\n                        \"index\": i,\n                        \"x\": int(startX),\n                        \"y\": int(startY),\n                        \"width\": int(width),\n                        \"height\": int(height),\n                        \"confidence\": float(confidence)\n                    })\n                    \n                    # Draw rectangle if requested\n                    if draw:\n                        cv2.rectangle(img_copy, (startX, startY), (endX, endY), color, thickness)\n                        # Draw confidence text\n                        text = f\"{confidence:.2f}\"\n                        y_text = startY - 10 if startY - 10 > 10 else startY + 10\n                        cv2.putText(img_copy, text, (startX, y_text),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n        \n        else:\n            raise ValueError(f\"Unsupported face detection method: {method}\")\n        \n        # Save and display\n        result_path = save_and_display(img_copy, image_path, f\"faces_{method}\")\n        \n        return {\n            \"face_count\": len(faces),\n            \"faces\": faces,\n            \"method\": method,\n            \"info\": get_image_info(img_copy),\n            \"path\": result_path,\n            \"output_path\": result_path  # Return path for chaining operations\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error detecting faces: {str(e)}\")\n        raise ValueError(f\"Failed to detect faces: {str(e)}\")\n\ndef detect_objects_tool(\n    image_path: str, \n    model_path: Optional[str] = None,\n    config_path: Optional[str] = None,\n    classes_path: Optional[str] = None,\n    confidence_threshold: float = 0.5,\n    nms_threshold: float = 0.4,\n    width: int = 416,\n    height: int = 416,\n    draw: bool = True,\n    color: Tuple[int, int, int] = (0, 255, 0),\n    thickness: int = 2\n) -> Dict[str, Any]:\n    \"\"\"\n    Detect objects using pre-trained DNN models\n    \n    Args:\n        image_path: Path to the image file\n        model_path: Path to model weights file (e.g., yolo.weights)\n        config_path: Path to model configuration file (e.g., yolo.cfg)\n        classes_path: Path to text file containing class names\n        confidence_threshold: Minimum confidence threshold\n        nms_threshold: Non-maximum suppression threshold\n        width: Network input width\n        height: Network input height\n        draw: Whether to draw boxes around objects\n        color: Color for drawing boxes (BGR format)\n        thickness: Thickness of the box\n        \n    Returns:\n        Dict: Image with objects and object information\n    \"\"\"\n    try:\n        # Read image from path\n        img = cv2.imread(image_path)\n        if img is None:\n            raise ValueError(f\"Failed to read image from path: {image_path}\")\n        \n        # Make a copy for drawing\n        img_copy = img.copy()\n        \n        # Get image dimensions\n        (orig_h, orig_w) = img.shape[:2]\n        \n        # Set default model directory\n        model_dir = os.environ.get(\"OPENCV_DNN_MODELS_DIR\", \"models\")\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir, exist_ok=True)\n        \n        # Use default YOLO model if paths not provided\n        if model_path is None:\n            model_path = os.path.join(model_dir, \"yolov3.weights\")\n        \n        if config_path is None:\n            config_path = os.path.join(model_dir, \"yolov3.cfg\")\n            \n        if classes_path is None:\n            classes_path = os.path.join(model_dir, \"coco.names\")\n        \n        # Check if model files exist\n        model_files_exist = os.path.exists(model_path) and os.path.exists(config_path)\n        \n        if not model_files_exist:\n            download_instructions = (\n                f\"YOLO model files not found at {model_dir}.\\n\"\n                \"To download the required files:\\n\"\n                \"1. Download YOLOv3 weights file (237MB) from: https://pjreddie.com/media/files/yolov3.weights\\n\"\n                \"2. Download YOLOv3 config file from: https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\\n\"\n                \"3. Download COCO class names file from: https://github.com/pjreddie/darknet/blob/master/data/coco.names\\n\"\n                \"4. Save all files to: {}\\n\"\n                \"   OR set the OPENCV_DNN_MODELS_DIR environment variable to your preferred directory\\n\"\n                \"5. Restart the application\".format(model_dir)\n            )\n            logger.warning(download_instructions)\n            return {\n                \"error\": \"YOLO model files not found\",\n                \"download_instructions\": download_instructions,\n                \"info\": {\n                    \"model_path\": model_path,\n                    \"config_path\": config_path,\n                    \"classes_path\": classes_path\n                }\n            }\n        \n        # Load class names\n        try:\n            with open(classes_path, 'r') as f:\n                classes = [line.strip() for line in f.readlines()]\n        except Exception as e:\n            logger.error(f\"Error loading class names: {str(e)}\")\n            # Provide a small subset of COCO classes as fallback\n            classes = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\"]\n        \n        # Try to load the model\n        try:\n            net = cv2.dnn.readNetFromDarknet(config_path, model_path)\n        except Exception as e:\n            logger.error(f\"Error loading DNN model: {str(e)}\")\n            return {\n                \"error\": f\"Failed to load DNN model: {str(e)}\",\n                \"info\": {\n                    \"model_path\": model_path,\n                    \"config_path\": config_path\n                }\n            }\n        \n        # Get output layer names\n        layer_names = net.getLayerNames()\n        \n        # OpenCV 4.5.4+ has a different indexing system\n        try:\n            # For newer OpenCV versions\n            output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n        except:\n            # For older OpenCV versions\n            output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n        \n        # Create a blob from the image\n        blob = cv2.dnn.blobFromImage(img, 1/255.0, (width, height), swapRB=True, crop=False)\n        \n        # Set the input to the network\n        net.setInput(blob)\n        \n        # Run forward pass\n        layer_outputs = net.forward(output_layers)\n        \n        # Initialize lists for detected objects\n        boxes = []\n        confidences = []\n        class_ids = []\n        \n        # Process each output layer\n        for output in layer_outputs:\n            # Process each detection\n            for detection in output:\n                # The first 4 elements are bounding box coordinates\n                # The rest are class probabilities\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n                \n                # Filter by confidence threshold\n                if confidence > confidence_threshold:\n                    # Get bounding box coordinates\n                    # YOLO returns center (x, y) and width, height\n                    center_x = int(detection[0] * orig_w)\n                    center_y = int(detection[1] * orig_h)\n                    width_box = int(detection[2] * orig_w)\n                    height_box = int(detection[3] * orig_h)\n                    \n                    # Calculate top-left corner\n                    x = int(center_x - width_box / 2)\n                    y = int(center_y - height_box / 2)\n                    \n                    # Add to lists\n                    boxes.append([x, y, width_box, height_box])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n        \n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n        \n        # Prepare object data\n        objects = []\n        \n        # Check if indices is not empty\n        if len(indices) > 0:\n            # Ensure indices is properly formatted (OpenCV 4.5.4+ compatibility)\n            if isinstance(indices, np.ndarray):\n                indices = indices.flatten()\n            \n            # Process each selected box\n            for i in indices:\n                # Extract bounding box coordinates\n                # In some versions this may be i[0] instead of just i\n                idx = i if isinstance(i, int) else i[0]\n                \n                box = boxes[idx]\n                x, y, w, h = box\n                \n                # Get class name\n                class_id = class_ids[idx]\n                class_name = classes[class_id] if class_id < len(classes) else f\"Class {class_id}\"\n                \n                # Prepare object data\n                objects.append({\n                    \"index\": idx,\n                    \"class_id\": int(class_id),\n                    \"class_name\": class_name,\n                    \"confidence\": float(confidences[idx]),\n                    \"x\": int(x),\n                    \"y\": int(y),\n                    \"width\": int(w),\n                    \"height\": int(h)\n                })\n                \n                # Draw rectangle if requested\n                if draw:\n                    # Ensure coordinates are within image bounds\n                    x = max(0, x)\n                    y = max(0, y)\n                    x_end = min(orig_w, x + w)\n                    y_end = min(orig_h, y + h)\n                    \n                    # Draw rectangle\n                    cv2.rectangle(img_copy, (x, y), (x_end, y_end), color, thickness)\n                    \n                    # Add label\n                    text = f\"{class_name}: {confidences[idx]:.2f}\"\n                    y_text = y - 10 if y - 10 > 10 else y + 10\n                    cv2.putText(img_copy, text, (x, y_text),\n                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n        \n        # Save and display\n        result_path = save_and_display(img_copy, image_path, \"objects_detected\")\n        \n        return {\n            \"object_count\": len(objects),\n            \"objects\": objects,\n            \"model_info\": {\n                \"model_path\": model_path,\n                \"config_path\": config_path,\n                \"classes_count\": len(classes),\n                \"input_size\": (width, height)\n            },\n            \"info\": get_image_info(img_copy),\n            \"path\": result_path,\n            \"output_path\": result_path  # Return path for chaining operations\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error detecting objects: {str(e)}\")\n        raise ValueError(f\"Failed to detect objects: {str(e)}\")\n\ndef register_tools(mcp):\n    \"\"\"\n    Register all computer vision tools with the MCP server\n    \n    Args:\n        mcp: The MCP server instance\n    \"\"\"\n    # Register tool implementations\n    mcp.add_tool(detect_features_tool)\n    mcp.add_tool(match_features_tool)\n    mcp.add_tool(detect_faces_tool)\n    mcp.add_tool(detect_objects_tool)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/opencv_mcp_server/computer_vision.py b/opencv_mcp_server/computer_vision.py
--- a/opencv_mcp_server/computer_vision.py	(revision 720a18e021be99302e7470fbc2719b8890b6b810)
+++ b/opencv_mcp_server/computer_vision.py	(date 1756889617079)
@@ -597,13 +597,16 @@
         # Get output layer names
         layer_names = net.getLayerNames()
         
-        # OpenCV 4.5.4+ has a different indexing system
-        try:
-            # For newer OpenCV versions
-            output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]
-        except:
-            # For older OpenCV versions
-            output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]
+        # Handle different OpenCV versions for getUnconnectedOutLayers()
+        unconnected_out_layers = net.getUnconnectedOutLayers()
+
+        # Check the shape to determine how to process the indices
+        if len(unconnected_out_layers.shape) == 1:
+            # Newer OpenCV versions (4.5.4+) return 1D array
+            output_layers = [layer_names[i - 1] for i in unconnected_out_layers]
+        else:
+            # Older OpenCV versions return 2D array
+            output_layers = [layer_names[i[0] - 1] for i in unconnected_out_layers]
         
         # Create a blob from the image
         blob = cv2.dnn.blobFromImage(img, 1/255.0, (width, height), swapRB=True, crop=False)
@@ -653,17 +656,28 @@
         # Prepare object data
         objects = []
         
-        # Check if indices is not empty
+        # Check if indices is not empty and handle different OpenCV versions
         if len(indices) > 0:
-            # Ensure indices is properly formatted (OpenCV 4.5.4+ compatibility)
+            # Handle different OpenCV versions for NMSBoxes return format
             if isinstance(indices, np.ndarray):
-                indices = indices.flatten()
-            
+                # For newer OpenCV versions, flatten if needed
+                if indices.ndim > 1:
+                    indices = indices.flatten()
+            elif isinstance(indices, tuple):
+                # Some versions might return a tuple
+                indices = list(indices)
+
             # Process each selected box
             for i in indices:
-                # Extract bounding box coordinates
-                # In some versions this may be i[0] instead of just i
-                idx = i if isinstance(i, int) else i[0]
+                # Extract the actual index value, handling both int and array formats
+                if isinstance(i, (np.ndarray, list, tuple)):
+                    idx = int(i[0]) if len(i) > 0 else 0
+                else:
+                    idx = int(i)
+
+                # Ensure idx is within bounds
+                if idx >= len(boxes) or idx >= len(confidences) or idx >= len(class_ids):
+                    continue
                 
                 box = boxes[idx]
                 x, y, w, h = box

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef71c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "import faster_whisper\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "from ctc_forced_aligner import (\n",
    "    generate_emissions,\n",
    "    get_alignments,\n",
    "    get_spans,\n",
    "    load_alignment_model,\n",
    "    postprocess_results,\n",
    "    preprocess_text,\n",
    ")\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
    "\n",
    "from utils import (\n",
    "    create_config,\n",
    "    process_language_arg,\n",
    "    get_realigned_ws_mapping_with_punctuation,\n",
    "    get_sentences_speaker_mapping,\n",
    "    get_speaker_aware_transcript,\n",
    "    get_words_speaker_mapping,\n",
    "    langs_to_iso,\n",
    "    process_language_arg,\n",
    "    punct_model_langs,\n",
    "    write_srt,\n",
    "    find_numeral_symbol_tokens,\n",
    ")\n",
    "\n",
    "# Notebook params (replicates argparse inputs)\n",
    "AUDIO_PATH = \"../data/audio/call_1.mp3\"\n",
    "STEMMING = True                  \n",
    "SUPPRESS_NUMERALS = False      \n",
    "MODEL_NAME = \"medium\"        \n",
    "BATCH_SIZE = 8                   \n",
    "LANGUAGE = None                 \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEMP_PATH = \"temp_outputs\"\n",
    "\n",
    "# precision map\n",
    "mtypes = {\"cpu\": \"int8\", \"cuda\": \"float16\"}\n",
    "\n",
    "# normalize/validate language same as script\n",
    "LANGUAGE = process_language_arg(LANGUAGE, MODEL_NAME)\n",
    "\n",
    "print(f\"device={DEVICE}  model={MODEL_NAME}  batch_size={BATCH_SIZE}  language={LANGUAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"sk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STEMMING:\n",
    "    # Isolate vocals from the rest of the audio\n",
    "    return_code = os.system(\n",
    "        f'python -m demucs.separate -n htdemucs --two-stems=vocals \"{AUDIO_PATH}\" -o temp_outputs'\n",
    "    )\n",
    "\n",
    "    if return_code != 0:\n",
    "        logging.warning(\n",
    "            \"Source splitting failed, using original audio file. \"\n",
    "            \"Use stemming=False to skip this step.\"\n",
    "        )\n",
    "        vocal_target = AUDIO_PATH\n",
    "    else:\n",
    "        vocal_target = os.path.join(\n",
    "            \"temp_outputs\",\n",
    "            \"htdemucs\",\n",
    "            os.path.splitext(os.path.basename(AUDIO_PATH))[0],\n",
    "            \"vocals.wav\",\n",
    "        )\n",
    "else:\n",
    "    vocal_target = AUDIO_PATH\n",
    "\n",
    "print(\"vocal_target:\", vocal_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_model = faster_whisper.WhisperModel(\n",
    "    MODEL_NAME, device=DEVICE, compute_type=mtypes[DEVICE]\n",
    ")\n",
    "whisper_pipeline = faster_whisper.BatchedInferencePipeline(whisper_model)\n",
    "\n",
    "audio_waveform = faster_whisper.decode_audio(vocal_target)\n",
    "\n",
    "suppress_tokens = (\n",
    "    find_numeral_symbol_tokens(whisper_model.hf_tokenizer)\n",
    "    if SUPPRESS_NUMERALS\n",
    "    else [-1]\n",
    ")\n",
    "\n",
    "if BATCH_SIZE > 0:\n",
    "    transcript_segments, info = whisper_pipeline.transcribe(\n",
    "        audio_waveform,\n",
    "        language,\n",
    "        suppress_tokens=suppress_tokens,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "else:\n",
    "    transcript_segments, info = whisper_model.transcribe(\n",
    "        audio_waveform,\n",
    "        language,\n",
    "        suppress_tokens=suppress_tokens,\n",
    "        vad_filter=True,\n",
    "    )\n",
    "\n",
    "full_transcript = \"\".join(segment.text for segment in transcript_segments)\n",
    "\n",
    "# clear gpu vram (same as script)\n",
    "del whisper_model, whisper_pipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Detected language:\", info.language)\n",
    "print(\"Transcript preview:\", full_transcript[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "alignment_model, alignment_tokenizer = load_alignment_model(\n",
    "    DEVICE, dtype=dtype\n",
    ")\n",
    "\n",
    "# safer device resolution than .device on Module\n",
    "align_dev = getattr(alignment_model, \"device\", next(alignment_model.parameters()).device)\n",
    "\n",
    "emissions, stride = generate_emissions(\n",
    "    alignment_model,\n",
    "    torch.from_numpy(audio_waveform).to(dtype).to(align_dev),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "del alignment_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokens_starred, text_starred = preprocess_text(\n",
    "    full_transcript,\n",
    "    romanize=True,\n",
    "    language=langs_to_iso[info.language],\n",
    ")\n",
    "\n",
    "segments, scores, blank_token = get_alignments(\n",
    "    emissions,\n",
    "    tokens_starred,\n",
    "    alignment_tokenizer,\n",
    ")\n",
    "\n",
    "spans = get_spans(tokens_starred, segments, blank_token)\n",
    "\n",
    "word_timestamps = postprocess_results(text_starred, spans, stride, scores)\n",
    "print(\"Aligned words:\", len(word_timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TEMP_PATH, exist_ok=True)\n",
    "\n",
    "mono_wav = os.path.join(TEMP_PATH, \"mono_file.wav\")\n",
    "if not os.path.exists(mono_wav):\n",
    "    torchaudio.save(mono_wav, torch.from_numpy(audio_waveform).unsqueeze(0).float(), 16000)\n",
    "\n",
    "cfg = create_config(TEMP_PATH)\n",
    "msdd = NeuralDiarizer(cfg=cfg).to(DEVICE)\n",
    "msdd.diarize()\n",
    "del msdd; torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38708a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"word_timestamps\" in globals(), \"word_timestamps not defined (run alignment cell).\"\n",
    "assert \"info\" in globals(), \"info (from transcription) not defined.\"\n",
    "assert \"audio_waveform\" in globals(), \"audio_waveform not defined.\"\n",
    "assert \"audio_path\" in globals(), \"audio_path not set.\"\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
    "mono_wav = os.path.join(temp_path, \"mono_file.wav\")\n",
    "assert os.path.exists(mono_wav), f\"Expected mono wav at {mono_wav}\"\n",
    "\n",
    "# Parse RTTM to speaker_ts\n",
    "rttm_path = os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\")\n",
    "if not os.path.exists(rttm_path):\n",
    "    raise FileNotFoundError(f\"RTTM not found at {rttm_path}. Re-run diarization and check create_config(out_dir).\")\n",
    "\n",
    "speaker_ts = []\n",
    "with open(rttm_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip() or line.startswith(\"#\"):\n",
    "            continue\n",
    "        # Keep indices exactly like your script:\n",
    "        line_list = line.split(\" \")\n",
    "        s = int(float(line_list[5]) * 1000)\n",
    "        e = s + int(float(line_list[8]) * 1000)\n",
    "        speaker_id = int(line_list[11].split(\"_\")[-1])\n",
    "        if e > s:\n",
    "            speaker_ts.append([s, e, speaker_id])\n",
    "\n",
    "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")\n",
    "\n",
    "def chunked(seq, n):\n",
    "    it = iter(seq)\n",
    "    while True:\n",
    "        chunk = list(islice(it, n))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "if info.language in punct_model_langs:\n",
    "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
    "    words_list = [x[\"word\"] for x in wsm]\n",
    "\n",
    "    labeled_words = []\n",
    "    for chunk in chunked(words_list, 220):\n",
    "        labeled_words.extend(punct_model.predict(chunk))  # returns (token, punct, score)\n",
    "\n",
    "    ending_puncts = \".?!\"\n",
    "    model_puncts = \".,;:!?\"\n",
    "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
    "\n",
    "    for word_dict, labeled in zip(wsm, labeled_words):\n",
    "        word = word_dict[\"word\"]\n",
    "        punct = labeled[1] if isinstance(labeled, (list, tuple)) and len(labeled) > 1 else \"\"\n",
    "        if (\n",
    "            word\n",
    "            and punct in ending_puncts\n",
    "            and (word[-1] not in model_puncts or is_acronym(word))\n",
    "        ):\n",
    "            word += punct\n",
    "            if word.endswith(\"..\"):\n",
    "                word = word.rstrip(\".\")\n",
    "            word_dict[\"word\"] = word\n",
    "else:\n",
    "    logging.warning(\n",
    "        f\"Punctuation restoration is not available for {info.language} language. Using the original punctuation.\"\n",
    "    )\n",
    "\n",
    "wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
    "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)\n",
    "\n",
    "base = os.path.splitext(AUDIO_PATH)[0]\n",
    "txt_out = Path(f\"{base}.txt\")\n",
    "srt_out = Path(f\"{base}.srt\")\n",
    "\n",
    "with open(txt_out, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    get_speaker_aware_transcript(ssm, f)\n",
    "\n",
    "with open(srt_out, \"w\", encoding=\"utf-8-sig\") as srt:\n",
    "    write_srt(ssm, srt)\n",
    "\n",
    "print(f\"Done\\n Transcript: {txt_out}\\n - Subtitles:  {srt_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c036d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miya-speechless",
   "language": "python",
   "name": "miya-speechless"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import pickle
import time
import asyncio
from typing import Optional
import redis.asyncio as aioredis
import traceback
import os

from crawlo import Request
from crawlo.utils.log import get_logger
from crawlo.utils.request_serializer import RequestSerializer


logger = get_logger(__name__)


class RedisPriorityQueue:
    """
    åŸºäº Redis çš„åˆ†å¸ƒå¼å¼‚æ­¥ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆç”Ÿäº§çº§ä¼˜åŒ–ç‰ˆï¼‰
    """

    def __init__(
            self,
            redis_url: str = None,
            queue_name: str = "crawlo:requests",
            processing_queue: str = "crawlo:processing",
            failed_queue: str = "crawlo:failed",
            max_retries: int = 3,
            timeout: int = 300,  # ä»»åŠ¡å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_connections: int = 10,  # è¿æ¥æ± å¤§å°
    ):
        # å¦‚æœæ²¡æœ‰æä¾› redis_urlï¼Œåˆ™ä»ç¯å¢ƒå˜é‡æ„é€ 
        if redis_url is None:
            redis_host = os.getenv('REDIS_HOST', 'localhost')
            redis_port = os.getenv('REDIS_PORT', '6379')
            redis_db = os.getenv('REDIS_DB', '0')
            redis_password = os.getenv('REDIS_PASSWORD', '')
            
            if redis_password:
                redis_url = f"redis://:{redis_password}@{redis_host}:{redis_port}/{redis_db}"
            else:
                redis_url = f"redis://{redis_host}:{redis_port}/{redis_db}"
        
        self.redis_url = redis_url
        self.queue_name = queue_name
        self.processing_queue = processing_queue
        self.failed_queue = failed_queue
        self.max_retries = max_retries
        self.timeout = timeout
        self.max_connections = max_connections
        self._redis = None
        self._lock = asyncio.Lock()  # ç”¨äºè¿æ¥åˆå§‹åŒ–çš„é”
        self.request_serializer = RequestSerializer()  # å¤„ç†åºåˆ—åŒ–

    async def connect(self, max_retries=3, delay=1):
        """å¼‚æ­¥è¿æ¥ Redisï¼Œæ”¯æŒé‡è¯•"""
        async with self._lock:
            if self._redis is not None:
                return self._redis

            for attempt in range(max_retries):
                try:
                    self._redis = await aioredis.from_url(
                        self.redis_url,
                        decode_responses=False,  # pickle éœ€è¦ bytes
                        max_connections=self.max_connections,
                        socket_connect_timeout=5,
                        socket_timeout=30,
                    )
                    # æµ‹è¯•è¿æ¥
                    await self._redis.ping()
                    logger.info("âœ… Redis è¿æ¥æˆåŠŸ")
                    return self._redis
                except Exception as e:
                    logger.warning(f"âš ï¸ Redis è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    logger.warning(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                    else:
                        raise ConnectionError(f"âŒ æ— æ³•è¿æ¥ Redis: {e}")

    async def _ensure_connection(self):
        """ç¡®ä¿è¿æ¥æœ‰æ•ˆ"""
        if self._redis is None:
            await self.connect()
        try:
            await self._redis.ping()
        except Exception as e:
            logger.warning(f"ğŸ”„ Redis è¿æ¥å¤±æ•ˆï¼Œå°è¯•é‡è¿...: {e}")
            self._redis = None
            await self.connect()

    async def put(self, request: Request, priority: int = 0) -> bool:
        """æ”¾å…¥è¯·æ±‚åˆ°é˜Ÿåˆ—"""
        await self._ensure_connection()
        score = -priority
        key = self._get_request_key(request)
        try:
            # ğŸ”¥ ä½¿ç”¨ä¸“ç”¨çš„åºåˆ—åŒ–å·¥å…·æ¸…ç† Request
            clean_request = self.request_serializer.prepare_for_serialization(request)
            
            serialized = pickle.dumps(clean_request)
            pipe = self._redis.pipeline()
            pipe.zadd(self.queue_name, {key: score})
            pipe.hset(f"{self.queue_name}:data", key, serialized)
            result = await pipe.execute()
            
            if result[0] > 0:
                logger.debug(f"âœ… æˆåŠŸå…¥é˜Ÿ: {request.url}")
            return result[0] > 0
        except Exception as e:
            logger.error(f"âŒ æ”¾å…¥é˜Ÿåˆ—å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return False

    async def get(self, timeout: float = 5.0) -> Optional[Request]:
        """
        è·å–è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶ï¼‰
        :param timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…æ— é™è½®è¯¢
        """
        await self._ensure_connection()
        start_time = asyncio.get_event_loop().time()

        while True:
            try:
                # å°è¯•è·å–ä»»åŠ¡
                result = await self._redis.zpopmin(self.queue_name, count=1)
                if result:
                    key, score = result[0]
                    serialized = await self._redis.hget(f"{self.queue_name}:data", key)
                    if not serialized:
                        continue

                    # ç§»åŠ¨åˆ° processing
                    processing_key = f"{key}:{int(time.time())}"
                    pipe = self._redis.pipeline()
                    pipe.zadd(self.processing_queue, {processing_key: time.time() + self.timeout})
                    pipe.hset(f"{self.processing_queue}:data", processing_key, serialized)
                    pipe.hdel(f"{self.queue_name}:data", key)
                    await pipe.execute()

                    return pickle.loads(serialized)

                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                if asyncio.get_event_loop().time() - start_time > timeout:
                    return None

                # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç©ºè½®è¯¢
                await asyncio.sleep(0.1)

            except Exception as e:
                logger.error(f"âŒ è·å–é˜Ÿåˆ—ä»»åŠ¡å¤±è´¥: {e}")
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
                return None

    async def ack(self, request: Request):
        """ç¡®è®¤ä»»åŠ¡å®Œæˆ"""
        await self._ensure_connection()
        key = self._get_request_key(request)
        cursor = 0
        while True:
            cursor, keys = await self._redis.zscan(self.processing_queue, cursor, match=f"{key}:*")
            if keys:
                pipe = self._redis.pipeline()
                for k in keys:
                    pipe.zrem(self.processing_queue, k)
                    pipe.hdel(f"{self.processing_queue}:data", k)
                await pipe.execute()
            if cursor == 0:
                break

    async def fail(self, request: Request, reason: str = ""):
        """æ ‡è®°ä»»åŠ¡å¤±è´¥"""
        await self._ensure_connection()
        key = self._get_request_key(request)
        await self.ack(request)

        retry_key = f"{self.failed_queue}:retries:{key}"
        retries = await self._redis.incr(retry_key)
        await self._redis.expire(retry_key, 86400)

        if retries <= self.max_retries:
            await self.put(request, priority=request.priority + 1)
            logger.info(f"ğŸ” ä»»åŠ¡é‡è¯• [{retries}/{self.max_retries}]: {request.url}")
        else:
            failed_data = {
                "url": request.url,
                "reason": reason,
                "retries": retries,
                "failed_at": time.time(),
                "request_pickle": pickle.dumps(request).hex(),  # å¯é€‰ï¼šä¿å­˜å®Œæ•´è¯·æ±‚
            }
            await self._redis.lpush(self.failed_queue, pickle.dumps(failed_data))
            logger.error(f"âŒ ä»»åŠ¡å½»åº•å¤±è´¥ [{retries}æ¬¡]: {request.url}")

    def _get_request_key(self, request: Request) -> str:
        """ç”Ÿæˆè¯·æ±‚å”¯ä¸€é”®"""
        return f"url:{hash(request.url)}"

    async def qsize(self) -> int:
        """è·å–é˜Ÿåˆ—å¤§å°"""
        await self._ensure_connection()
        return await self._redis.zcard(self.queue_name)

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self._redis:
            await self._redis.close()
            self._redis = None
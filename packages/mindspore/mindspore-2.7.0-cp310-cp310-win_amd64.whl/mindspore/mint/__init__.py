# Copyright 2024 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""mint module."""
from __future__ import absolute_import
import mindspore.ops as ops
from mindspore.ops.primitive import constexpr
from mindspore.common.tensor import Tensor
from mindspore.ops.function.array_func import gather_ext as gather
from mindspore.ops.function.nn_func import conv2d_ext as conv2d
from mindspore.mint.nn.functional import sigmoid
from mindspore.mint.nn import functional
from mindspore.mint import linalg
from mindspore.mint import special
from mindspore.mint import distributed
from mindspore.ops import erf
from mindspore.ops.function.math_func import linspace_ext as linspace
from mindspore.ops.function.math_func import median_ext as median
from mindspore.ops.function.array_func import ones_like_ext as ones_like
from mindspore.ops.function.array_func import full_ext as full
from mindspore.ops.function.array_func import zeros_like_ext as zeros_like
from mindspore.ops.function.array_func import unique_ext as unique
from mindspore.ops.function.array_func import chunk_ext as chunk
from mindspore.ops.functional_overload import empty
from mindspore.ops.functional_overload import empty_like
from mindspore.ops.function.math_func import isclose
from mindspore.ops.auto_generate import abs
from mindspore.ops.auto_generate import clone
from mindspore.ops.function.array_func import full_like_ext as full_like
# 1
from mindspore.ops.function.math_func import divide
from mindspore.ops.auto_generate import topk_ext as topk
from mindspore.ops.function.math_func import roll
# 2
from mindspore.ops.function.math_func import sin
# 3
from mindspore.ops.functional_overload import clamp, where
from mindspore.ops.functional_overload import clip
from mindspore.ops.functional_overload import fmod
from mindspore.ops.functional_overload import max
from mindspore.ops.functional_overload import min
# 4
from mindspore.ops.auto_generate import sinc
from mindspore.ops.auto_generate import sinh
from mindspore.ops.auto_generate import cosh
from mindspore.ops.functional_overload import xlogy
# 5
from mindspore.ops.auto_generate import cumsum_ext as cumsum
# 6
from mindspore.ops.auto_generate import stack_ext as stack

# 7
from mindspore.ops.function.array_func import unsqueeze
# 8
from mindspore.ops.auto_generate import transpose_ext_view as transpose
from mindspore.ops.auto_generate import batch_norm_elemt
from mindspore.ops.auto_generate import batch_norm_gather_stats_with_counts
from mindspore.ops.auto_generate import batch_norm_stats
# 9
from mindspore.ops.auto_generate import masked_select
from mindspore.ops.function.math_func import cross
# 10
from mindspore.ops.function.math_func import ne
# 11
from mindspore.ops.function.math_func import cdist as cdist_
# 12
from mindspore.ops.functional_overload import repeat_interleave
# 13
from mindspore.ops.functional import flip
# 14
from mindspore.ops.auto_generate import mv
# 15
from mindspore.ops.auto_generate import flatten_ext as flatten
# 16
from mindspore.ops.functional import matmul
from mindspore.ops.auto_generate import bmm_ext as bmm
# 17

# 18

# 19
from mindspore.ops.functional import log
# 20

# 21
from mindspore.ops.function.math_func import mul_ext as mul
# 22
from mindspore.ops.functional import cumprod
# 23
from mindspore.ops.auto_generate import exp2
# 24

# 25
from mindspore.ops.functional import greater, gt
# 26
from mindspore.ops.functional import eq
# 27
from mindspore.ops.functional import reciprocal
# 28
from mindspore.ops.functional import exp
# 29

# 30
from mindspore.ops.functional import searchsorted
# 31

# 32
from mindspore.ops.function.math_func import einsum_ext as einsum
# 33

# 34

# 35
from mindspore.ops.functional import erfinv
# 36

# 37
from mindspore.ops.function.array_func import nonzero
# 38

# 39

# 40

# 41

# 42
from mindspore.ops.function.math_func import argmax_ext as argmax
# 43

# 44
from mindspore.ops.functional import cos
# 45

# 46
from mindspore.ops.function.math_func import bitwise_and_ext as bitwise_and
# 47
from mindspore.ops.function.math_func import bitwise_or_ext as bitwise_or
# 48
from mindspore.ops.function.math_func import bitwise_xor_ext as bitwise_xor
# 49
from mindspore.ops.function.math_func import baddbmm_ext as baddbmm
# 50
from mindspore.ops.functional import tile
# 51

# 52
from mindspore.ops.functional_overload import addcdiv
# 53

# 54
from mindspore.ops.function.random_func import normal_ext as normal
# 55

# 56
from mindspore.ops.function.math_func import norm_ext as norm
# 57
from mindspore.ops.functional import broadcast_to
# 58
from mindspore.ops.functional_overload import greater_equal, ge

# 59
from mindspore.ops.functional import square
# 60

# 61
from mindspore.ops.functional import rsqrt
# 62
from mindspore.ops.functional import maximum
# 63
from mindspore.ops.functional import minimum
# 64
from mindspore.ops.functional import ravel
# 65
from mindspore.ops.functional import logical_and
# 66
from mindspore.ops.functional import logical_not
# 67
from mindspore.ops.functional import logical_or
# 68
from mindspore.ops.functional import logical_xor
# 69
from mindspore.ops.functional import less_equal, le
# 70
from mindspore.ops.functional import negative, neg
# 71

# 72

# 73
from mindspore.ops.functional import ceil
# 74
from mindspore.ops.function.array_func import sort_ext as sort
# 75
from mindspore.ops.functional import less, lt
# 76
from mindspore.ops.function.math_func import pow_ext as pow
# 77

# 78
from mindspore.ops.function import arange_ext as arange
# 79

# 80
from mindspore.ops.functional_overload import div
# 81
from mindspore.ops.auto_generate import index_select_ext as index_select
# 82
from mindspore.ops.auto_generate import cummin_ext as cummin
# 83
from mindspore.ops.auto_generate import narrow
# 84

# 85
from mindspore.mint import nn, optim
# 86

# 87
from mindspore.ops.auto_generate import trunc
# 88

# 89
from mindspore.ops.auto_generate import argsort_ext as argsort
# 90
from mindspore.ops.auto_generate import isinf
# 91

# 92
from mindspore.ops.function.math_func import polar
# 93

# 94
from mindspore.ops.function.math_func import tanh
# 95
from mindspore.ops.function.math_func import diff_ext as diff
# 96

# 97

# 98

# 99

# 100

# 101

# 102

# 103

# 104

# 105

# 106

# 107

# 108

# 109
from mindspore.ops.auto_generate import argmin_ext as argmin
# 110
from mindspore.ops.function.nn_func import softmax_ext
# 111

# 112

# 113

# 114

# 115

# 116

# 117

# 118

# 119
from mindspore.ops.functional_overload import any
# 120
from mindspore.ops.auto_generate import isneginf_ext as isneginf
# 121

# 122

# 123
from mindspore.ops.function.math_func import var_ext as var

# 151
from mindspore.ops.function.math_func import acos_ext as acos
from mindspore.ops.function.math_func import arccos_ext as arccos
# 152
from mindspore.ops.function.math_func import acosh_ext as acosh
from mindspore.ops.function.math_func import arccosh_ext as arccosh
# 172
from mindspore.ops.function.math_func import addcmul_ext as addcmul

from mindspore.ops.function.math_func import asin_ext as asin
from mindspore.ops.function.math_func import arcsin_ext as arcsin
# 173
from mindspore.ops.function.math_func import asinh_ext as asinh
from mindspore.ops.function.math_func import arcsinh_ext as arcsinh
# 174
from mindspore.ops.function.math_func import atan_ext as atan
from mindspore.ops.function.math_func import arctan_ext as arctan
# 175
from mindspore.ops.function.math_func import atanh
from mindspore.ops.function.math_func import arctanh
# 176
from mindspore.ops.function.math_func import atan2_ext as atan2
from mindspore.ops.function.math_func import arctan2_ext as arctan2

# 177
from mindspore.ops.function.math_func import round

# 182
from mindspore.ops.function.math_func import bernoulli_ext as bernoulli

# 201
from mindspore.ops.auto_generate import diag_ext as diag

# 204
from mindspore.ops.auto_generate import erfc
# 207
from mindspore.ops.auto_generate import expm1
# 208
from mindspore.ops.function.array_func import eye
from mindspore.ops.function.random_func import randperm_ext as randperm
from mindspore.ops.function.random_func import rand_ext as rand
from mindspore.ops.function.random_func import rand_like_ext as rand_like
from mindspore.ops.function.random_func import randn_ext as randn
from mindspore.ops.function.random_func import randn_like_ext as randn_like
from mindspore.ops.function.random_func import randint_ext as randint
from mindspore.ops.function.random_func import randint_like_ext as randint_like
# 210
from mindspore.ops.auto_generate import floor
# 231
from mindspore.ops.function.math_func import inverse_ext as inverse
# 239
from mindspore.ops.functional_overload import lerp
# 244
from mindspore.ops.auto_generate import log1p
# 261
from mindspore.ops.function.random_func import multinomial_ext as multinomial
# 275
from mindspore.ops.functional_overload import remainder
# 285
from mindspore.ops.function.array_func import scatter_add_ext as scatter_add
# 289
from mindspore.ops.auto_generate import sign

from mindspore.ops.auto_generate import select_ext_view as select

# 301
from mindspore.ops.function.math_func import tan

# 303
from mindspore.ops.auto_generate import trace_ext as trace
from mindspore.ops.auto_generate import gcd

from mindspore.ops.function.array_func import reshape

from mindspore.ops.auto_generate import outer_ext as outer

# 304
from mindspore.ops.function.array_func import tril_ext as tril
# 520
from mindspore.ops.function.math_func import bincount_ext as bincount

# 305
from mindspore.ops import triu

# 308
from mindspore.ops.auto_generate import mm_ext as mm

# 382
from mindspore.ops.function.math_func import dstack

# 501
from mindspore.ops.function.math_func import addbmm_ext as addbmm

# 502
from mindspore.ops.function.math_func import addmm_ext as addmm

# 505
from mindspore.ops.function.math_func import addmv_ext as addmv

# 510
from mindspore.ops.function.math_func import amax_ext as amax

# 511
from mindspore.ops.function.math_func import amin_ext as amin

# 521
from mindspore.ops.functional_overload import bitwise_not

# 526
from mindspore.ops.auto_generate import dot

# 533
from mindspore.ops.function.math_func import frac_ext as frac

# 538
from mindspore.ops.function.math_func import histc_ext as histc
# 549
from mindspore.ops.functional_overload import kthvalue
# 552
from mindspore.ops.auto_generate import log10_ext as log10

# 553
from mindspore.ops.auto_generate import logaddexp_ext as logaddexp
from mindspore.ops.auto_generate import logaddexp2

# 557
from mindspore.ops.auto_generate import logsumexp_ext as logsumexp

# 582
from mindspore.ops.function.math_func import std_mean_ext as std_mean

# 584
from mindspore.ops.function.array_func import take

# 588
from mindspore.ops.function.math_func import var_mean_ext as var_mean

# 610
from mindspore.ops.function.math_func import nan_to_num

# 613
from mindspore.ops.functional_overload import nansum

# 615
from mindspore.ops.auto_generate import triangular_solve

# 664
from mindspore.ops.function.array_func import meshgrid_ext as meshgrid

# 695
from mindspore.ops.auto_generate import count_nonzero

# 697
from mindspore.ops.function.math_func import float_power_ext as float_power

# 708
from mindspore.ops.function.math_func import std_ext as std

# 719
from mindspore.ops.functional_overload import add

# 720
from mindspore.ops.functional_overload import sub

# 739
from mindspore.ops.function.array_func import hstack

# 826
from mindspore.ops.functional_overload import floor_divide

# 887
from mindspore.ops.auto_generate import log2_ext as log2

# 889
from mindspore.ops.function.math_func import isnan_ext as isnan

# 916
from mindspore.ops.functional_overload import index_add

# 1007
from mindspore.ops.auto_generate import t_ext as t
from mindspore.ops.auto_generate.pyboost_inner_prim import squeeze_impl
from mindspore.ops.auto_generate.gen_ops_prim import equal_ext_op


# 1023
from mindspore.ops.function.array_func import unbind_ext as unbind


def all(input, dim=None, keepdim=False):
    r"""
    all(input) -> Tensor

    Tests if all element in `input` evaluates to `True`.

    Args:
        input (Tensor): The input Tensor.

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[True, False], [True, True]])
        >>> output = mindspore.mint.all(input)
        >>> print(output)
        False

    .. function:: all(input, dim, keepdim=False) -> Tensor
        :noindex:

    Tests if all element in `input` evaluates to `True` along the given axes.

    Args:
        input (Tensor): The input tensor.
        dim (Union[int, tuple(int), list(int), Tensor]): The dimensions to reduce. If ``None`` ,
                all dimensions are reduced. Default ``None`` .
        keepdim (bool, optional): Whether the output tensor has dim retained or not. Default ``False`` .

    Returns:
        Tensor

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> input = mindspore.tensor([[True, False], [True, True]])
        >>>
        >>> # case 1: Reduces a dimension along dim 1, with keepdim False.
        >>> mindspore.mint.all(input, dim=1)
        Tensor(shape=[2], dtype=Bool, value= [False,  True])
        >>>
        >>> # case 2: Reduces a dimension along dim (0, 1), with keepdim False.
        >>> mindspore.mint.all(input, dim=(0,1))
        Tensor(shape=[], dtype=Bool, value= False)
        >>>
        >>> # case 3: Reduces a dimension along dim [0, 1], with keepdim True.
        >>> mindspore.mint.all(input, dim=[0,1], keepdim=True)
        Tensor(shape=[1, 1], dtype=Bool, value=
        [[False]])
    """
    return ops.function.math_func.all(input, dim, keepdim)


def allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False):
    """
    Returns a new Tensor with boolean elements representing if each element of `input`
    is “close” to the corresponding element of `other`. Closeness is defined as:

    .. math::
        |input-other| ≤ atol + rtol × |other|

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): First tensor to compare.
            Support dtype: float16, float32, float64, int8, int16, int32, int64 and uint8.
            On Ascend, more dtypes are support: bool and bfloat16.
        other (Tensor): Second tensor to compare. Dtype must be same as `input`.
        rtol (Union[float, int, bool], optional): Relative tolerance. Default: ``1e-05`` .
        atol (Union[float, int, bool], optional): Absolute tolerance. Default: ``1e-08`` .
        equal_nan (bool, optional): If ``True`` , then two NaNs will be considered equal. Default: ``False``.

    Returns:
        A bool Scalar.

    Raises:
        TypeError: `input` or `other` is not Tensor.
        TypeError: `input` or `other` dtype is not support.
        TypeError: `atol` or `rtol` is not float, int or bool.
        TypeError: `equal_nan` is not bool.
        TypeError: `input` and `other` have different dtypes.
        ValueError: `input` and `other` cannot broadcast.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input = Tensor(np.array([1.3, 2.1, 3.2, 4.1, 5.1]), mindspore.float16)
        >>> other = Tensor(np.array([1.3, 3.3, 2.3, 3.1, 5.1]), mindspore.float16)
        >>> output = mint.allclose(input, other)
        >>> print(output)
        False
    """
    return isclose(input, other, rtol, atol, equal_nan).all().item()


def cat(tensors, dim=0):
    r"""
    Connect input tensors along with the given dimension.

    The input data is a tuple or a list of tensors. These tensors have the same rank :math:`R`.
    Set the given dimension as :math:`m`, and :math:`0 \le m < R`. Set the number of input tensors as :math:`N`.
    For the :math:`i`-th tensor :math:`t_i`, it has the shape of :math:`(x_1, x_2, ..., x_{mi}, ..., x_R)`.
    :math:`x_{mi}` is the :math:`m`-th dimension of the :math:`t_i`. Then, the shape of the output tensor is

    .. math::

        (x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)

    Args:
        tensors (Union[tuple, list]): A tuple or a list of input tensors.
            Suppose there are two tensors in this tuple or list, namely t1 and t2.
            To perform `concat` in the dimension 0 direction, except for the :math:`0`-th dimension,
            all other dimensions should be equal, that is,
            :math:`t1.shape[1] = t2.shape[1], t1.shape[2] = t2.shape[2], ..., t1.shape[R-1] = t2.shape[R-1]`,
            where :math:`R` represents the rank of tensor.
        dim (int, optional): The specified dimension, whose value is in range :math:`[-R, R)`. Default: ``0`` .

    Returns:
        Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.
        The data type is the same with `tensors`.

    Raises:
        TypeError: If `dim` is not an int.
        ValueError: If `tensors` have different dimension of tensor.
        ValueError: If `dim` not in range :math:`[-R, R)`.
        ValueError: If tensor's shape in `tensors` except for `dim` are different.
        ValueError: If `tensors` is an empty tuple or list.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>> from mindspore import mint
        >>> input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))
        >>> input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))
        >>> output = mint.cat((input_x1, input_x2))
        >>> print(output)
        [[0. 1.]
         [2. 1.]
         [0. 1.]
         [2. 1.]]
        >>> output = mint.cat((input_x1, input_x2), 1)
        >>> print(output)
        [[0. 1. 0. 1.]
         [2. 1. 2. 1.]]
    """
    return ops.auto_generate.cat(tensors, dim)


def concat(tensors, dim=0):
    r"""
    Alias for :func:`mindspore.mint.cat`.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``
    """
    return cat(tensors, dim)


def cummax(input, dim):
    r"""
    Returns a tuple (values, indices) where `values` is the cumulative maximum value of input Tensor `input`
    along the dimension `dim`, and `indices` is the index location of each maximum value.

    .. math::
        \begin{array}{ll} \\
            y_{i} = \max(x_{1}, x_{2}, ... , x_{i})
        \end{array}

    .. note::
        GE backend is not supported in Ascend.

    Args:
        input (Tensor): The input Tensor. Rank of `input` must be greater than 0.
        dim (int): The dimension to do the operation over. The value of `dim` must be in the range
            `[-input.ndim, input.ndim - 1]`.

    Returns:
        tuple [Tensor], tuple of 2 Tensors, containing the cumulative maximum of elements and the index.
        The shape of each output tensor is the same as that of input `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not an int.
        ValueError: If `dim` is out the range of `[-input.ndim, input.ndim - 1]`.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor
        >>> from mindspore import ops
        >>> x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))
        >>> output = mint.cummax(x, dim=0)
        >>> print(output[0])
        [[ 3.  4.  6. 10.]
         [ 3.  6.  7. 10.]
         [ 4.  6.  8. 10.]
         [ 4.  6.  8. 10.]]
        >>> print(output[1])
        [[0 0 0 0]
         [0 1 1 0]
         [2 1 2 0]
         [2 1 2 0]]
    """
    return ops.auto_generate.cummax(input, dim)


def not_equal(input, other):
    r"""
    Alias for :func:`mindspore.mint.ne` .

    Supported Platforms:
        ``Ascend``
    """
    return ne(input, other)


def softmax(input, dim, *, dtype=None):
    r"""
    Alias for :func:`mindspore.mint.nn.functional.softmax`.

    Supported Platforms:
        ``Ascend``
    """
    return softmax_ext(input, dim, dtype)


def equal(input, other):
    r"""
    Computes the equivalence between two tensors.

    Note:
        `input` and `other` comply with the implicit type conversion rules to make the data types consistent.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The first input.
        other (Tensor): The second input.

    Returns:
        bool.

    Raises:
        TypeError: If `input` or `other` is not a Tensor.

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import mindspore
        >>> from mindspore import Tensor, mint
        >>> x = Tensor([1, 2, 3], mindspore.int32)
        >>> y = Tensor([1, 2, 4], mindspore.int32)
        >>> output = mint.equal(x, y)
        >>> print(output)
        False
    """
    result = equal_ext_op(input, other)
    return result.item()


def isfinite(input):
    r"""
    Determine which elements are finite for each position. If elements are not ``NaN`` , ``-INF`` , ``INF``,
    they are finite.

    .. math::
        out_i = \begin{cases}
          & \text{ if } input_{i} = \text{Finite},\ \ True \\
          & \text{ if } input_{i} \ne \text{Finite},\ \ False
        \end{cases}

    Args:
        input (Tensor): The input tensor.

    Returns:
        Tensor, has the same shape of input, and the dtype is bool.

    Raises:
        TypeError: If input is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)
        >>> output = mint.isfinite(x)
        >>> print(output)
        [False True False]
        >>> x = Tensor(2.1, mindspore.float64)
        >>> output = mint.isfinite(x)
        >>> print(output)
        True
    """
    return ops.auto_generate.isfinite(input)


def item(input):
    r"""
    Returns the value of this tensor as a standard Python number.

    Note:
        This only works for tensors with one element.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.

    Returns:
        number.

    Raises:
        TypeError: If `input` is not a Tensor.
        RuntimeError: If the number of `input` elements is not 1.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([1]).astype(np.float32))
        >>> result = mint.item(x)
        >>> print(result)
        1.0
    """
    if not isinstance(input, Tensor):
        raise TypeError(f"the input must be a Tensor, but got {type(input)}")
    if input.size != 1:
        raise RuntimeError(
            "a Tensor with {} elements cannot be converted to Scalar".format(input.size))
    return input.asnumpy().item()


def mean(input, dim=None, keepdim=False, *, dtype=None):
    r"""
    mean(input, *, dtype=None) -> Tensor

    Reduces all dimension of a tensor by averaging all elements.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],
        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),
        ... mindspore.float32)
        >>> output = mint.mean(x)
        >>> print(output)
        5.0
        >>> print(output.shape)
        ()

    .. function:: mean(input, dim, keepdim=False, *, dtype=None) -> Tensor
        :noindex:

    Reduces all dimension of a tensor by averaging all elements in the dimension, by default.
    And reduce a dimension of `input` along the specified `dim`. `keepdim`
    determines whether the dimensions of the output and input are the same.

    Note:
        The `dim` with tensor type is only used for compatibility with older versions and is not recommended.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
        dim (Union[int, tuple(int), list(int), Tensor]): The dimensions to reduce.
            Only constant value is allowed. Assume the rank of `input` is r, and the value range is [-r,r).
        keepdim (bool): If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor.

        - If `dim` is int, set as 1, and `keepdim` is ``False`` ,
          the shape of output is :math:`(input_0, input_2, ..., input_R)`.
        - If `dim` is tuple(int) or list(int), set as (1, 2), and `keepdim` is ``False`` ,
          the shape of output is :math:`(input_0, input_3, ..., input_R)`.
        - If `dim` is 1-D Tensor, set as [1, 2], and `keepdim` is ``False`` ,
          the shape of output is :math:`(input_0, input_3, ..., input_R)`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not one of the following: int, tuple, list or Tensor.
        TypeError: If `keepdim` is not a bool.
        ValueError: If `dim` is out of range.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],
        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),
        ... mindspore.float32)
        >>> output = mint.mean(x, 0, True)
        >>> print(output)
        [[[4. 4. 4. 4. 4. 4.]
          [5. 5. 5. 5. 5. 5.]
          [6. 6. 6. 6. 6. 6.]]]
    """
    return ops.auto_generate.mean_ext(input, dim, keepdim, dtype)


def prod(input, dim=None, keepdim=False, *, dtype=None):
    r"""
    prod(input, *, dtype=None) -> Tensor

    Multiplying all elements of input.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)
        >>> output = mint.prod(x)
        >>> print(output)
        2.2833798e+33
        >>> print(output.shape)
        ()

    .. function:: prod(input, dim, keepdim=False, *, dtype=None) -> Tensor
        :noindex:

    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can
    reduce a dimension of `input` along the `dim`. Determine whether the dimensions of the output and input are the
    same by controlling `keepdim`.

    Args:
        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.
            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.
        dim (int): The dimensions to reduce. Only constant value is allowed.
            Assume the rank of `x` is r, and the value range is [-r,r).
        keepdim (bool): If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        Tensor.

        - If `dim` is int, set as 1, and `keepdim` is ``False`` ,
          the shape of output is :math:`(input_0, input_2, ..., input_R)`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not int.
        TypeError: If `keepdim` is not a bool.
        ValueError: If `dim` is out of range.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)
        >>> output = mint.prod(x, 0, True)
        >>> print(output)
        [[[ 28.  28.  28.  28.  28.  28.]
          [ 80.  80.  80.  80.  80.  80.]
          [162. 162. 162. 162. 162. 162.]]]
    """
    return ops.auto_generate.prod_ext(input, dim, keepdim, dtype)


def sum(input, dim=None, keepdim=False, *, dtype=None):
    r'''
    sum(input, *, dtype=None) -> Tensor

    Calculate sum of all elements in Tensor.

    Args:
        input (Tensor): The input tensor.

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        A Tensor, sum of all elements in `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> from mindspore import dtype as mstype
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mstype.float32)
        >>> out = mint.sum(x)
        >>> print(out)
        270.0

    .. function:: sum(input, dim, keepdim=False, *, dtype=None) -> Tensor
        :noindex:

    Calculate sum of Tensor elements over a given dim.

    Note:
        The `dim` with tensor type is only used for compatibility with older versions and is not recommended.

    Args:
        input (Tensor): The input tensor.
        dim (Union[int, tuple(int), list(int), Tensor]): Dimensions along which a sum is performed.
            If the `dim` is a tuple or list of ints, a sum is performed on all the dimensions specified in the tuple.
            Must be in the range :math:`[-input.ndim, input.ndim)` .
        keepdim (bool): Whether the output tensor has `dim` retained or not.
            If ``True`` , keep these reduced dimensions and the length is 1.
            If ``False`` , don't keep these dimensions. Default: ``False`` .

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: ``None`` .

    Returns:
        A Tensor, sum of elements over a given `dim` in `input`.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If `dim` is not an int, tulpe(int), list(int) or Tensor.
        ValueError: If `dim` is not in the range :math:`[-input.ndim, input.ndim)` .
        TypeError: If `keepdim` is not a bool.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> from mindspore import dtype as mstype
        >>> x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],
        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],
        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mstype.float32)
        >>> out = mint.sum(x)
        >>> print(out)
        270.0
        >>> out = mint.sum(x, dim=2)
        >>> print(out)
        [[ 6. 12. 18.]
         [24. 30. 36.]
         [42. 48. 54.]]
        >>> out = mint.sum(x, dim=2, keepdim=True)
        >>> print(out)
        [[[ 6.]
          [12.]
          [18.]]
         [[24.]
          [30.]
          [36.]]
         [[42.]
          [48.]
          [54.]]]
    '''
    return ops.auto_generate.sum_ext(input, dim, keepdim, dtype)


def ones(size, *, dtype=None):
    r"""
    Creates a tensor filled with value ones.

    Creates a tensor with shape described by the first argument and fills it with value ones in type of the second
    argument.

    Args:
        size (Union[tuple[int], list[int], int, Tensor]): The specified shape of output tensor. Only positive integer or
            tuple or Tensor containing positive integers are allowed. If it is a Tensor,
            it must be a 0-D or 1-D Tensor with int32 or int64 dtypes.

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor. If `dtype` is ``None`` ,
            `mindspore.float32` will be used. Default: ``None`` .

    Returns:
        Tensor, whose dtype and size are defined by input.

    Raises:
        TypeError: If `size` is neither an int nor a tuple/list/Tensor of int.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import mint
        >>> output = mint.ones((2, 2), dtype=mindspore.float32)
        >>> print(output)
        [[1. 1.]
         [1. 1.]]
    """
    return ops.auto_generate.ones(size, dtype)


def permute(input, dims):
    """
    Permutes the dimensions of the input tensor according to input `dims` .

    Args:
        input (Tensor): Input Tensor.
        dims (tuple(int)): The order of the dimensions. Permute rearranges the `input` according
            to the order of the `dims`.

    Returns:
        Tensor, has the same dimension as input tensor, with `axis` suitably permuted.

    Raises:
        ValueError: If `dims` is None.
        ValueError: If the number of elements of `dims` is not equal to `input` ndim.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> input_x = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)
        >>> input_perm = (0, 2, 1)
        >>> print(mint.permute(input_x, input_perm))
        [[[ 1.  4.]
          [ 2.  5.]
          [ 3.  6.]]
         [[ 7. 10.]
          [ 8. 11.]
          [ 9. 12.]]]
    """
    return ops.functional.permute(input, dims)


def split(tensor, split_size_or_sections, dim=0):
    """
    Splits the Tensor into chunks along the given dim.

    Args:
        tensor (Tensor): A Tensor to be divided.
        split_size_or_sections (Union[int, tuple(int), list(int)]):
            If `split_size_or_sections` is an int type, `tensor` will be split into equally sized chunks,
            each chunk with size `split_size_or_sections`. Last chunk will be smaller than `split_size_or_sections`
            if `tensor.shape[dim]` is not divisible by `split_size_or_sections`.
            If `split_size_or_sections` is a list type, then `tensor` will be split into len(split_size_or_sections)
            chunks with sizes `split_size_or_sections` along the given `dim`.
        dim (int, optional): The dim along which to split. Default: ``0`` .

    Returns:
        A tuple of sub-tensors.

    Raises:
        TypeError: If argument `tensor` is not Tensor.
        TypeError: If argument `dim` is not int.
        ValueError: If argument `dim` is out of range of [-tensor.ndim, tensor.ndim).
        TypeError: If each element in `split_size_or_sections` is not integer.
        TypeError: If argument `split_size_or_sections` is not int, tuple(int) or list(int).
        ValueError: The sum of `split_size_or_sections` is not equal to tensor.shape[dim].

    Supported Platforms:
        ``Ascend``

    Examples:
        >>> import numpy as np
        >>> from mindspore import ops, Tensor
        >>> input_x = np.arange(9).astype("float32")
        >>> output = ops.split(Tensor(input_x), 3)
        >>> print(output)
        (Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),
         Tensor(shape=[3], dtype=Float32, value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),
         Tensor(shape=[3], dtype=Float32, value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))
    """
    return ops.function.array_func.split_ext(tensor, split_size_or_sections, dim)


def sqrt(input):
    r"""
    Returns sqrt of a tensor element-wise.

    .. math::

        out_{i} = \sqrt{input_{i}}

    Args:
        input (Tensor): The input tensor with a dtype of number.Number.

    Returns:
        Tensor, has the same shape as the `input`.

    Raises:
        TypeError: If `input` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> input = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)
        >>> output = mint.sqrt(input)
        >>> print(output)
        [1. 2. 3.]
    """
    return ops.auto_generate.sqrt(input)


def squeeze(input, dim):
    r"""
    Return the Tensor after deleting the dimension of size 1 in the specified `dim`.

    If :math:`dim=()`, it will remove all the dimensions of size 1.
    If `dim` is specified, it will remove the dimensions of size 1 in the given `dim`.
    For example, if the dimension is not specified :math:`dim=()`, input shape is (A, 1, B, C, 1, D),
    then the shape of the output Tensor is (A, B, C, D). If the dimension is specified, the squeeze operation
    is only performed in the specified dimension. If input shape is (A, 1, B), when :math:`dim=0` or :math:`dim=2`,
    the input tensor is not changed, while when :math:`dim=1`, the input tensor shape is changed to (A, B).

    Note:
        - Please note that in dynamic graph mode, the output Tensor will share data with the input Tensor,
          and there is no Tensor data copy process.
        - The dimension index starts at 0 and must be in the range `[-input.ndim, input.ndim]`.
        - In GE mode, only support remove dimensions of size 1 from the shape of input tensor.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): Used to calculate Squeeze. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.
        dim (Union[int, tuple(int)]): Specifies the dimension indexes of shape to be removed, which will
            remove all the dimensions of size 1 in the given dim parameter. If specified, it must be int32 or int64.

    Returns:
        Tensor, the shape of tensor is :math:`(x_1, x_2, ..., x_S)`.

    Raises:
        TypeError: If `input` is not a tensor.
        TypeError: If `dim` is not an int, tuple.
        TypeError: If `dim` is a tuple whose elements are not all int.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> input = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)
        >>> output = mint.squeeze(input, 2)
        >>> print(output)
        [[1. 1.]
         [1. 1.]
         [1. 1.]]
    """
    return squeeze_impl(input, dim)


def swapaxes(input, axis0, axis1):
    '''
    Alias for :func:`mindspore.mint.transpose` . The `input` corresponds to the `input` in the reference interface,
    and the parameters `axis0` and `axis1` correspond to `dim0` and `dim1` in the reference interface respectively.

    For more details, see :func:`mindspore.mint.transpose` .

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Examples:
        >>> import numpy as np
        >>> from mindspore import mint
        >>> from mindspore import Tensor
        >>> input = Tensor(np.ones((2,3,4), dtype=np.float32))
        >>> output = mint.swapaxes(input, 0, 2)
        >>> print(output.shape)
        (4, 3, 2)
    '''
    return transpose(input, axis0, axis1)


def unique_consecutive(input, return_inverse=False, return_counts=False, dim=None):
    r"""
    Returns the elements that are unique in each consecutive group of equivalent elements in the input tensor.

    When `return_inverse=True` , it returns a tensor containing the indices of the elements in the input tensor
    within the output tensor.

    When `return_counts=True` , it returns a tensor representing the number of occurrences of each output element
    in the input.

    .. warning::
        This is an experimental API that is subject to change or deletion.

    Args:
        input (Tensor): The input tensor.
        return_inverse (bool, optional): Whether to return the index of where the element in the original input
            maps to the position in the output. Default: ``False`` .
        return_counts (bool, optional): Whether to return the counts of each unique element. Default: ``False`` .
        dim (int, optional): The dimension to apply unique. If ``None`` , the unique of the flattened input is
            returned. If the dimension is specified, it must be int32 or int64. Default: ``None`` .

    Returns:
        A tensor or a tuple of tensors containing tensor objects (`output`, `inverse_indices`, `counts`).

        - **output** (Tensor): the output tensor has the same type as `input` and represents the output list of
          unique scalar elements.
        - **inverse_indices** (Tensor, optional): if `return_inverse` is True, there will be an additional returned
          tensor `inverse_indices`. `inverse_indices` has the same shape as `input` and represents the index of where
          the element in the original input maps to the position in the output.
        - **counts** (Tensor, optional):  if `return_counts` is True, there will be an additional returned tensor
          `counts`. `counts` has the same shape as `output` or `output.shape[dim]` if dim was specified and represents
          the number of occurrences for each unique value or tensor.

    Raises:
        TypeError: If `input` is not a Tensor.
        TypeError: If dtype of `input` is not supported.
        TypeError: If `return_inverse` is not a bool.
        TypeError: If `return_counts` is not a bool.
        TypeError: If `dim` is not an int.
        ValueError: If `dim` is not in the range of :math:`[-ndim, ndim-1]`.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, mint
        >>> from mindspore import dtype as mstype
        >>> x = Tensor(np.array([1, 1, 2, 2, 3, 1, 1, 2]), mstype.int64)
        >>> output, inverse_indices, counts = mint.unique_consecutive(x, True, True, None)
        >>> print(output)
        [1 2 3 1 2]
        >>> print(inverse_indices)
        [0 0 1 1 2 3 3 4]
        >>> print(counts)
        [2 2 1 2 1]
    """

    return ops.function.array_func.unique_consecutive(input, return_inverse, return_counts, dim)


def zeros(size, *, dtype=None):
    """
    Creates a tensor filled with 0 with shape described by `size` and fills it with value 0 in type of `dtype`.

    Args:
        size (Union[tuple[int], list[int], int, Tensor]): The specified shape of output tensor. Only positive integer or
            tuple or Tensor containing positive integers are allowed. If it is a Tensor,
            it must be a 0-D or 1-D Tensor with int32 or int64 dtypes.

    Keyword Args:
        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor. If `dtype` is ``None`` ,
            mindspore.float32 will be used. Default: ``None`` .

    Returns:
        Tensor, whose dtype and size are defined by input.

    Raises:
        TypeError: If `size` is neither an int nor a tuple/list/Tensor of int.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> from mindspore import mint
        >>> output = mint.zeros((2, 2), dtype=mindspore.float32)
        >>> print(output)
        [[0. 0.]
         [0. 0.]]
    """
    return ops.auto_generate.zeros(size, dtype)


def fix(input):
    """
    Alias for :func:`mindspore.mint.trunc` .

    For more details, see :func:`mindspore.mint.trunc` .

    Supported Platforms:
        ``Ascend``
    """
    return trunc(input)


def scatter(input, dim, index, src):
    """
    Update the value in `src` to `input` according to the specified index.
    For a 3-D tensor, the output will be:

    .. code-block::

        output[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0

        output[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1

        output[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2

    .. note::
        The backward is supported only for the case `src.shape == index.shape` when `src` is a tensor.

    Args:
        input (Tensor): The target tensor. The rank of `input` must be at least 1.
        dim (int): Which axis to scatter. Accepted range is [-r, r) where r = rank(input).
        index (Tensor): The index to do update operation whose data must be positive number with type of mindspore.int32
            or mindspore.int64. Same rank as `input` . And accepted range is [-s, s) where s is the size along axis.
        src (Tensor, float): The data doing the update operation with `input`. Can be a tensor with the same data type
            as `input` or a float number to scatter.

    Returns:
        Tensor, has the same shape and type as `input` .

    Raises:
        TypeError: If `index` is neither int32 nor int64.
        ValueError: If rank of any of `input` , `index` and `src` is less than 1.
        ValueError: If the rank of `src` is not equal to the rank of `input` .
        TypeError: If the data types of `input` and `src` have different dtypes.
        RuntimeError: If `index` has negative elements.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> import mindspore as ms
        >>> from mindspore import Tensor, mint
        >>> input = Tensor(np.array([[1, 2, 3, 4, 5]]), dtype=ms.float32)
        >>> src = Tensor(np.array([[8, 8]]), dtype=ms.float32)
        >>> index = Tensor(np.array([[2, 4]]), dtype=ms.int64)
        >>> out = mint.scatter(input=input, dim=1, index=index, src=src)
        >>> print(out)
        [[1. 2. 8. 4. 8.]]
        >>> input = Tensor(np.zeros((5, 5)), dtype=ms.float32)
        >>> src = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dtype=ms.float32)
        >>> index = Tensor(np.array([[0, 0, 0], [2, 2, 2], [4, 4, 4]]), dtype=ms.int64)
        >>> out = mint.scatter(input=input, dim=0, index=index, src=src)
        >>> print(out)
        [[1. 2. 3. 0. 0.]
        [0. 0. 0. 0. 0.]
        [4. 5. 6. 0. 0.]
        [0. 0. 0. 0. 0.]
        [7. 8. 9. 0. 0.]]
        >>> input = Tensor(np.zeros((5, 5)), dtype=ms.float32)
        >>> src = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dtype=ms.float32)
        >>> index = Tensor(np.array([[0, 2, 4], [0, 2, 4], [0, 2, 4]]), dtype=ms.int64)
        >>> out = mint.scatter(input=input, dim=1, index=index, src=src)
        >>> print(out)
        [[1. 0. 2. 0. 3.]
        [4. 0. 5. 0. 6.]
        [7. 0. 8. 0. 9.]
        [0. 0. 0. 0. 0.]
        [0. 0. 0. 0. 0.]]
    """
    return ops.function.array_func.scatter(input, dim, index, src)


def cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary'):
    """
    Computes p-norm distance between each pair of row vectors of two input Tensors.

    .. warning::
        This is an experimental optimizer API that is subject to change.

    Note:
        On Ascend, the supported dtypes are float16 and float32.

    Args:
        x1 (Tensor): Input tensor of shape :math:`(B, P, M)`.
            Letter :math:`B` represents 0 or positive int number.
            When :math:`B` is equal to 0, it means this dimension can be ignored,
            i.e. shape of the tensor is :math:`(P, M)`.
        x2 (Tensor): Input tensor of shape :math:`(B, R, M)`, has the same dtype as `x1`.
        p (float, optional): P value for the p-norm distance to calculate between each
            vector pair, P >= 0. Default: ``2.0`` .
        compute_mode (string, optional): Specify the cumpute mode. Setting this parameter currently has no effect.
            Default: ``'use_mm_for_euclid_dist_if_necessary'`` .

    Returns:
        Tensor, p-norm distance, has the same dtype as `x1`, its shape is :math:`(B, P, R)`.

    Raises:
        TypeError: If `x1` or `x2` is not Tensor.
        TypeError: If dtype of `x1` or `x2` is not listed in the "Note" above.
        TypeError: If `p` is not float32.
        ValueError: If `p` is negative.
        ValueError: If dimension of `x1` is not the same as `x2`.
        ValueError: If dimension of `x1` or `x2` is neither 2 nor 3.
        ValueError: If the batch dim of `x1` and `x2` can not broadcast.
        ValueError: If the number of columns of `x1` is not the same as that of `x2`.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> x = Tensor(np.array([[[1.0, 1.0], [2.0, 2.0]]]).astype(np.float32))
        >>> y = Tensor(np.array([[[3.0, 3.0], [3.0, 3.0]]]).astype(np.float32))
        >>> output = ops.cdist(x, y, 2.0)
        >>> print(output)
        [[[2.8284273 2.8284273]
          [1.4142137 1.4142137]]]
    """
    return cdist_(x1, x2, p)


__all__ = [
    'conv2d',
    'full',
    'ones_like',
    'zeros_like',
    'abs',
    'clone',
    'erf',
    'where',
    'isclose',
    'empty',
    'empty_like',
    'full_like',
    # 1
    'div',
    'divide',
    'topk',
    'roll',
    # 2
    'sin',
    # 3
    'clamp',
    'xlogy',
    'fmod',
    # 4
    'sinc',
    'sinh',
    'cosh',
    # 5
    'cumsum',
    # 6
    'stack',
    # 7
    'zeros',
    # 8
    'transpose',
    'swapaxes',
    "batch_norm_elemt",
    "batch_norm_gather_stats_with_counts",
    "batch_norm_stats",
    # 9
    'squeeze',
    # 10
    'ne',
    'not_equal',
    # 11
    'unsqueeze',
    # 12
    "repeat_interleave",
    # 13
    "flip",
    # 14
    'mv',
    # 15
    'flatten',
    # 16
    'matmul',
    'bmm',
    # 17
    'mean',
    # 18
    'sum',
    # 19
    'log',
    # 20
    'prod',
    # 21
    'mul',
    # 22
    'cumprod',
    # 23
    'exp2',
    # 24
    'cdist',
    # 25
    'greater',
    'gt',
    # 26
    'eq',
    # 27
    'reciprocal',
    # 28
    'exp',
    # 29
    'sqrt',
    # 30
    'searchsorted',
    # 31
    'cummax',
    'cummin',
    'einsum',
    'sub',
    # 33
    'split',
    # 34

    # 35
    'erfinv',
    # 36

    # 37
    'nonzero',
    # 38

    # 39

    # 40
    'any',
    # 41
    'add',
    # 42
    'argmax',
    # 43
    'cat',
    # 44
    'cos',
    # 45
    'concat',
    # 46
    'bitwise_and',
    'bitwise_or',
    'bitwise_xor',
    # 47
    'max',
    # 48
    'min',
    # 49
    'baddbmm',
    # 50
    'tile',
    # 51
    'permute',
    # 52
    'addcdiv',
    # 53

    # 54
    'normal',
    # 55
    'cross',
    # 56
    'norm',
    # 57
    'broadcast_to',
    # 58
    'greater_equal',
    # 59
    'square',
    # 60
    'all',
    # 61
    'rsqrt',
    # 62
    'maximum',
    # 63
    'minimum',
    # 64
    'ravel',
    # 65
    'logical_and',
    # 66
    'logical_not',
    # 67
    'logical_or',
    # 68
    'logical_xor',
    # 69
    'less_equal',
    'le',
    # 70
    'negative',
    'neg',
    # 71
    'isfinite',
    # 72

    # 73
    'ceil',
    # 74
    'sort',
    # 75
    'less',
    'lt',
    # 76
    'pow',
    # 77

    # 78
    'arange',

    # 79

    # 80

    # 81
    'index_select',
    # 82

    # 83
    'narrow',
    # 84
    'masked_select',

    # 86
    'select',

    # 87

    # 88
    'chunk',
    # 89
    'argsort',
    # 90
    'isinf',
    # 91

    # 92
    'polar',
    # 93

    # 94
    'tanh',
    # 95

    # 96

    # 97

    # 98

    # 99

    # 100

    # 101

    # 102

    # 103

    # 104

    # 105

    # 106

    # 107

    # 108

    # 109
    'argmin',
    # 110
    'softmax',
    # 111

    # 112

    # 113

    # 114

    # 115

    # 116

    # 117

    # 118

    # 119

    # 120
    'isneginf',
    # 121

    # 122

    # 123
    'var',

    # 151
    'acos',
    'arccos',
    # 152
    'acosh',
    'arccosh',
    # 153

    # 154

    # 155

    # 156

    # 157
    'scatter',
    # 172
    'asin',
    'arcsin',
    # 173
    'asinh',
    'arcsinh',
    # 174
    'atan',
    'arctan',
    # 175
    'atanh',
    'arctanh',
    # 176
    'atan2',
    'arctan2',

    # 177
    'round',

    # 182
    'bernoulli',
    # 201
    'diag',
    # 207
    'expm1',
    # 204
    'erfc',
    # 208
    'eye',
    # 239
    'lerp',
    # 256
    'median',
    'randperm',
    'rand',
    'rand_like',
    'randn',
    'randn_like',
    'randint',
    'randint_like',
    # 210
    'floor',
    # 231
    'inverse',
    # 244
    'log1p',
    # 261
    'multinomial',
    # 275
    'remainder',
    # 285
    'scatter_add',
    # 289
    'sign',
    # 301
    'tan',
    # 303
    'trace',
    'gcd',
    'reshape',
    'outer',
    # 304
    'tril',

    # 305
    'triu',

    # 308
    'mm',

    # 382
    'dstack',

    # 406
    'allclose',

    # 501
    'addbmm',

    # 502
    'addmm',

    # 505
    'addmv',

    # 510
    'amax',

    # 511
    'amin',

    # 520
    'bincount',

    # 521
    'bitwise_not',

    # 526
    'dot',

    # 533
    'frac',

    # 538
    'histc',

    # 549
    'kthvalue',

    # 552
    'log10',

    # 553
    'logaddexp',
    'logaddexp2',

    # 557
    'logsumexp',

    # 582
    'std_mean',

    # 584
    'take',

    # 588
    'var_mean',

    # 586
    'unique_consecutive',

    # 610
    'nan_to_num',

    # 613
    'nansum',

    # 615
    'triangular_solve',

    # 664
    'meshgrid',

    # 695
    'count_nonzero',

    # 697
    'float_power',

    # 708
    'std',

    # 739
    'hstack',

    # 826
    'floor_divide',

    # 887
    'log2',

    # 889
    'isnan',

    # 890

    # 891

    # 892

    # 893

    # 894

    # 895

    # 896

    # 897

    # 898

    # 899

    # 900

    # 916
    'index_add',

    # 1007
    't',

    # 1023
    'unbind',

    # 1100
    'diff',
]

__all__.extend(functional.__all__)
__all__.extend(nn.__all__)
__all__.extend(optim.__all__)
__all__.extend(linalg.__all__)
__all__.extend(special.__all__)
__all__.extend(distributed.__all__)

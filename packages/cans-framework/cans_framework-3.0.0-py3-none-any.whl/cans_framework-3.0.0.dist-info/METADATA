Metadata-Version: 2.4
Name: cans-framework
Version: 3.0.0
Summary: A production-ready deep learning framework for causal inference on structured, textual, and heterogeneous data
Home-page: https://github.com/rdmurugan/cans-framework
Author: Durai Rajamanickam
Author-email: Durai Rajamanickam <durai@infinidatum.net>
Maintainer-email: Durai Rajamanickam <durai@infinidatum.net>
License: MIT License
        
        Copyright (c) 2024 Durai Rajamanickam
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
Project-URL: Homepage, https://github.com/rdmurugan/cans-framework
Project-URL: Documentation, https://github.com/rdmurugan/cans-framework#readme
Project-URL: Repository, https://github.com/rdmurugan/cans-framework
Project-URL: Bug Reports, https://github.com/rdmurugan/cans-framework/issues
Project-URL: Changelog, https://github.com/rdmurugan/cans-framework/releases
Keywords: causal-inference,deep-learning,graph-neural-networks,transformers,counterfactual,treatment-effects,machine-learning,pytorch,bert,gnn,causal-ai,econometrics,statistics
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Education
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Operating System :: OS Independent
Classifier: Environment :: Console
Classifier: Environment :: GPU :: NVIDIA CUDA
Classifier: Natural Language :: English
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.38.0
Requires-Dist: torch-geometric>=2.3.0
Requires-Dist: scikit-learn>=1.3.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: tqdm>=4.66.0
Requires-Dist: networkx>=3.1
Requires-Dist: matplotlib>=3.7.0
Requires-Dist: seaborn>=0.12.0
Requires-Dist: plotly>=5.15.0
Requires-Dist: scipy>=1.11.0
Requires-Dist: statsmodels>=0.14.0
Requires-Dist: PyYAML>=6.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10.0; extra == "dev"
Requires-Dist: pytest-xdist>=3.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.990; extra == "dev"
Requires-Dist: pre-commit>=2.20.0; extra == "dev"
Requires-Dist: bandit>=1.7.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=5.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Requires-Dist: sphinxcontrib-napoleon>=0.7; extra == "docs"
Requires-Dist: myst-parser>=0.18.0; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=1.19.0; extra == "docs"
Provides-Extra: notebooks
Requires-Dist: jupyter>=1.0.0; extra == "notebooks"
Requires-Dist: jupyterlab>=3.4.0; extra == "notebooks"
Requires-Dist: ipykernel>=6.15.0; extra == "notebooks"
Requires-Dist: ipywidgets>=8.0.0; extra == "notebooks"
Requires-Dist: nbconvert>=7.0.0; extra == "notebooks"
Provides-Extra: visualization
Requires-Dist: plotly>=5.15.0; extra == "visualization"
Requires-Dist: seaborn>=0.12.0; extra == "visualization"
Requires-Dist: matplotlib>=3.7.0; extra == "visualization"
Requires-Dist: graphviz>=0.20.0; extra == "visualization"
Provides-Extra: gpu
Requires-Dist: torch>=2.0.0; extra == "gpu"
Requires-Dist: torch-geometric>=2.3.0; extra == "gpu"
Provides-Extra: all
Requires-Dist: cans-framework[dev,docs,notebooks,visualization]; extra == "all"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# ğŸ§  CANS: Causal Adaptive Neural System

**CANS (Causal Adaptive Neural System)** is a **production-ready** deep learning framework for **causal inference** on structured, textual, and heterogeneous data. It seamlessly integrates **Graph Neural Networks (GNNs)**, **Transformers (e.g., BERT)**, a **Gated Fusion mechanism**, and **Counterfactual Regression Networks (CFRNet)**.

Initially developed for misinformation propagation on social networks, CANS generalizes to domains like **healthcare**, **legal**, and **finance**, offering a robust, well-tested pipeline for real-world causal modeling and counterfactual simulation.

## ğŸš€ What's New in v3.0 - Enhanced Causal Inference

- ğŸ”¬ **Causal Assumption Testing**: Automated testing of unconfoundedness, positivity, and SUTVA
- ğŸ¯ **Multiple Identification Strategies**: Backdoor criterion, IPW, doubly robust estimation
- ğŸ“Š **CATE Estimation**: X-Learner, T-Learner, S-Learner, Neural CATE, Causal Forest
- ğŸ² **Uncertainty Quantification**: Bayesian methods, ensemble approaches, conformal prediction
- ğŸ—ï¸ **Advanced Graph Construction**: Multi-node, temporal, and global graph architectures
- ğŸ”„ **Causal-Specific Losses**: CFR, IPW, DragonNet, TARNet with representation balancing
- ğŸ“ˆ **Comprehensive Evaluation**: PEHE, policy evaluation, calibration metrics
- ğŸ’¾ **Memory-Efficient Processing**: Lazy loading and batch processing for large datasets

### Previous v2.0 Features:
- ğŸ”§ **Configuration Management**: Centralized, validated configs with JSON/YAML support
- ğŸ›¡ï¸ **Enhanced Error Handling**: Comprehensive validation with informative error messages  
- ğŸ“Š **Logging & Checkpointing**: Built-in experiment tracking with automatic model saving
- ğŸ§ª **Comprehensive Testing**: 100+ unit tests ensuring production reliability
- ğŸ“ˆ **Advanced Data Pipeline**: Multi-format loading (CSV, JSON) with automatic preprocessing
- âš¡ **Enhanced Training**: Early stopping, gradient clipping, multiple loss functions

## ğŸ”§ Key Features

### Core Architecture
- âœ… **Hybrid Neural Architecture**: GNNs + Transformers + CFRNet for multi-modal causal inference
- âœ… **Gated Fusion Layer**: Adaptive mixing of graph and textual representations
- âœ… **Flexible Graph Construction**: Single-node, multi-node, temporal, and global graphs
- âœ… **Production-Ready**: Comprehensive error handling, logging, and testing

### Causal Inference Capabilities  
- âœ… **Rigorous Assumption Testing**: Automated validation of causal identification conditions
- âœ… **Multiple Identification Methods**: Backdoor, IPW, doubly robust, with sensitivity analysis
- âœ… **Heterogeneous Treatment Effects**: CATE estimation with 5+ methods (X/T/S-Learners, etc.)
- âœ… **Advanced Loss Functions**: CFR, DragonNet, TARNet with representation balancing
- âœ… **Uncertainty Quantification**: Bayesian, ensemble, conformal prediction approaches

### Data Processing & Evaluation
- âœ… **Smart Data Loading**: CSV, JSON, synthetic data with automatic graph construction
- âœ… **Comprehensive Evaluation**: PEHE, ATE, policy value, calibration metrics
- âœ… **Memory Efficiency**: Lazy loading, batch processing for large-scale datasets
- âœ… **Easy Configuration**: JSON/YAML configs with validation and experiment tracking



## ğŸ—ï¸ Architecture

```
 +-----------+     +-----------+
 |  GNN Emb  |     |  BERT Emb |
 +-----------+     +-----------+
        \             /
         \ Fusion Layer /
          \     /
         +-----------+
         |  Fused Rep |
         +-----------+
               |
           CFRNet
        /          \
   mu_0(x)       mu_1(x)
```

## ğŸš€ Enhanced Causal Analysis Workflow

### Complete Example with Assumption Testing & CATE Estimation

```python
from cans import (
    CANSConfig, CANS, GCN, CANSRunner,
    create_sample_dataset, get_data_loaders,
    CausalAssumptionTester, CausalLossManager, 
    CATEManager, UncertaintyManager,
    advanced_counterfactual_analysis
)

# 1. Configuration with enhanced causal features
config = CANSConfig()
config.model.gnn_type = "GCN"
config.training.loss_type = "cfr"  # Causal loss function
config.data.graph_construction = "global"  # Multi-node graphs

# 2. Test causal assumptions BEFORE modeling
assumption_tester = CausalAssumptionTester()
results = assumption_tester.comprehensive_test(X, T, Y)
print(f"Causal assumptions valid: {results['causal_identification_valid']}")

# 3. Create datasets with enhanced graph construction
datasets = create_sample_dataset(n_samples=1000, config=config.data)
train_loader, val_loader, test_loader = get_data_loaders(datasets)

# 4. Setup model with causal loss functions
from transformers import BertModel
gnn = GCN(in_dim=64, hidden_dim=128, output_dim=256)
bert = BertModel.from_pretrained("distilbert-base-uncased")
model = CANS(gnn, bert, fusion_dim=256)

loss_manager = CausalLossManager("cfr", alpha=1.0, beta=0.5)

# 5. Train with causal-aware pipeline
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
runner = CANSRunner(model, optimizer, config)
history = runner.fit(train_loader, val_loader)

# 6. Multiple counterfactual identification methods
cf_results = advanced_counterfactual_analysis(
    model, test_loader, 
    methods=['backdoor', 'ipw', 'doubly_robust']
)

# 7. CATE estimation with multiple learners
cate_manager = CATEManager(method="x_learner")
cate_manager.fit(X, T, Y)
individual_effects = cate_manager.estimate_cate(X_test)

# 8. Uncertainty quantification
uncertainty_manager = UncertaintyManager(method="conformal")
uncertainty_manager.setup(model)
intervals = uncertainty_manager.estimate_uncertainty(test_loader)

print(f"ATE: {cf_results['backdoor']['ate']:.3f}")
print(f"Coverage: {intervals['coverage_rate']:.3f}")
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/rdmurugan/cans-framework.git
cd cans-framework

# Install dependencies
pip install -r cans/requirements.txt.rtf
```

**Core Dependencies:**
- `torch>=2.0.0`
- `transformers>=4.38.0`
- `torch-geometric>=2.3.0`
- `scikit-learn>=1.3.0`
- `pandas>=2.0.0`

### Basic Usage (30 seconds to results)

```python
from cans.config import CANSConfig
from cans.utils.data import create_sample_dataset, get_data_loaders
from cans.models import CANS
from cans.models.gnn_modules import GCN
from cans.pipeline.runner import CANSRunner
from transformers import BertModel
import torch

# 1. Create configuration
config = CANSConfig()
config.training.epochs = 10

# 2. Load data (or create sample data)
datasets = create_sample_dataset(n_samples=1000, n_features=64)
train_loader, val_loader, test_loader = get_data_loaders(datasets, batch_size=32)

# 3. Create model
gnn = GCN(in_dim=64, hidden_dim=128, output_dim=256)
bert = BertModel.from_pretrained("bert-base-uncased")
model = CANS(gnn, bert, fusion_dim=256)

# 4. Train
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
runner = CANSRunner(model, optimizer, config)
history = runner.fit(train_loader, val_loader)

# 5. Evaluate
results = runner.evaluate(test_loader)
print(f"Test MSE: {results['mse']:.4f}")
print(f"Average Treatment Effect: {results['ate']:.4f}")
```

## ğŸ“Š Usage Examples

### Example 1: CSV Data with Real Causal Inference

```python
from cans.utils.data import load_csv_dataset
from cans.config import CANSConfig, DataConfig

# Configure data processing
config = CANSConfig()
config.data.graph_construction = "knn"  # or "similarity" 
config.data.knn_k = 5
config.data.scale_node_features = True

# Load your CSV data
datasets = load_csv_dataset(
    csv_path="your_data.csv",
    text_column="review_text",        # Column with text data
    treatment_column="intervention",   # Binary treatment (0/1)
    outcome_column="conversion_rate",  # Continuous outcome  
    feature_columns=["age", "income", "education"],  # Numerical features
    config=config.data
)

train_dataset, val_dataset, test_dataset = datasets

# Check data quality
stats = train_dataset.get_statistics()
print(f"Treatment proportion: {stats['treatment_proportion']:.3f}")
print(f"Propensity overlap valid: {stats['propensity_overlap_valid']}")
```

### Example 2: Advanced Configuration & Experiment Tracking

```python
from cans.config import CANSConfig

# Create detailed configuration
config = CANSConfig()

# Model configuration
config.model.gnn_type = "GCN"
config.model.gnn_hidden_dim = 256
config.model.fusion_dim = 512
config.model.text_model = "distilbert-base-uncased"  # Faster BERT variant

# Training configuration  
config.training.learning_rate = 0.001
config.training.batch_size = 64
config.training.epochs = 50
config.training.early_stopping_patience = 10
config.training.gradient_clip_norm = 1.0
config.training.loss_type = "huber"  # Robust to outliers

# Experiment tracking
config.experiment.experiment_name = "healthcare_causal_analysis"
config.experiment.save_every_n_epochs = 5
config.experiment.log_level = "INFO"

# Save configuration for reproducibility
config.save("experiment_config.json")

# Later: load and use
loaded_config = CANSConfig.load("experiment_config.json")
```

### Example 3: Counterfactual Analysis & Treatment Effects

```python
from cans.utils.causal import simulate_counterfactual
import numpy as np

# After training your model...
runner = CANSRunner(model, optimizer, config)
runner.fit(train_loader, val_loader)

# Comprehensive evaluation
test_metrics = runner.evaluate(test_loader)
print("Performance Metrics:")
for metric, value in test_metrics.items():
    print(f"  {metric}: {value:.4f}")

# Counterfactual analysis
cf_control = simulate_counterfactual(model, test_loader, intervention=0)
cf_treatment = simulate_counterfactual(model, test_loader, intervention=1)

# Calculate causal effects
ate = np.mean(cf_treatment) - np.mean(cf_control)
print(f"\nCausal Analysis:")
print(f"Average Treatment Effect (ATE): {ate:.4f}")
print(f"Expected outcome under control: {np.mean(cf_control):.4f}")
print(f"Expected outcome under treatment: {np.mean(cf_treatment):.4f}")

# Individual treatment effects
individual_effects = np.array(cf_treatment) - np.array(cf_control)
print(f"Treatment effect std: {np.std(individual_effects):.4f}")
print(f"% benefiting from treatment: {(individual_effects > 0).mean()*100:.1f}%")
```

### Example 4: Custom Data Pipeline

```python
from cans.utils.preprocessing import DataPreprocessor, GraphBuilder
from cans.config import DataConfig
import pandas as pd

# Custom preprocessing pipeline
config = DataConfig()
config.graph_construction = "similarity"
config.similarity_threshold = 0.7
config.scale_node_features = True

preprocessor = DataPreprocessor(config)

# Process your DataFrame  
df = pd.read_csv("social_media_posts.csv")
dataset = preprocessor.process_tabular_data(
    data=df,
    text_column="post_content",
    treatment_column="fact_check_label",
    outcome_column="share_count",
    feature_columns=["user_followers", "post_length", "sentiment_score"],
    text_model="bert-base-uncased",
    max_text_length=256
)

# Split with custom ratios
train_ds, val_ds, test_ds = preprocessor.split_dataset(
    dataset, 
    train_size=0.7, 
    val_size=0.2, 
    test_size=0.1
)
```

## ğŸ§ª Testing & Development

```bash
# Run all tests
pytest tests/ -v

# Run specific test categories  
pytest tests/test_models.py -v        # Model tests
pytest tests/test_validation.py -v    # Validation tests
pytest tests/test_pipeline.py -v      # Training pipeline tests

# Run with coverage
pytest tests/ --cov=cans --cov-report=html

# Run example scripts
python examples/enhanced_usage_example.py
python examples/enhanced_causal_analysis_example.py
```


## ğŸ“ Framework Structure

```
cans-framework/
â”œâ”€â”€ cans/
â”‚   â”œâ”€â”€ __init__.py              # Main imports
â”‚   â”œâ”€â”€ config.py                # âœ¨ Configuration management
â”‚   â”œâ”€â”€ exceptions.py            # âœ¨ Custom exceptions
â”‚   â”œâ”€â”€ validation.py            # âœ¨ Data validation utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ cans.py             # Core CANS model (enhanced)
â”‚   â”‚   â””â”€â”€ gnn_modules.py      # GNN implementations
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ runner.py           # âœ¨ Enhanced training pipeline
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ causal.py           # Counterfactual simulation
â”‚       â”œâ”€â”€ data.py             # âœ¨ Enhanced data loading
â”‚       â”œâ”€â”€ preprocessing.py     # âœ¨ Advanced preprocessing
â”‚       â”œâ”€â”€ logging.py          # âœ¨ Structured logging
â”‚       â””â”€â”€ checkpointing.py    # âœ¨ Model checkpointing
â”œâ”€â”€ tests/                       # âœ¨ Comprehensive test suite
â”œâ”€â”€ examples/                    # Usage examples
â””â”€â”€ CLAUDE.md                   # Development guide
```
**âœ¨ = New/Enhanced in v2.0**

## ğŸ¯ Use Cases & Applications

### Healthcare & Medical
```python
# Analyze treatment effectiveness with patient records + clinical notes
datasets = load_csv_dataset(
    csv_path="patient_outcomes.csv",
    text_column="clinical_notes",
    treatment_column="medication_type", 
    outcome_column="recovery_score",
    feature_columns=["age", "bmi", "comorbidities"]
)
```

### Marketing & A/B Testing  
```python
# Marketing campaign effectiveness with customer profiles + ad content
datasets = load_csv_dataset(
    csv_path="campaign_data.csv", 
    text_column="ad_content",
    treatment_column="campaign_variant",
    outcome_column="conversion_rate",
    feature_columns=["customer_ltv", "demographics", "behavior_score"]
)
```

### Social Media & Content Moderation
```python  
# Impact of content moderation on engagement
datasets = load_csv_dataset(
    csv_path="posts_data.csv",
    text_column="post_text", 
    treatment_column="moderation_action",
    outcome_column="engagement_score",
    feature_columns=["user_followers", "post_length", "sentiment"]
)
```

## ğŸ”¬ Research & Methodology

CANS implements state-of-the-art causal inference techniques:

- **Counterfactual Regression Networks (CFRNet)**: Learn representations that minimize treatment assignment bias
- **Gated Fusion**: Adaptively combine graph-structured and textual information  
- **Balanced Representation**: Minimize distributional differences between treatment groups
- **Propensity Score Validation**: Automatic overlap checking for reliable causal estimates

**Key Papers:**
- Shalit et al. "Estimating individual treatment effect: generalization bounds and algorithms" (ICML 2017)
- Yao et al. "Representation learning for treatment effect estimation from observational data" (NeurIPS 2018)

## ğŸš€ Performance & Scalability

- **Memory Efficient**: Optimized batch processing and gradient checkpointing
- **GPU Acceleration**: Full CUDA support with automatic device selection
- **Parallel Processing**: Multi-core data loading and preprocessing
- **Production Ready**: Comprehensive error handling and logging

**Benchmarks** (approximate, hardware-dependent):
- **Small**: 1K samples, 32 features â†’ ~30 sec training  
- **Medium**: 100K samples, 128 features â†’ ~10 min training
- **Large**: 1M+ samples â†’ Scales with batch size and hardware

## ğŸ“š Additional Resources

- **Documentation**: See `CLAUDE.md` for detailed development guide
- **Examples**: Check `examples/` directory for complete workflows
- **Tests**: `tests/` contains 100+ unit tests demonstrating usage
- **Issues**: Report bugs and feature requests on GitHub

## ğŸ¤ Contributing

Contributions welcome! Please:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Add tests** for new functionality  
4. **Run tests**: `pytest tests/ -v`
5. **Submit** a pull request

Areas we'd love help with:
- Additional GNN architectures (GraphSAGE, Graph Transformers)
- More evaluation metrics for causal inference
- Integration with popular ML platforms (MLflow, Weights & Biases)
- Performance optimizations

## ğŸ‘¨â€ğŸ”¬ Authors

**Durai Rajamanickam** â€“ [@duraimuruganr](https://github.com/rdmurugan)
reach out to durai@infinidatum.net

## ğŸ“œ License

MIT License. Free to use, modify, and distribute with attribution.

---

**Ready to get started?** Try the 30-second quick start above, or dive into the detailed examples! ğŸš€


llm:
    default:
        provider: openai
        model: gpt-4o-mini
        # apiKey: 'sk-...' # Highly recommended to set via environment variable, not hard-coded in the config file
        apiKeyEnvVar: 'OPENAI_API_KEY' # Set it in your environment variables or .env file
        # baseUrl: 'your-custom-base-url'
        baseUrlEnvVar: 'OPENAI_BASE_URL' # Set it in your environment variables or .env file
        ollamaBaseUrl: 'http://localhost:11434' # Only needed when provider is ollama
        temperature: 0.7

    commands: # Can override the default settings for each command
        commit:
            provider: openai
            model: gpt-4o-mini
            apiKeyEnvVar: 'OPENAI_API_KEY'
            baseUrlEnvVar: 'OPENAI_BASE_URL'
        pr:
            provider: deepseek    # Can be changed to 'deepseek' or 'ollama' ...
            model: deepseek-chat
            apiKeyEnvVar: 'DEEPSEEK_API_KEY'
            baseUrlEnvVar: 'DEEPSEEK_BASE_URL'
            temperature: 0.4

commit:
    suggestions: 3
    prompt_template: './prompts/commit_prompt.txt'

pr:
    base_branch: main          # When user doesn't pass --target
    include_file_tree: true    # Whether to include the file tree generated by ls-files
    include_unstaged: false    # Corresponds to --unstaged
    max_lines_per_file: 300    # Diff truncation threshold
    warn_on_conflict: true     # Whether to interrupt when merge conflicts are detected
    prompt_template: ./prompts/pr_prompt.txt

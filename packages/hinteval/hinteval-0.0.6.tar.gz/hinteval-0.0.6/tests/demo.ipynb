{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HintEval\n",
        "\n",
        "HintEval is a powerful framework designed for both generating and evaluating hints. These hints serve as subtle clues, guiding users toward the correct answer without directly revealing it. As the first tool of its kind, HintEval allows users to create and assess hints from various perspectives."
      ],
      "metadata": {
        "id": "QWdRa8FoAgRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKrSHAyXAY2Z"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n",
        "!pip install hinteval\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ **Important:**\n",
        "After installing the dependencies above, you need to **restart the session** in Google Colab.\n",
        "\n",
        "Go to the menu:\n",
        "`Runtime` → `Restart session`\n",
        "\n",
        "After restarting, re-run the cells below to continue."
      ],
      "metadata": {
        "id": "Kbs6-BJ5AwwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a Synthetic Hint Dataset\n",
        "\n",
        "This tutorial provides step-by-step guidance on how to generate a synthetic hint dataset using large language models via the TogetherAI platform. To proceed, make sure you have an active API key for TogetherAI.\n"
      ],
      "metadata": {
        "id": "Kbs6-BJ5AwwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pipeline Argument\n",
        "\n",
        "api_key = '' # @param {type:\"string\"}\n",
        "base_url = 'https://api.together.xyz/v1' # @param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D7J71wTGA9Ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question/Answer Pairs\n",
        "\n",
        "First, we'll gather a collection of question/answer pairs to use as a foundation for generating `Question/Answer/Hint` triples. We'll load 10 questions from the [WebQuestions](https://aclanthology.org/D13-1160.pdf) dataset using the [🤗datasets](https://pypi.org/project/datasets/) library.\n"
      ],
      "metadata": {
        "id": "If5e1C1EBDmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "webq = load_dataset(\"Stanford/web_questions\", split='test')\n",
        "question_answers = webq.select_columns(['question', 'answers'])[10:20]\n",
        "qa_pairs = zip(question_answers['question'], question_answers['answers'])"
      ],
      "metadata": {
        "id": "zXPcR0SJBG5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a set of question/answer pairs ready for creating synthetic `Question/Answer/Hint` instances.\n"
      ],
      "metadata": {
        "id": "bBNYYQ8EBJPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Creation\n",
        "\n",
        "Next, we'll use HintEval's `Dataset` class to create a new dataset called `synthetic_hint_dataset`, which will include 10 question/answer pairs within a subset named `entire`.\n"
      ],
      "metadata": {
        "id": "6SoPRnvmBOMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hinteval import Dataset\n",
        "from hinteval.cores import Subset, Instance\n",
        "\n",
        "dataset = Dataset('synthetic_hint_dataset')\n",
        "subset = Subset('entire')\n",
        "\n",
        "for q_id, (question, answers) in enumerate(qa_pairs, 1):\n",
        "    instance = Instance.from_strings(question, answers, [])\n",
        "    subset.add_instance(instance, f'id_{q_id}')\n",
        "\n",
        "dataset.add_subset(subset)\n",
        "dataset.prepare_dataset(fill_question_types=True)"
      ],
      "metadata": {
        "id": "rFBUZczfBQup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hint Generation\n",
        "\n",
        "Now, we can generate 5 hints for each question using HintEval's `AnswerAware` model. For this example, we will use the [Meta LLaMA-3.1-70b-Instruct-Turbo](https://www.llama.com/) model from TogetherAI.\n"
      ],
      "metadata": {
        "id": "yrywvCyIBWAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hinteval.model import AnswerAware\n",
        "\n",
        "generator = AnswerAware('meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
        "                        api_key, base_url, num_of_hints=5, enable_tqdm=True)\n",
        "generator.generate(dataset['entire'].get_instances())\n",
        "pass"
      ],
      "metadata": {
        "id": "z1R9BYRIBd9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Depending on the LLM provider you are using, you may need to configure the `model` and other parameters in the `AnswerAware` function. See the [Model](https://hinteval.readthedocs.io/en/latest/concepts/model.html) and [References](https://hinteval.readthedocs.io/en/latest/references/model.html) for more information.\n"
      ],
      "metadata": {
        "id": "sTalhGFhBi8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the Dataset\n",
        "\n",
        "Once the hints are generated, we can export the synthetic hint dataset to a pickle file."
      ],
      "metadata": {
        "id": "Z8fYA02PBoef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.store('./synthetic_hint_dataset.pickle')"
      ],
      "metadata": {
        "id": "-ez3qE_TBsJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the Hints\n",
        "\n",
        "Finally, let's view the hints generated for the third question in the dataset."
      ],
      "metadata": {
        "id": "K5CqOcGfBuwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.load('./synthetic_hint_dataset.pickle')\n",
        "\n",
        "third_question = dataset['entire'].get_instance('id_3')\n",
        "print(f'Question: {third_question.question.question}')\n",
        "print(f'Answer: {third_question.answers[0].answer}')\n",
        "print()\n",
        "for idx, hint in enumerate(third_question.hints, 1):\n",
        "    print(f'Hint {idx}: {hint.hint}')\n"
      ],
      "metadata": {
        "id": "oSNW1PUxBxN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example output:\n",
        "\n",
        "```\n",
        "Question: who is governor of ohio 2011?\n",
        "Answer: John Kasich\n",
        "\n",
        "Hint 1: The answer is a Republican politician who served as the 69th governor of the state.\n",
        "Hint 2: This person was a member of the U.S. House of Representatives for 18 years before becoming governor.\n",
        "Hint 3: The governor was known for his conservative views and efforts to reduce government spending.\n",
        "Hint 4: During their term, they implemented several reforms related to education, healthcare, and the economy.\n",
        "Hint 5: This governor served two consecutive terms, from 2011 to 2019, and ran for the U.S. presidency in 2016.\n",
        "```\n",
        "\n",
        "Because of the generative nature of large language models, the output hints may vary."
      ],
      "metadata": {
        "id": "z7FEWLXmBzjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Your Hint Dataset\n",
        "\n",
        "Once your hint dataset is ready—whether you've created your own or used the synthetic dataset generation module—it's time to evaluate the hints. This guide will help you set up HintEval quickly so you can focus on improving your hint generation pipelines.\n",
        "\n",
        "> By default, these metrics use TogetherAI's API to compute the score. If you prefer to run them locally, you can set `api_key` to `None`. You can also try other platforms by changing the `base_url` accordingly.\n",
        "\n",
        "Let's start by loading the data.\n"
      ],
      "metadata": {
        "id": "T47u-4bkCFGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Data\n",
        "\n",
        "For this tutorial, we'll use the synthetic dataset generated in the synthetic dataset generation module. Alternatively, you can load a preprocessed dataset using the [Dataset.download_and_load_dataset()](https://hinteval.readthedocs.io/en/latest/references/dataset.html#hinteval.cores.dataset.dataset.Dataset.download_and_load_dataset) function. Each subset of the dataset includes a number of `Instance` objects, each of which contains:\n",
        "- **question**: The question to evaluate along with its answers and hints.\n",
        "- **answers**: A list of correct answers for the question.\n",
        "- **hints**: A list of hints to help arrive at the correct answers."
      ],
      "metadata": {
        "id": "SuFr6yjMCSNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hinteval import Dataset\n",
        "\n",
        "dataset = Dataset.load('./synthetic_hint_dataset.pickle')"
      ],
      "metadata": {
        "id": "J799iqeuCUf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "HintEval provides several metrics to evaluate different aspects of the hints:\n",
        "\n",
        "1. **Relevance**: Measures how relevant the hints are to the question.\n",
        "2. **Readability**: Assesses the readability of the hints and questions.\n",
        "3. **Convergence**: Evaluates how well the hints narrow down potential answers to the question.\n",
        "4. **Familiarity**: Measures how common or well-known the information in the hints is.\n",
        "5. **AnswerLeakage**: Determines how much the hints reveal the correct answers.\n",
        "\n",
        "Each metric contains various methods, which you can explore in the [metrics guide](https://hinteval.readthedocs.io/en/latest/concepts/metrics/index.html).\n",
        "\n",
        "Let's import the metrics:"
      ],
      "metadata": {
        "id": "QU_fdXxOCWtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hinteval.evaluation.relevance import Rouge\n",
        "from hinteval.evaluation.readability import MachineLearningBased\n",
        "from hinteval.evaluation.convergence import LlmBased\n",
        "from hinteval.evaluation.familiarity import WordFrequency\n",
        "from hinteval.evaluation.answer_leakage import ContextualEmbeddings"
      ],
      "metadata": {
        "id": "SY1QqCIECcwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we’re using five metrics, but what do they represent?\n",
        "\n",
        "1. **Relevance (_Rouge_)**: Measures the relevance of the hints to the question using ROUGE-L algorithms.\n",
        "2. **Readability (_MachineLearningBased_)**: Uses a Random Forest algorithm to measure the readability of the hints and questions.\n",
        "3. **Convergence (_LLmBased_)**: Assesses how well the hints help eliminate incorrect answers using the Meta LLaMA-3.1-70b-Instruct-Turbo model.\n",
        "4. **Familiarity (_WordFrequency_)**: Evaluates the familiarity of the information in the hints, questions, and answers based on word occurancy.\n",
        "5. **AnswerLeakage (_ContextualEmbeddings_)**: Measures how much the hints reveal the answers by calculating the similarity between the hints and answers using contextual embeddings.\n",
        "\n",
        "To explore other metrics, check the [metrics guide](https://hinteval.readthedocs.io/en/latest/concepts/metrics/index.html)."
      ],
      "metadata": {
        "id": "kBHEua08CgKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "First, extract the `question`, `hints`, and `answers` from each instance in the dataset and store them in separate lists.\n"
      ],
      "metadata": {
        "id": "IJf5apmzCifL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instances = dataset['entire'].get_instances()\n",
        "questions = [instance.question for instance in instances]\n",
        "answers = []\n",
        "[answers.extend(instance.answers) for instance in instances]\n",
        "hints = []\n",
        "[hints.extend(instance.hints) for instance in instances]\n",
        "pass"
      ],
      "metadata": {
        "id": "zDkIcGGeCkY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the dataset, call the `evaluate` method for each metric.\n"
      ],
      "metadata": {
        "id": "LsGKx5r4Cmrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Rouge('rougeL', enable_tqdm=True).evaluate(instances)\n",
        "MachineLearningBased('random_forest', enable_tqdm=True).evaluate(questions + hints)\n",
        "LlmBased('llama-3-70b', together_ai_api_key=api_key, enable_tqdm=True).evaluate(instances)\n",
        "WordFrequency(enable_tqdm=True).evaluate(questions + hints + answers)\n",
        "ContextualEmbeddings(enable_tqdm=True).evaluate(instances)\n",
        "pass"
      ],
      "metadata": {
        "id": "iz5i0wv6Cod6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it—all the evaluation scores you need.\n",
        "\n",
        "> Depending on the LLM provider you're using, you may need to configure parameters such as `model`, `max_tokens`, and `batch_size` based on the provider’s rate limits."
      ],
      "metadata": {
        "id": "K1tIv3IACqjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Viewing the Evaluation Metrics\n",
        "\n",
        "Finally, let's view the metrics evaluated for the second hint of third question in the dataset."
      ],
      "metadata": {
        "id": "hr_GFPfvFhU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "third_question = dataset['entire'].get_instance('id_3')\n",
        "second_hint = third_question.hints[1]\n",
        "\n",
        "print(f'Question: {third_question.question.question}')\n",
        "print(f'Answer: {third_question.answers[0].answer}')\n",
        "print(f'Second Hint: {second_hint.hint}')\n",
        "print()\n",
        "\n",
        "for metric in second_hint.metrics:\n",
        "    print(f'{metric}: {second_hint.metrics[metric].value}')"
      ],
      "metadata": {
        "id": "FhT9KaS8Ekuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the Results\n",
        "\n",
        "If you want to further analyze the results, you can export the evaluated dataset to a JSON file, including the scores."
      ],
      "metadata": {
        "id": "ds2WvZcQCu1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.store_json('./evaluated_synthetic_hint_dataset.json')"
      ],
      "metadata": {
        "id": "FYuyvIH_CwY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The evaluated scores and metrics are automatically stored in the dataset. Saving the dataset will also save the scores.\n"
      ],
      "metadata": {
        "id": "dOExvSvQCzhF"
      }
    }
  ]
}

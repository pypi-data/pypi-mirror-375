{"query": "experiment tracking", "matching_document": "## Track Experiments\n### How it works\nTrack a machine learning experiment with a few lines of code:\n1. Create a W&B run.\n2. Store a dictionary of hyperparameters, such as learning rate or model type, into your configuration (`wandb.config`).\n3. Log metrics (`wandb.log()`) over time in a training loop, such as accuracy and loss.\n4. Save outputs of a run, like the model weights or a table of predictions.  \n\nThe proceeding pseudocode demonstrates a common W&B Experiment tracking workflow:  \n\n```python showLineNumbers\n\n# 1. Start a W&B Run\n\nwandb.init(entity=\"\", project=\"my-project-name\")\n\n# 2. Save mode inputs and hyperparameters\n\nwandb.config.learning\\_rate = 0.01\n\n# Import model and data\n\nmodel, dataloader = get\\_model(), get\\_data()\n\n# Model training code goes here\n\n# 3. Log metrics over time to visualize performance\n\nwandb.log({\"loss\": loss})\n\n# 4. Log an artifact to W&B\n\nwandb.log\\_artifact(model)\n```", "query_id": "5e878c76-25c1-4bad-8cae-6a40ca4c8138", "label": "artifact", "labels": "other"}
{"query": "Bayesian optimization", "matching_document": "## Methods for Automated Hyperparameter Optimization\n### Bayesian Optimization\nBayesian optimization is a hyperparameter tuning technique that uses a surrogate function to determine the next set of hyperparameters to evaluate. In contrast to grid search and random search, Bayesian optimization is an informed search method.  \n\n### Inputs  \n\n* A set of hyperparameters you want to optimize\n* A continuous search space for each hyperparameter as a value range\n* A performance metric to optimize\n* Explicit number of runs: Because the search space is continuous, you must manually stop the search or define a maximum number of runs.  \n\nThe differences in grid search are highlighted in bold above.  \n\nA popular way to implement Bayesian optimization in Python is to use BayesianOptimization from the [bayes_opt](https://github.com/fmfn/BayesianOptimization) library. Alternatively, as shown below, you can set up Bayesian optimization for hyperparameter tuning with W&B.  \n\n### Steps  \n\n### Output  \n\n### Advantages  \n\n### Disadvantages", "query_id": "d7b77e8a-e86c-4953-bc9f-672618cdb751", "label": "other"}
{"query": "How to integrate Weights & Biases with PyTorch?", "matching_document": "## \ud83d\udd25 = W&B \u2795 PyTorch\n\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.  \n\n## What this notebook covers:  \n\nWe show you how to integrate Weights & Biases with your PyTorch code to add experiment tracking to your pipeline.  \n\n## The resulting interactive W&B dashboard will look like:  \n\n## In pseudocode, what we'll do is:  \n\n```\n# import the library\nimport wandb\n\n# start a new experiment\nwandb.init(project=\"new-sota-model\")\n\n#\u2003capture a dictionary of hyperparameters with config\nwandb.config = {\"learning\\_rate\": 0.001, \"epochs\": 100, \"batch\\_size\": 128}\n\n# set up model and data\nmodel, dataloader = get\\_model(), get\\_data()\n\n# optional: track gradients\nwandb.watch(model)\n\nfor batch in dataloader:\nmetrics = model.training\\_step()\n#\u2003log metrics inside your training loop to visualize model performance\nwandb.log(metrics)\n\n# optional: save model at the end\nmodel.to\\_onnx()\nwandb.save(\"model.onnx\")\n\n```  \n\n## Follow along with a video tutorial!", "query_id": "023fca2d-78cb-4cc1-ad11-1e054428a891", "label": "integrations"}
{"query": "model registry W&B", "matching_document": "## Register models\n### Model Registry\nAfter logging a bunch of checkpoints across multiple runs during experimentation, now comes time to hand-off the best checkpoint to the next stage of the workflow (e.g. testing, deployment).  \n\nThe Model Registry is a central page that lives above individual W&B projects. It houses **Registered Models**, portfolios that store \"links\" to the valuable checkpoints living in individual W&B Projects.  \n\nThe model registry offers a centralized place to house the best checkpoints for all your model tasks. Any `model` artifact you log can be \"linked\" to a Registered Model.  \n\n### Creating **Registered Models** and Linking through the UI  \n\n#### 1. Access your team's model registry by going the team page and selecting `Model Registry`  \n\n#### 2. Create a new Registered Model.  \n\n#### 3. Go to the artifacts tab of the project that holds all your model checkpoints  \n\n#### 4. Click \"Link to Registry\" for the model artifact version you want.  \n\n### Creating Registered Models and Linking through the **API**", "query_id": "188162c6-744e-4bd1-b7b7-9146fcb00068", "label": "other"}
{"query": "What is Weights & Biases?", "matching_document": "## What is W&B?\n\nWeights & Biases (W&B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models.  \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your models and data are tracked and versioned in a reliable system of record.  \n\nThis diagram outlines the relationship between W&B products.  \n\n**W&B Models** is a set of lightweight, interoperable tools for machine learning practitioners training and fine-tuning models.\n- Experiments: Machine learning experiment tracking\n- Model Registry: Manage production models centrally\n- Launch: Scale and automate workloads\n- Sweeps: Hyperparameter tuning and model optimization  \n\n**W&B Prompts** is for LLM debugging and monitoring, including usage of OpenAI's GPT API.", "query_id": "cb7b845c-ada3-4a58-a9fa-5b0dc875967a", "label": "other"}
{"query": "Weights & Biases integration OpenAI", "matching_document": "## OpenAI API\n\nUse the Weights & Biases OpenAI API integration to log requests, responses, token counts and model metadata with 1 line of code for all OpenAI models, including fine-tuned models.  \n\n:::info\nThe W&B autolog integration works with `openai <= 0.28.1`. Install the correct version of `openai` with `pip install openai==0.28.1`.\n:::  \n\n**Try in a Colab Notebook here \u2192**  \n\nWith just 1 line of code you can now automatically log inputs and outputs from the OpenAI Python SDK to Weights & Biases!  \n\nOnce you start logging your API inputs and outputs you can quickly evaluate the performance of difference prompts, compare different model settings (such as temperature), and track other usage metrics such as token usage.  \n\nTo get started, pip install the `wandb` library, then follow the steps below:  \n\n### 1. Import autolog and initialise it  \n\nFirst, import `autolog` from `wandb.integration.openai` and initialise it.  \n\n### 2. Call the OpenAI API  \n\n### 3. View your OpenAI API inputs and responses", "query_id": "4bd23890-f2f4-4965-8e36-4c61e1a806cc", "label": "integrations"}
{"query": "PyTorch Weights & Biases integration tutorial", "matching_document": "## \ud83d\udd25 = W&B \u2795 PyTorch\n\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.  \n\n## What this notebook covers:  \n\nWe show you how to integrate Weights & Biases with your PyTorch code to add experiment tracking to your pipeline.  \n\n## The resulting interactive W&B dashboard will look like:  \n\n## In pseudocode, what we'll do is:  \n\n```\n# import the library\nimport wandb\n\n# start a new experiment\nwandb.init(project=\"new-sota-model\")\n\n#\u2003capture a dictionary of hyperparameters with config\nwandb.config = {\"learning\\_rate\": 0.001, \"epochs\": 100, \"batch\\_size\": 128}\n\n# set up model and data\nmodel, dataloader = get\\_model(), get\\_data()\n\n# optional: track gradients\nwandb.watch(model)\n\nfor batch in dataloader:\nmetrics = model.training\\_step()\n#\u2003log metrics inside your training loop to visualize model performance\nwandb.log(metrics)\n\n# optional: save model at the end\nmodel.to\\_onnx()\nwandb.save(\"model.onnx\")\n\n```  \n\n## Follow along with a video tutorial!", "query_id": "cbbf4fe7-c053-4d54-bc84-e3bf2d6e9f7f", "label": "integrations"}
{"query": "Best practices for managing multiple runs in WANDB.", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "cf7112a4-5fe1-459d-ba6b-42ef8bb4a2d4", "label": "other"}
{"query": "How to integrate YOLOv5 with Weights & Biases?", "matching_document": "## \ud83d\udce6 YOLOv5 +\ud83e\ude84 W&B\n\nTo install YOLOv5 with W&B integration all we need to is simply [clone this repo](https://github.com/awsaf49/yolov5-wandb). This s repo is synced with official YOLOv5 repo so you'll get the latest features of YOLOv5 along with support of W&B.  \n\n>  \n\n```\n!git clone https://github.com/awsaf49/yolov5-wandb.git yolov5 # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks\n```", "query_id": "7eff21b3-8ec2-4279-b3a6-e4923aa5ec87", "label": "integrations"}
{"query": "tracking experiments with wandb", "matching_document": "## Track Experiments\n### How it works\nTrack a machine learning experiment with a few lines of code:\n1. Create a W&B run.\n2. Store a dictionary of hyperparameters, such as learning rate or model type, into your configuration (`wandb.config`).\n3. Log metrics (`wandb.log()`) over time in a training loop, such as accuracy and loss.\n4. Save outputs of a run, like the model weights or a table of predictions.  \n\nThe proceeding pseudocode demonstrates a common W&B Experiment tracking workflow:  \n\n```python showLineNumbers\n\n# 1. Start a W&B Run\n\nwandb.init(entity=\"\", project=\"my-project-name\")\n\n# 2. Save mode inputs and hyperparameters\n\nwandb.config.learning\\_rate = 0.01\n\n# Import model and data\n\nmodel, dataloader = get\\_model(), get\\_data()\n\n# Model training code goes here\n\n# 3. Log metrics over time to visualize performance\n\nwandb.log({\"loss\": loss})\n\n# 4. Log an artifact to W&B\n\nwandb.log\\_artifact(model)\n```", "query_id": "839e408e-dfe8-4256-b4b8-3993b0588554", "label": "other"}
{"query": "Custom chart creation in Weights & Biases using Vega", "matching_document": "## Step 2: Create a custom chart for the confusion matrix\nW&B custom charts are written in [Vega](https://vega.github.io/vega/), a powerful and flexible visualization language. You can find many examples and walkthroughs online, and it can help to start with an existing preset that is most similar to your desired custom visualization. You can iterate from small changes in our [IDE](https://wandb.ai/wandb/posts/reports/The-W-B-Machine-Learning-Visualization-IDE--VmlldzoyOTQxNDY5/edit), which renders the plot as you change its definition.  \n\nHere is the full Vega spec for this multi-class confusion matrix:  \n\n* From your project workspace or report, click on \"Add a visualization\" and select \"Custom chart\"\n* Pick any existing preset and replace its definition with the Vega spec below\n* Click \"Save As\" to give this preset a name for easier reference (I recommend \"confusion_matrix\" :)", "query_id": "195b8ea5-165c-4555-baea-4e8e37e2001e", "label": "visualisation"}
{"query": "Custom charts in Wandb", "matching_document": "## Custom Charts\n\nUse **Custom Charts** to create charts that aren't possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of Vega.  \n\n* **What's possible**: Read the launch announcement \u2192\n* **Code**: Try a live example in a hosted notebook \u2192\n* **Video**: Watch a quick walkthrough video \u2192\n* **Example**: Quick Keras and Sklearn demo notebook \u2192  \n\n### How it works  \n\n1. **Log data**: From your script, log config and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table`\n2. **Customize the chart**: Pull in any of this logged data with a GraphQL query. Visualize the results of your query with Vega, a powerful visualization grammar.\n3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`.  \n\n## Log charts from a script  \n\n### Builtin presets  \n\n### Custom presets", "query_id": "ec601db6-c291-49fd-bb92-fdace481e9f7", "label": "visualisation"}
{"query": "WandB logging best practices", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "f77585d7-4ea4-4269-85f9-e83f64c6bdde", "label": "other"}
{"query": "updating artifacts in W&B", "matching_document": "## Update artifacts\n```\nimport wandb\n\napi = wandb.Api()\n\nartifact = api.artifact(\"entity/project/artifact:alias\")\n\n# Update the description\nartifact.description = \"My new description\"\n\n# Selectively update metadata keys\nartifact.metadata[\"oldKey\"] = \"new value\"\n\n# Replace the metadata entirely\nartifact.metadata = {\"newKey\": \"new value\"}\n\n# Add an alias\nartifact.aliases.append(\"best\")\n\n# Remove an alias\nartifact.aliases.remove(\"latest\")\n\n# Completely replace the aliases\nartifact.aliases = [\"replaced\"]\n\n# Persist all artifact modifications\nartifact.save()\n\n```  \n\nFor more information, see the Weights and Biases Artifact API.", "query_id": "6788e56f-588b-4707-998e-dd8f129b3bc7", "label": "artifact"}
{"query": "PyTorch integration with Weights & Biases", "matching_document": "## \ud83d\udd25 = W&B \u2795 PyTorch\n\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.  \n\n## What this notebook covers:  \n\nWe show you how to integrate Weights & Biases with your PyTorch code to add experiment tracking to your pipeline.  \n\n## The resulting interactive W&B dashboard will look like:  \n\n## In pseudocode, what we'll do is:  \n\n```\n# import the library\nimport wandb\n\n# start a new experiment\nwandb.init(project=\"new-sota-model\")\n\n#\u2003capture a dictionary of hyperparameters with config\nwandb.config = {\"learning\\_rate\": 0.001, \"epochs\": 100, \"batch\\_size\": 128}\n\n# set up model and data\nmodel, dataloader = get\\_model(), get\\_data()\n\n# optional: track gradients\nwandb.watch(model)\n\nfor batch in dataloader:\nmetrics = model.training\\_step()\n#\u2003log metrics inside your training loop to visualize model performance\nwandb.log(metrics)\n\n# optional: save model at the end\nmodel.to\\_onnx()\nwandb.save(\"model.onnx\")\n\n```  \n\n## Follow along with a video tutorial!", "query_id": "ed67c0e9-84cf-423e-9eb6-a9bc80abb8c9", "label": "integrations"}
{"query": "Weights & Biases sweeps", "matching_document": "## Sweep 101\n\nUse Weights & Biases Sweeps to automate hyperparameter optimization and explore the space of possible models.  \n\n## Check out Hyperparameter Optimization in PyTorch using W&B Sweeps $\\rightarrow$  \n\nRunning a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:  \n\n1. **Define the sweep:** We do this by creating a dictionary or a YAML file that specifies the parameters to search through, the search strategy, the optimization metric et all.\n2. **Initialize the sweep:**\n`sweep_id = wandb.sweep(sweep_config)`\n3. **Run the sweep agent:**\n`wandb.agent(sweep_id, function=train)`  \n\nAnd voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.", "query_id": "04dbadfc-e290-4e9e-af5d-ab334feb75f3", "label": "other"}
{"query": "hyperparameter optimization", "matching_document": "## How Do You Optimize Hyperparameters?\nBefore starting with automated hyperparameter optimization, you need to specify all of the above. But you can adjust the hyperparameter search space and the number of trial runs during manual hyperparameter tuning.  \n\nThe\u00a0generic steps\u00a0for hyperparameter tuning are:  \n\n* Select a set of hyperparameter values to evaluate\n* Run an ML experiment for the selected set of hyperparameters and their values, and evaluate and log its performance metric.\n* Repeat for the specified number of trial runs or until you are happy with the model\u2019s performance  \n\nDepending on whether you manually conduct these steps or automate them, we talk about\u00a0manual or automated hyperparameter optimization.  \n\nAfter\u00a0this process, you will end up with a list of experiments, including their hyperparameters and performance metrics. An automated hyperparameter optimization algorithm returns the experiment with the best performance metric and the respective hyperparameter values.", "query_id": "a73fa825-a2ff-4dee-8e51-2adb9c0cefb7", "label": "other"}
{"query": "wandb.init() code saving", "matching_document": "---  \n\n## displayed\\_sidebar: default  \n\n# Code Saving  \n\nBy default, we only save the latest git commit hash. You can turn on more code features to compare the code between your experiments dynamically in the UI.  \n\nStarting with `wandb` version 0.8.28, we can save the code from your main training file where you call `wandb.init()`. This will get sync'd to the dashboard and show up in a tab on the run page, as well as the Code Comparer panel. Go to your settings page to enable code saving by default.  \n\n## Save Library Code  \n\nWhen code saving is enabled, wandb will save the code from the file that called `wandb.init()`. To save additional library code, you have two options:  \n\n* Call `wandb.run.log_code(\".\")` after calling `wandb.init()`\n* Pass a settings object to `wandb.init` with code\\_dir set: `wandb.init(settings=wandb.Settings(code_dir=\".\"))`  \n\n## Code Comparer  \n\n## Jupyter Session History  \n\n## Jupyter diffing", "query_id": "bdb107ca-cbe1-43b0-b235-ffb516205185", "label": "other"}
{"query": "W&B artifacts", "matching_document": "## Register models\n### Log Data and Model Checkpoints as Artifacts\nW&B Artifacts allows you to track and version arbitrary serialized data (e.g. datasets, model checkpoints, evaluation results). When you create an artifact, you give it a name and a type, and that artifact is forever linked to the experimental system of record. If the underlying data changes, and you log that data asset again, W&B will automatically create new versions through checksummming its contents. W&B Artifacts can be thought of as a lightweight abstraction layer on top of shared unstructured file systems.  \n\n### Anatomy of an artifact  \n\nThe `Artifact` class will correspond to an entry in the W&B Artifact registry. The artifact has\n\\* a name\n\\* a type\n\\* metadata\n\\* description\n\\* files, directory of files, or references  \n\nExample usage:  \n\n```\nrun = wandb.init(project=\"my-project\")\nartifact = wandb.Artifact(name=\"my\\_artifact\", type=\"data\")\nartifact.add\\_file(\"/path/to/my/file.txt\")\nrun.log\\_artifact(artifact)\nrun.finish()\n\n```", "query_id": "fe3519f2-3d05-43a1-b52b-5fd806bdfd49", "label": "other"}
{"query": "Wandb custom charts", "matching_document": "## Custom Charts Walkthrough\n\nTo go beyond the built-in charts in W&B, use the new **Custom Charts** feature to control the details of exactly what data you're loading in to a panel and how you visualize that data.  \n\n**Overview**  \n\n1. Log data to W&B\n2. Create a query\n3. Customize the chart  \n\n## 1. Log data to W&B  \n\nFirst, log data in your script. Use wandb.config for single points set at the beginning of training, like hyperparameters. Use wandb.log() for multiple points over time, and log custom 2D arrays with wandb.Table(). We recommend logging up to 10,000 data points per logged key.  \n\n```\n# Logging a custom table of data\nmy\\_custom\\_data = [[x1, y1, z1], [x2, y2, z2]]\nwandb.log(\n{\"custom\\_data\\_table\": wandb.Table(data=my\\_custom\\_data, columns=[\"x\", \"y\", \"z\"])}\n)\n\n```  \n\nTry a quick example notebook to log the data tables, and in the next step we'll set up custom charts. See what the resulting charts look like in the live report.  \n\n## 2. Create a query  \n\n## 3. Customize the chart", "query_id": "448056d9-0e40-4723-8c39-20c50e98aa74", "label": "visualisation"}
{"query": "W&B best practices", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "8cebd252-add6-4c93-80f2-a729d433ce4a", "label": "other"}
{"query": "Are there any best practices for using wandb in a distributed training environment?", "matching_document": "## Add wandb to Any Library\n#### Distributed Training\n\nFor frameworks supporting distributed environments, you can adapt any of the following workflows:  \n\n* Detect which is the \u201cmain\u201d process and only use `wandb` there. Any required data coming from other processes must be routed to the main process first. (This workflow is encouraged).\n* Call `wandb` in every process and auto-group them by giving them all the same unique `group` name  \n\nSee Log Distributed Training Experiments for more details", "query_id": "bd68547f-07ab-4b4f-a7b0-95731f8a282c", "label": "other"}
{"query": "How to invite team members to a project in Weights & Biases?", "matching_document": "## Teams\n### Invite team members\nInvite new members to your team.\n1. Ensure the team member already has a W&B Account.\n2. Navigate to <https://wandb.ai/subscriptions>.\n3. Select **Manage members**.\n4. A model will appear. Provide the username or email for the **Email or Username** field, select a team for them to join from the **Team** dropdown menu, and select a role type from the **Organizational Role** dropdown menu.  \n\n1. Select the **Add** button.  \n\n:::info\n\\* If you have an Enterprise account, please contact your Account Executive to invite new members to your team.\n:::", "query_id": "d80de3d8-06b7-45d3-9447-a6c847b6315f", "label": "other"}
{"query": "wandb.init best practices", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "d80c7b46-59c3-4e6f-b377-ee4276e22397", "label": "other"}
{"query": "building LLM-powered apps with W&B", "matching_document": "## Prompts for LLMs\n\nW&B Prompts is a suite of LLMOps tools built for the development of LLM-powered applications. Use W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.  \n\n## Use Cases  \n\nW&B Prompts provides several solutions for building and monitoring LLM-based apps. Software developers, prompt engineers, ML practitioners, data scientists, and other stakeholders working with LLMs need cutting-edge tools to:  \n\n* Explore and debug LLM chains\u00a0and prompts with greater granularity.\n* Monitor and observe LLMs to better understand and evaluate performance, usage, and budgets.  \n\n## Products  \n\n### Traces  \n\nW&B\u2019s LLM tool is called\u00a0*Traces*.\u00a0**Traces**\u00a0allow you to track and visualize the inputs and outputs, execution flow, model architecture, and any intermediate results of your LLM chains.  \n\n### Weave  \n\n### How it works  \n\n## Integrations", "query_id": "dfff1f01-8dd1-46bc-84bf-78783878d810", "label": "other"}
{"query": "Weights & Biases features", "matching_document": "## Weights & Biases overview\nW&B is a platform that helps data scientists track their models, datasets, system information and more. With a few lines of code, you can start tracking everything about these features. It's free for personal use. Team use is normally a paid utility, but teams for academic purposes are free. You can use W&B with your favourite framework, like TensorFlow, Keras, PyTorch, Sklearn, fastai and many others.  \n\nAll tracking information is sent to a dedicated project page on the W&B UI, where you can open high quality visualizations, aggregate information and compare models or parameters. One of the advantages of remotely storing the experiment\u2019s information is that it is easy to collaborate on the same project and share the results with your teammates.  \n\nW&B provides 4 useful tools:  \n\n* Dashboard: Experiment tracking\n* Artifacts: Dataset versioning, model versioning\n* Sweeps: Hyperparameter optimization\n* Reports: Save and share reproducible findings", "query_id": "2bf96622-ec30-45e9-a83c-fee84e3e685c", "label": "other"}
{"query": "Weights & Biases artifacts", "matching_document": "## Artifacts\n### How it works\nCreate an artifact with four lines of code:\n1. Create a W&B run.\n2. Create an artifact object with the `wandb.Artifact` API.\n3. Add one or more files, such as a model file or dataset, to your artifact object.\n4. Log your artifact to W&B.  \n\n`python showLineNumbers\nrun = wandb.init(project=\"artifacts-example\", job_type=\"add-dataset\")\nartifact = wandb.Artifact(name=\"my_data\", type=\"dataset\")\nartifact.add_dir(local_path=\"./dataset.h5\") # Add dataset directory to artifact\nrun.log_artifact(artifact) # Logs the artifact version \"my_data:v0\"`  \n\n:::tip\nThe preceding code snippet, and the colab linked on this page, show how to track files by uploading them to W&B. See the track external files page for information on how to add references to files or directories that are stored in external object storage (for example, in an Amazon S3 bucket).\n:::", "query_id": "ecdc6050-d98d-481c-8770-cc4a083e8aa3", "label": "artifact"}
{"query": "wandb confusion matrix chart", "matching_document": "## Step 2: Create a custom chart for the confusion matrix\n```\n{\n\"$schema\": \"https://vega.github.io/schema/vega-lite/v4.json\",\n\"description\": \"Multi-class confusion matrix\",\n\"data\": {\n\"name\": \"wandb\"\n},\n\"width\": 40,\n\"height\": {\"step\":6},\n\"spacing\": 5,\n\"mark\" : \"bar\",\n\"encoding\": {\n\"y\": {\"field\": \"name\", \"type\": \"nominal\", \"axis\" : {\"labels\" : false},\n\"title\" : null, \"scale\": {\"zero\": false}},\n\"x\": {\n\"field\": \"${field:count}\",\n\"type\": \"quantitative\",\n\"axis\" : null,\n\"title\" : null\n},\n\"tooltip\": [\n{\"field\": \"${field:count}\", \"type\": \"quantitative\", \"title\" : \"Count\"},\n{\"field\": \"name\", \"type\": \"nominal\", \"title\" : \"Run name\"}\n],\n\"color\": {\n\"field\": \"name\",\n\"type\": \"nominal\",\n\"legend\": {\"orient\": \"top\", \"titleOrient\": \"left\"},\n\"title\": \"Run name\"\n},\n\"row\": {\"field\": \"${field:actual}\", \"title\": \"Actual\",\n\"header\": {\"labelAlign\" : \"left\", \"labelAngle\": 0}},\n\"column\": {\"field\": \"${field:predicted}\", \"title\": \"Predicted\"}\n}\n}\n```", "query_id": "ea5d6313-2e41-49ef-b455-0f3fd335d513", "label": "visualisation"}
{"query": "How does wandb.save function and what are its use cases?", "matching_document": "## Save your machine learning model\n* Use wandb.save(filename).\n* Put a file in the wandb run directory, and it will get uploaded at the end of the run.  \n\nIf you want to sync files as they're being written, you can specify a filename or glob in wandb.save.  \n\nHere's how you can do this in just a few lines of code. See [this colab](https://colab.research.google.com/drive/1pVlV6Ua4C695jVbLoG-wtc50wZ9OOjnC) for a complete example.  \n\n```\n# \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training\nmodel.save(os.path.join(wandb.run.dir, \"model.h5\"))\n\n# Save a model file manually from the current directory:\nwandb.save('model.h5')\n\n# Save all files that currently exist containing the substring \"ckpt\":\nwandb.save('../logs/*ckpt*')\n\n# Save any files starting with \"checkpoint\" as they're written to:\nwandb.save(os.path.join(wandb.run.dir, \"checkpoint*\"))\n```", "query_id": "4ee9a123-0dee-447d-9033-d792b28d8382", "label": "other"}
{"query": "Best practices for managing data artifacts in W&B", "matching_document": "## Storage\n\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.  \n\n## Manage storage consumption  \n\nW&B offers different methods of optimizing your storage consumption:  \n\n* Use\u00a0reference artifacts\u00a0to track files saved outside the W&B system, instead of uploading them to W&B storage.\n* Use an external cloud storage bucket for storage. *(Enterprise only)*  \n\n## Delete data  \n\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:  \n\n* Delete data interactively with the app UI.\n* Set a TTL policy on Artifacts so they are automatically deleted.", "query_id": "a09113c1-8ac2-4adc-9fb1-fc38a03ef476", "label": "other"}
{"query": "hyperparameter tuning", "matching_document": "## What Are Hyperparameters?\nWhat\u2019s even more troublesome is that machine learning models are very sensitive to their hyperparameter configurations. The performance of a machine learning model with a certain hyperparameter configuration may not be similar when the hyperparameter configuration is changed.  \n\nTo address these problems, we resort to hyperparameter tuning. The general process of this is roughly:  \n\n* Select the hyperparameters to be tuned (there can be several hyperparameters in a machine learning model).\n* Specify a grid of acceptable values for the specified hyperparameters or specify distributions that would generate the acceptable values.\n* Train several machine learning models pertaining to each of the different hyperparameter configurations results from the above two steps.\n* Select the model that performs the best from the pool of many models.  \n\nAlthough there are many niche techniques that help us in effectively tuning the hyperparameters, the most predominant ones are:  \n\n* Grid search\n* Random search", "query_id": "9a124dd2-4ae4-4dd3-8c76-7ca713f34a68", "label": "other"}
{"query": "versioning datasets in W&B", "matching_document": "## Dataset Versioning\n### Core Artifacts features\n1. **Upload**: Start tracking and versioning any data (files or directories) with `run.log_artifact()`. You can also track datasets in a remote filesystem (e.g. cloud storage in S3 or GCP) by reference, using a link or URI instead of the raw contents.\n2. **Version**: Define an artifact by giving it a type (`\"raw_data\"`, `\"preprocessed_data\"`, `\"balanced_data\"`) and a name (`\"imagenet_cats_10K\"`). When you log the same name again, W&B automatically creates a new version of the artifact with the latest contents.\n3. **Alias**: Set an alias like `\"best\"` or `\"production\"` to highlight the important versions in a lineage of artifacts.\n4. **Compare**: Select any two versions to browse the contents side-by-side. We're also working on a tool for dataset visualization, learn more here.\n5. **Download**: Obtain a local copy of the artifact or verify the contents by reference.  \n\nFor more detail on these features, check out Artifacts how it works.", "query_id": "22eaec09-07dd-4b5b-a1ef-a039d8dfd42f", "label": "artifact"}
{"query": "wandb init best practices", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "0517fc00-1d3c-41fa-b45e-15ff00e10103", "label": "other"}
{"query": "How to use IAM roles with SageMaker for training job access control?", "matching_document": "## Set up for SageMaker\n### Prerequisites\n1. **Setup SageMaker in your AWS account.** See the SageMaker Developer guide for more information.\n2. **Create an Amazon ECR repository** to store images you want to execute on Amazon SageMaker. See the Amazon ECR documentation for more information.\n3. **Create an Amazon S3 buckets** to store SageMaker inputs and outputs for your SageMaker training jobs. See the Amazon S3 documentation for more information. Make note of the S3 bucket URI and directory.\n4. **Create IAM execution role.** The role used in the SageMaker training job requires the following permissions to work. These permissions allow for logging events, pulling from ECR, and interacting with input and output buckets. (Note: if you already have this role for SageMaker training jobs, you do not need to create it again.)\nIAM role policy\n```\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"cloudwatch:PutMetricData\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:CreateLogGroup\",\n\"logs:DescribeLogStreams\",\n\"ecr:GetAuthorizationToken\"\n],\n\"Resource\": \"\\*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:ListBucket\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:GetObject\",\n\"s3:PutObject\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>/<object>\",\n\"arn:aws:s3:::<output-bucket>/<path>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:BatchGetImage\"\n],\n\"Resource\": \"arn:aws:ecr:<region>:<account-id>:repository/<repo>\"\n}\n]\n}\n\n```", "query_id": "f3a4cd72-3c85-4c8c-8012-b378f8f41a81", "label": "other"}
{"query": "Weights & Biases dashboard features", "matching_document": "## Tutorial\n### Dashboards\nNow we can look at the results. The run we have executed is now shown on the left side, in our project, with the group and experiment names we listed. We have access to a lot of information that W&B has automatically recorded.  \n\nWe have several sections like:  \n\n* Charts - contains information about losses, accuracy, etc. Also, it contains some examples from our data.\n* System - contains system load information: memory usage, CPU utilization, GPU temp, etc. This is very useful information because you can control the usage of your GPU and choose the optimal batch size.\n* Model - contains information about our model structure (graph).\n* Logs - include Keras default logging.\n* Files - contains all files that were created during the experiment, such as: config, best model, output logs, requirements, etc. The requirements file is very important because, in order to recreate a specific experiment, you need to install specific versions of the libraries.", "query_id": "24d9f9d8-8844-4c5c-b389-251abfcfbd26", "label": "other"}
{"query": "wandb integration with OpenAI", "matching_document": "## OpenAI API\n\nUse the Weights & Biases OpenAI API integration to log requests, responses, token counts and model metadata with 1 line of code for all OpenAI models, including fine-tuned models.  \n\n:::info\nThe W&B autolog integration works with `openai <= 0.28.1`. Install the correct version of `openai` with `pip install openai==0.28.1`.\n:::  \n\n**Try in a Colab Notebook here \u2192**  \n\nWith just 1 line of code you can now automatically log inputs and outputs from the OpenAI Python SDK to Weights & Biases!  \n\nOnce you start logging your API inputs and outputs you can quickly evaluate the performance of difference prompts, compare different model settings (such as temperature), and track other usage metrics such as token usage.  \n\nTo get started, pip install the `wandb` library, then follow the steps below:  \n\n### 1. Import autolog and initialise it  \n\nFirst, import `autolog` from `wandb.integration.openai` and initialise it.  \n\n### 2. Call the OpenAI API  \n\n### 3. View your OpenAI API inputs and responses", "query_id": "185d5989-2c39-4375-b9cd-7dfa024a502f", "label": "integrations"}
{"query": "Best practices for tracking LLM experiments in Weights & Biases", "matching_document": "### Using Weights & Biases to track experiments\n\nExperimenting with prompts, function calling and response model schema is critical to get good results. As LLM Engineers, we will be methodical and use Weights & Biases to track our experiments.  \n\nHere are a few things you should consider logging:  \n\n1. Save input and output pairs for later analysis\n2. Save the JSON schema for the response\\_model\n3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.  \n\nThis is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We will use the `wandb` library to track our experiments and save the results to a dashboard.", "query_id": "092105c9-c876-45ac-bca0-0b8f0027e350", "label": "other"}
{"query": "How does artifact versioning work in W&B?", "matching_document": "## Dataset Versioning\n### Core Artifacts features\n1. **Upload**: Start tracking and versioning any data (files or directories) with `run.log_artifact()`. You can also track datasets in a remote filesystem (e.g. cloud storage in S3 or GCP) by reference, using a link or URI instead of the raw contents.\n2. **Version**: Define an artifact by giving it a type (`\"raw_data\"`, `\"preprocessed_data\"`, `\"balanced_data\"`) and a name (`\"imagenet_cats_10K\"`). When you log the same name again, W&B automatically creates a new version of the artifact with the latest contents.\n3. **Alias**: Set an alias like `\"best\"` or `\"production\"` to highlight the important versions in a lineage of artifacts.\n4. **Compare**: Select any two versions to browse the contents side-by-side. We're also working on a tool for dataset visualization, learn more here.\n5. **Download**: Obtain a local copy of the artifact or verify the contents by reference.  \n\nFor more detail on these features, check out Artifacts how it works.", "query_id": "5b3973b1-1720-41dc-97a1-edb081dce61c", "label": "artifact"}
{"query": "Can W&B reports be made public?", "matching_document": "---  \n\n## description: Collaborate and share W&B Reports with peers, co-workers, and your team.  \n\n# Collaborate on reports  \n\nCollaborate and Share W&B Reports  \n\nOnce you have saved a report, you can select the **Share** button to collaborate. A draft copy of the report is created when you select the **Edit** button. Draft reports auto-save. Select **Save to report** to publish your changes to the shared report.  \n\nA warning notification will appear if an edit conflict occurs. This can occur if you and another collaborator edit the same report at the same time. The warning notification will guide you to resolve potential edit conflicts.  \n\n### Comment on reports  \n\nClick the comment button on a panel in a report to add a comment directly to that panel.  \n\n### Who can edit and share reports?  \n\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.", "query_id": "f3ecf203-acff-49a8-a9e1-f697ee23fc39", "label": "other"}
{"query": "W&B artifacts deletion", "matching_document": "## Delete artifacts\n:::note\nArtifacts that are scheduled for deletion with a TTL policy, deleted with the W&B SDK, or deleted with the W&B App UI are first soft-deleted. Artifacts that are soft deleted undergo garbage collection before they are hard-deleted.\n:::  \n\n### Delete an artifact version  \n\nTo delete an artifact version:  \n\n1. Select the name of the artifact. This will expand the artifact view and list all the artifact versions associated with that artifact.\n2. From the list of artifacts, select the artifact version you want to delete.\n3. On the right hand side of the workspace, select the kebab dropdown.\n4. Choose Delete.  \n\nAn artifact version can also be deleted programatically via the delete() method. See the examples below.  \n\n### Delete multiple artifact versions with aliases  \n\nThe following code example demonstrates how to delete artifacts that have aliases associated with them. Provide the entity, project name, and run ID that created the artifacts.  \n\n### Delete multiple artifact versions with a specific alias", "query_id": "4526c55a-a0c7-49b0-a7c2-d414c0505b43", "label": "artifact"}
{"query": "Weights & Biases GPU utilization", "matching_document": "## Monitor & Improve GPU Usage for Model Training\n#### 1. Measure your GPU usage consistently over your entire training runs\nYou can\u2019t improve GPU usage without measuring it.  It\u2019s not hard to take a snapshot of your usage with useful tools like nvidia-smi, but a simple way to find issues is to track usage over time.  Anyone can turn on system monitoring in the background, which will track GPU, CPU, memory usage etc over time by adding two lines to their code:  \n\n```\nimport wandb\nwandb.init()\n```  \n\nThe wandb.init() function will create a lightweight child process that will collect system metrics and send them to a wandb server where you can look at them and compare across runs with graphs like these:  \n\nThe danger of taking a single measurement is that GPU usage can change over time.  This is a common pattern we see where our user Boris is training an RNN; mid-training, his usage plummets from 80 percent to around 25 percent.", "query_id": "2bdd7895-220b-4276-a91c-8991cc0d217d", "label": "other"}
{"query": "YOLOv5 Weights & Biases integration guide", "matching_document": "## \ud83d\udce6 YOLOv5 +\ud83e\ude84 W&B\n\nTo install YOLOv5 with W&B integration all we need to is simply [clone this repo](https://github.com/awsaf49/yolov5-wandb). This s repo is synced with official YOLOv5 repo so you'll get the latest features of YOLOv5 along with support of W&B.  \n\n>  \n\n```\n!git clone https://github.com/awsaf49/yolov5-wandb.git yolov5 # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks\n```", "query_id": "10e0156c-c62b-4d8e-9051-49b0f8421ee6", "label": "integrations"}
{"query": "wandb features", "matching_document": "## Weights & Biases overview\nW&B is a platform that helps data scientists track their models, datasets, system information and more. With a few lines of code, you can start tracking everything about these features. It's free for personal use. Team use is normally a paid utility, but teams for academic purposes are free. You can use W&B with your favourite framework, like TensorFlow, Keras, PyTorch, Sklearn, fastai and many others.  \n\nAll tracking information is sent to a dedicated project page on the W&B UI, where you can open high quality visualizations, aggregate information and compare models or parameters. One of the advantages of remotely storing the experiment\u2019s information is that it is easy to collaborate on the same project and share the results with your teammates.  \n\nW&B provides 4 useful tools:  \n\n* Dashboard: Experiment tracking\n* Artifacts: Dataset versioning, model versioning\n* Sweeps: Hyperparameter optimization\n* Reports: Save and share reproducible findings", "query_id": "0eab6b7f-f7c5-44e4-86ba-ef702d8a9518", "label": "other"}
{"query": "Best practices for managing artifacts in W&B", "matching_document": "## Dagster\n### Best practices\n1. Use the IO Manager to read and write Artifacts.\nYou should never need to use `Artifact.download()` or `Run.log_artifact()` directly. Those methods are handled by integration. Simply return the data you wish to store in Artifact and let the integration do the rest. This will provide better lineage for the Artifact in W&B.\n2. Only build an Artifact object yourself for complex use cases.\nPython objects and W&B objects should be returned from your ops/assets. The integration handles bundling the Artifact.\nFor complex use cases, you can build an Artifact directly in a Dagster job. We recommend you pass an Artifact object to the integration for metadata enrichment such as the source integration name and version, the python version used, the pickle protocol version and more.\n3. Add files, directories and external references to your Artifacts through the metadata.\nUse the integration `wandb_artifact_configuration` object to add any file, directory or external references (Amazon S3, GCS, HTTP\u2026). See the advanced example in the Artifact configuration section for more information.\n4. Use an @asset instead of an @op when an Artifact is produced.\nArtifacts are assets. It is recommended to use an asset when Dagster maintains that asset. This will provide better observability in the Dagit Asset Catalog.\n5. Use a SourceAsset to consume an Artifact created outside Dagster.\nThis allows you to take advantage of the integration to read externally created Artifacts. Otherwise, you can only use Artifacts created by the integration.\n6. Use W&B Launch to orchestrate training on dedicated compute for large models.\nYou can train small models inside your Dagster cluster and you can run Dagster in a Kubernetes cluster with GPU nodes. We recommend using W&B Launch for large model training. This will prevent overloading your instance and provide access to more adequate compute.\n7. When experiment tracking within Dagster, set your W&B Run ID to the value of your Dagster Run ID.\nWe recommend that you both: make the Run resumable and set the W&B Run ID to the Dagster Run ID or to a string of your choice. Following this recommendation ensures your W&B metrics and W&B Artifacts are stored in the same W&B Run when you train models inside of Dagster.", "query_id": "7f410b9b-06d1-4126-a31f-b5f1439b2b13", "label": "artifact"}
{"query": "Weights & Biases artifact versioning", "matching_document": "## Create new artifact versions\n### Create new artifact versions from scratch\n* **Single run**: A single run provides all the data for a new version. This is the most common case and is best suited when the run fully recreates the needed data. For example: outputting saved models or model predictions in a table for analysis.\n* **Distributed runs**: A set of runs collectively provides all the data for a new version. This is best suited for distributed jobs which have multiple runs generating data, often in parallel. For example: evaluating a model in a distributed manner, and outputting the predictions.  \n\nW&B will create a new artifact and assign it a `v0` alias if you pass a name to the `wandb.Artifact` API that does not exist in your project. W&B checksums the contents when you log again to the same artifact. If the artifact changed, W&B saves a new version `v1`.  \n\nW&B will retrieve an existing artifact if you pass a name and artifact type to the `wandb.Artifact` API that matches an existing artifact in your project. The retrieved artifact will have a version greater than 1.", "query_id": "6586068a-9f59-474f-995b-dfa5ab7ca73b", "label": "artifact"}
{"query": "What is the configuration file or environment variable to set the artifact cache directory in W&B?", "matching_document": "### Optional Environment Variables\n| Variable name | Usage |\n| --- | --- |\n| **WANDB\\_ANONYMOUS** | Set this to \"allow\", \"never\", or \"must\" to let users create anonymous runs with secret urls. |\n| **WANDB\\_API\\_KEY** | Sets the authentication key associated with your account. You can find your key on your settings page. This must be set if `wandb login` hasn't been run on the remote machine. |\n| **WANDB\\_BASE\\_URL** | If you're using wandb/local you should set this environment variable to `http://YOUR_IP:YOUR_PORT` |\n| **WANDB\\_CACHE\\_DIR** | This defaults to \\~/.cache/wandb, you can override this location with this environment variable |\n| **WANDB\\_CONFIG\\_DIR** | This defaults to \\~/.config/wandb, you can override this location with this environment variable |\n| **WANDB\\_CONFIG\\_PATHS** | Comma separated list of yaml files to load into wandb.config. See config. |\n| **WANDB\\_CONSOLE** | Set this to \"off\" to disable stdout / stderr logging. This defaults to \"on\" in environments that support it. |\n| **WANDB\\_DIR** | Set this to an absolute path to store all generated files here instead of the *wandb* directory relative to your training script. *be sure this directory exists and the user your process runs as can write to it* |\n| **WANDB\\_DISABLE\\_GIT** | Prevent wandb from probing for a git repository and capturing the latest commit / diff. |\n| **WANDB\\_DISABLE\\_CODE** | Set this to true to prevent wandb from saving notebooks or git diffs. We'll still save the current commit if we're in a git repo. |\n| **WANDB\\_DOCKER** | Set this to a docker image digest to enable restoring of runs. This is set automatically with the wandb docker command. You can obtain an image digest by running `wandb docker my/image/name:tag --digest` |", "query_id": "33422ea3-ecbb-4f32-9abd-f8e441c6afd7", "label": "artifact"}
{"query": "W&B logging features", "matching_document": "## Managing and Tracking ML Experiments With W&B\n### Logging Advanced\u00a0Things\u200b\n\nOne of the coolest things about W&B is that you can literally log anything. You can log custom metrics, matplotlib plots, datasets, embeddings from your models, prediction distribution, etc.  \n\nRecently, Weights & Biases announced the Tables feature, which allows you to log, query and analyze tabular data. You can even visualize model predictions and compare them across models. For example: see the image below (taken from[ W&B Docs](http://docs.wandb.ai)), which compares two segmentation models.  \n\nYou can log audio data, images, histograms, text, video, and tabular data and visualize/inspect them interactively. To learn more about W&B Tables, go through their [documentation](https://docs.wandb.ai/guides/data-vis).  \n\nYou can even export the dashboard in CSV files to analyze them further. W&B also supports exports in PNG, SVG, PDF, and CSV, depending on the type of data you are trying to export.", "query_id": "efde5c0a-582a-46b9-a3ff-333833e43044", "label": "other"}
{"query": "Weights & Biases integration with LangChain", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "96fdfcf0-dec7-4df0-97ad-68b7969dcaf3", "label": "integrations"}
{"query": "What are the best practices for using Weights & Biases in deep learning projects?", "matching_document": "## \u270d\ufe0f W&B Best Practices\n\n1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\"experiment-1\")`\n3. **Tags**: Add tags to track your current baseline or production model.\n4. **Notes**: Type notes in the table to track the changes between runs.\n5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.", "query_id": "c50a98ea-bbd5-4007-97af-d0db7a25b2b7", "label": "other"}
{"query": "Overview of Weights & Biases features", "matching_document": "## Weights & Biases overview\nW&B is a platform that helps data scientists track their models, datasets, system information and more. With a few lines of code, you can start tracking everything about these features. It's free for personal use. Team use is normally a paid utility, but teams for academic purposes are free. You can use W&B with your favourite framework, like TensorFlow, Keras, PyTorch, Sklearn, fastai and many others.  \n\nAll tracking information is sent to a dedicated project page on the W&B UI, where you can open high quality visualizations, aggregate information and compare models or parameters. One of the advantages of remotely storing the experiment\u2019s information is that it is easy to collaborate on the same project and share the results with your teammates.  \n\nW&B provides 4 useful tools:  \n\n* Dashboard: Experiment tracking\n* Artifacts: Dataset versioning, model versioning\n* Sweeps: Hyperparameter optimization\n* Reports: Save and share reproducible findings", "query_id": "399c542d-f4b5-45b8-84fb-4bc8f26d1f78", "label": "other"}
{"query": "Examples of logging images in Wandb", "matching_document": "## Log Media & Objects\n### Images\n```\nimages = wandb.Image(image\\_array, caption=\"Top: Output, Bottom: Input\")\n\nwandb.log({\"examples\": images})\n\n```  \n\nWe assume the image is gray scale if the last dimension is 1, RGB if it's 3, and RGBA if it's 4. If the array contains floats, we convert them to integers between `0` and `255`. If you want to normalize your images differently, you can specify the `mode` manually or just supply a `PIL.Image`, as described in the \"Logging PIL Images\" tab of this panel.  \n\nFor full control over the conversion of arrays to images, construct the `PIL.Image` yourself and provide it directly.  \n\n```\nimages = [PIL.Image.fromarray(image) for image in image\\_array]\n\nwandb.log({\"examples\": [wandb.Image(image) for image in images]})\n\n```  \n\nFor even more control, create images however you like, save them to disk, and provide a filepath.  \n\n```\nim = PIL.fromarray(...)\nrgb\\_im = im.convert(\"RGB\")\nrgb\\_im.save(\"myimage.jpg\")\n\nwandb.log({\"example\": wandb.Image(\"myimage.jpg\")})\n\n```", "query_id": "597c4228-16a2-432e-9e8e-d9e9a672299b", "label": "other"}
{"query": "Artifact versioning in Weights & Biases with S3 datasets", "matching_document": "## Dataset Versioning\n### Core Artifacts features\n1. **Upload**: Start tracking and versioning any data (files or directories) with `run.log_artifact()`. You can also track datasets in a remote filesystem (e.g. cloud storage in S3 or GCP) by reference, using a link or URI instead of the raw contents.\n2. **Version**: Define an artifact by giving it a type (`\"raw_data\"`, `\"preprocessed_data\"`, `\"balanced_data\"`) and a name (`\"imagenet_cats_10K\"`). When you log the same name again, W&B automatically creates a new version of the artifact with the latest contents.\n3. **Alias**: Set an alias like `\"best\"` or `\"production\"` to highlight the important versions in a lineage of artifacts.\n4. **Compare**: Select any two versions to browse the contents side-by-side. We're also working on a tool for dataset visualization, learn more here.\n5. **Download**: Obtain a local copy of the artifact or verify the contents by reference.  \n\nFor more detail on these features, check out Artifacts how it works.", "query_id": "c5f70f42-cd07-44a5-904e-96256dbf21ce", "label": "artifact"}
{"query": "Weights & Biases artifacts documentation", "matching_document": "Arguments:\n        name: A human-readable name for the artifact. Use the name to identify\n            a specific artifact in the W&B App UI or programmatically. You can\n            interactively reference an artifact with the `use_artifact` Public API.\n            A name can contain letters, numbers, underscores, hyphens, and dots.\n            The name must be unique across a project.\n        type: The artifact's type. Use the type of an artifact to both organize\n            and differentiate artifacts. You can use any string that contains letters,\n            numbers, underscores, hyphens, and dots. Common types include `dataset` or `model`.\n            Include `model` within your type string if you want to link the artifact\n            to the W&B Model Registry.\n        description: A description of the artifact. For Model or Dataset Artifacts,\n            add documentation for your standardized team model or dataset card. View\n            an artifact's description programmatically with the `Artifact.description`\n            attribute or programmatically with the W&B App UI. W&B renders the\n            description as markdown in the W&B App.\n        metadata: Additional information about an artifact. Specify metadata as a\n            dictionary of key-value pairs. You can specify no more than 100 total keys.\n\n    Returns:\n        An `Artifact` object.\n    \"\"\"", "query_id": "ad1e2a8b-2401-4f9f-b586-7ebd2e2e9a12", "label": "artifact"}
{"query": "W&B team collaboration", "matching_document": "## Teams\n\nUse W&B Teams as a central workspace for your ML team to build better models faster.  \n\n* **Track all the experiments** your team has tried so you never duplicate work.\n* **Save and reproduce** previously trained models.\n* **Share progress** and results with your boss and collaborators.\n* **Catch regressions** and immediately get alerted when performance drops.\n* **Benchmark model performance** and compare model versions.  \n\n## Create a collaborative team  \n\n1. **Sign up or log in** to your free W&B account.\n2. Click **Invite Team** in the navigation bar.\n3. Create your team and invite collaborators.  \n\n:::info\n**Note**: Only the admin of an organization can create a new team.\n:::  \n\n## Invite team members  \n\n## Create a Team Profile  \n\nYou can customize your team's profile page to show an introduction and showcase reports and projects that are visible to the public or team members. Present reports, projects, and external links.  \n\n## Remove team members  \n\n## Team Roles and Permissions", "query_id": "a9077f1a-f73d-4f65-9685-218ca73fc760", "label": "other"}
{"query": "W&B model versioning", "matching_document": "## Terms and concepts\n\nModel Registry terms and concepts  \n\nThe following terms describe key components of the W&B Model Registry: *model version*, *model artifact*, and *registered model*.  \n\n## Model version  \n\nA model version represents a single model checkpoint. Model versions are a snapshot at a point in time of a model and its files within an experiment.  \n\nA model version is an immutable directory of data and metadata that describes a trained model. W&B suggests that you add files to your model version that let you store (and restore) your model architecture and learned parameters at a later date.  \n\nA model version belongs to one, and only one, model artifact. A model version can belong to zero or more, registered models. Model versions are stored in a model artifact in the order they are logged to the model artifact. W&B automatically creates a new model version if it detects that a model you log (to the same model artifact) has different contents than a previous model version.  \n\n## Model alias", "query_id": "767786b8-a725-486d-ae0a-a7766f09bb38", "label": "artifact"}
{"query": "Weights & Biases", "matching_document": "## What is W&B?\n\nWeights & Biases (W&B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models.  \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your models and data are tracked and versioned in a reliable system of record.  \n\nThis diagram outlines the relationship between W&B products.  \n\n**W&B Models** is a set of lightweight, interoperable tools for machine learning practitioners training and fine-tuning models.\n- Experiments: Machine learning experiment tracking\n- Model Registry: Manage production models centrally\n- Launch: Scale and automate workloads\n- Sweeps: Hyperparameter tuning and model optimization  \n\n**W&B Prompts** is for LLM debugging and monitoring, including usage of OpenAI's GPT API.", "query_id": "6e80e67d-296e-4096-96f9-2ad408caccac", "label": "other"}
{"query": "How to structure Weights & Biases runs for hyperparameter tuning?", "matching_document": "## Whats Next? Hyperparameters with Sweeps\n\nWe tried out two different hyperparameter settings by hand. You can use Weights & Biases Sweeps to automate hyperparameter testing and explore the space of possible models and optimization strategies.  \n\n## Check out Hyperparameter Optimization in TensorFlow uisng W&B Sweep $\\rightarrow$  \n\nRunning a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:  \n\n1. **Define the sweep:** We do this by creating a dictionary or a YAML file that specifies the parameters to search through, the search strategy, the optimization metric et all.\n2. **Initialize the sweep:**\n`sweep_id = wandb.sweep(sweep_config)`\n3. **Run the sweep agent:**\n`wandb.agent(sweep_id, function=train)`  \n\nAnd voila! That's all there is to running a hyperparameter sweep! In the notebook below, we'll walk through these 3 steps in more detail.", "query_id": "3911dd96-0114-4179-adc0-ce9126a9568e", "label": "other"}
{"query": "machine learning model tracking", "matching_document": "## Track a model\n\nTrack a model, the model's dependencies, and other information relevant to that model with the W&B Python SDK.  \n\nUnder the hood, W&B creates a lineage of model artifact that you can view with the W&B App UI or programmatically with the W&B Python SDK. See the Create model lineage map for more information.  \n\n## How to log a model  \n\nUse the `run.log_model` API to log a model. Provide the path where your model files are saved to the `path` parameter. The path can be a local file, directory, or reference URI to an external bucket such as `s3://bucket/path`.  \n\nOptionally provide a name for the model artifact for the `name` parameter. If `name` is not specified, W&B uses the basename of the input path prepended with the run ID.  \n\nCopy and paste the proceeding code snippet. Ensure to replace values enclosed in `<>` with your own.", "query_id": "01f7c67c-9125-45a6-a149-4ad7eb671f88", "label": "artifact"}
{"query": "saving files with wandb", "matching_document": "## Save & Restore Files\n### Saving Files\n#### Examples of `wandb.save`\n```\n# Save a model file from the current directory\nwandb.save(\"model.h5\")\n\n# Save all files that exist containing the substring \"ckpt\"\nwandb.save(\"../logs/\\*ckpt\\*\")\n\n# Save files starting with \"checkpoint\" as they're written to\nwandb.save(os.path.join(wandb.run.dir, \"checkpoint\\*\"))\n\n```  \n\n:::info\nW&B's local run directories are by default inside the `./wandb` directory relative to your script, and the path looks like `run-20171023_105053-3o4933r0` where `20171023_105053` is the timestamp and `3o4933r0` is the ID of the run. You can set the `WANDB_DIR` environment variable, or the `dir` keyword argument of `wandb.init`, to an absolute path and files will be written within that directory instead.\n:::", "query_id": "8c55b62f-ce98-4d00-acd3-76057644bdd7", "label": "other"}
{"query": "YOLOv5 integration", "matching_document": "## \ud83d\udce6 YOLOv5 +\ud83e\ude84 W&B\n\nTo install YOLOv5 with W&B integration all we need to is simply [clone this repo](https://github.com/awsaf49/yolov5-wandb). This s repo is synced with official YOLOv5 repo so you'll get the latest features of YOLOv5 along with support of W&B.  \n\n>  \n\n```\n!git clone https://github.com/awsaf49/yolov5-wandb.git yolov5 # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks\n```", "query_id": "20eceb0d-f141-4315-9f96-3ead46c4b8fd", "label": "integrations"}
{"query": "Integrating Weights & Biases with Hugging Face's Accelerate", "matching_document": "## Hugging Face Accelerate\n\nAccelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code, making training and inference at scale made simple, efficient and adaptable.  \n\nAccelerate includes a Weights & Biases Tracker which we show how to use below. You can also read more about Accelerate Trackers in **their docs here**  \n\n## Start logging with Accelerate  \n\nTo get started with Accelerate and Weights & Biases you can follow the pseudocode below:  \n\n## Accessing Accelerates' Internal W&B Tracker  \n\nYou can quickly access the wandb tracker using the Accelerator.get\\_tracker() method. Just pass in the string corresponding to a tracker\u2019s .name attribute and it will return that tracker on the main process.  \n\n```\nwandb\\_tracker = accelerator.get\\_tracker(\"wandb\")\n\n```  \n\nFrom there you can interact with wandb\u2019s run object like normal:  \n\n```\nwandb\\_tracker.log\\_artifact(some\\_artifact\\_to\\_log)\n\n```  \n\n## Accelerate Articles", "query_id": "7734ba38-f91b-4899-8097-a50f280f2b58", "label": "integrations"}
{"query": "Steps to link a run to the model registry in Weights & Biases.", "matching_document": "## W&B Tutorial with Pytorch Lightning\n### Model Registry\n\nAfter logging a bunch of checkpoints across multiple runs during experimentation, now comes time to hand-off the best checkpoint to the next stage of the workflow (e.g. testing, deployment).  \n\nThe model registry offers a centralized place to house the best checkpoints for all your model tasks. Any `model` artifact you log can be \"linked\" to a Registered Model. Here are the steps to start using the model registry for more organized model management:\n1. Access your team's model registry by going the team page and selecting `Model Registry`  \n\n1. Create a new Registered Model.\n2. Go to the artifacts tab of the project that holds all your model checkpoints\n3. Click \"Link to Registry\" for the model artifact version you want. (Alternatively you can link a model via api with `wandb.run.link_artifact`)", "query_id": "63fde3c5-38e2-497d-914f-b338b2f6d338", "label": "artifact"}
{"query": "How to download artifacts from Weights & Biases?", "matching_document": "## Download and use artifacts\n#### Download and use an artifact stored on W&B\nUse the object returned to download all the contents of the artifact:  \n\n```\ndatadir = artifact.download()\n\n```  \n\nYou can optionally pass a path to the root parameter to download the contents of the artifact to a specific directory. For more information, see the Python SDK Reference Guide.  \n\nUse the `get_path` method to download only subset of files:  \n\n```\npath = artifact.get\\_path(name)\n\n```  \n\nThis fetches only the file at the path `name`. It returns an `Entry` object with the following methods:  \n\n* `Entry.download`: Downloads file from the artifact at path `name`\n* `Entry.ref`: If the entry was stored as a reference using `add_reference`, returns the URI  \n\nReferences that have schemes that W&B knows how to handle can be downloaded just like artifact files. For more information, see Track external files.  \n\nFirst, import the W&B SDK. Next, create an artifact from the Public API Class. Provide the entity, project, artifact, and alias associated with that artifact:", "query_id": "fcafab53-ed41-4fec-9639-d7dc0cd2878d", "label": "artifact"}
{"query": "What are the best practices for using artifacts to manage large datasets in Weights & Biases?", "matching_document": "## Track external files\n### Track artifacts outside of Weights & Biases\nUse Weights & Biases Artifacts for dataset versioning and model lineage, and use **reference artifacts** to track files saved outside the W&B server. In this mode an artifact only stores metadata about the files, such as URLs, size, and checksums. The underlying data never leaves your system. See the Quick start for information on how to save files and directories to W&B servers instead.  \n\nFor an example of tracking reference files in GCP, see the Guide to Tracking Artifacts by Reference.  \n\nThe following describes how to construct reference artifacts and how to best incorporate them into your workflows.  \n\n### Amazon S3 / GCS References  \n\nUse Weights & Biases Artifacts for dataset and model versioning to track references in cloud storage buckets. With artifact references, seamlessly layer tracking on top of your buckets with no modifications to your existing storage layout.  \n\n### Download a reference artifact  \n\n### Tying it together  \n\n### Filesystem References", "query_id": "3f8b3679-df1b-4e28-8990-94066bd84772", "label": "artifact"}
{"query": "Tracking and comparing LLM experiments in Weights & Biases", "matching_document": "### Using Weights & Biases to track experiments\n\nExperimenting with prompts, function calling and response model schema is critical to get good results. As LLM Engineers, we will be methodical and use Weights & Biases to track our experiments.  \n\nHere are a few things you should consider logging:  \n\n1. Save input and output pairs for later analysis\n2. Save the JSON schema for the response\\_model\n3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.  \n\nThis is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We will use the `wandb` library to track our experiments and save the results to a dashboard.", "query_id": "27a47969-0106-488b-8643-b509fb1e36ca", "label": "other"}
{"query": "log prompts wandb", "matching_document": "def log_index(vector_store_dir: str, run: \"wandb.run\"):\n    \"\"\"Log a vector store to wandb\n\n    Args:\n        vector_store_dir (str): The directory containing the vector store to log\n        run (wandb.run): The wandb run to log the artifact to.\n    \"\"\"\n    index_artifact = wandb.Artifact(name=\"vector_store\", type=\"search_index\")\n    index_artifact.add_dir(vector_store_dir)\n    run.log_artifact(index_artifact)\n\ndef log_prompt(prompt: dict, run: \"wandb.run\"):\n    \"\"\"Log a prompt to wandb\n\n    Args:\n        prompt (str): The prompt to log\n        run (wandb.run): The wandb run to log the artifact to.\n    \"\"\"\n    prompt_artifact = wandb.Artifact(name=\"chat_prompt\", type=\"prompt\")\n    with prompt_artifact.new_file(\"prompt.json\") as f:\n        f.write(json.dumps(prompt))\n    run.log_artifact(prompt_artifact)", "query_id": "fdbc1ed3-11d1-4187-b6a5-d4f595a0c7f6", "label": "other"}
{"query": "How to integrate LangChain with Weights & Biases", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "b47401c0-07c4-4e79-b122-785a09dac059", "label": "integrations"}
{"query": "exporting data from Weights & Biases charts", "matching_document": "\"  \n\n# Export Your Data from W&B  \n\nDescription: Learn how to export CSV, PDF, and LaTeX from W&B  \n\nBody:  \n\nWe're excited to announce our latest highly requested feature! You can now export tables, graphs and reports to CSV, LaTeX and PDF. Here are some examples of how you can export data easily from the Weights & Biases UI.  \n\nIn any graph, click on the arrow in the upper right and click \u201cPanel Export\u201d.  \n\nYou\u2019ll see a popup table with the underlying data - click Save as CSV to start the CSV download.  \n\nIn a table click the \u201cExport Table\u201d button in the upper right corner.  \n\nIn a report, click the \u201cDownload as LaTeX\u201d link.  \n\nThis will download a zip file with a report.tex and images of your panels attached. The .tex file is a latex file that you can modify to style your document. You can load the zip file into Overleaf.com and use their visual editor, or you can download pdflatex and run \u2018pdflatext report.tex\u2019 to generate a report.pdf file.  \n\nHere\u2019s an example of an exported LaTeX report:  \n\n\"", "query_id": "e7b2b6ce-10c6-4488-8ea7-c05aed28e1fd", "label": "visualisation"}
{"query": "How can LangChain be used in conjunction with Weights & Biases?", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "9862d742-5fb3-4cab-b24d-64df6ef18869", "label": "integrations"}
{"query": "logging distributed training wandb", "matching_document": "## Experiments FAQ\n#### How can I use wandb with multiprocessing, e.g. distributed training?\n\nIf your training program uses multiple processes you will need to structure your program to avoid making wandb method calls from processes where you did not run `wandb.init()`.\\\n\\\nThere are several approaches to managing multiprocess training:  \n\n1. Call `wandb.init` in all your processes, using the group keyword argument to define a shared group. Each process will have its own wandb run and the UI will group the training processes together.\n2. Call `wandb.init` from just one process and pass data to be logged over multiprocessing queues.  \n\n:::info\nCheck out the Distributed Training Guide for more detail on these two approaches, including code examples with Torch DDP.\n:::", "query_id": "9cfba31d-ee16-485e-aabd-4357e3506db5", "label": "other"}
{"query": "How to implement AWS IAM authentication in SageMaker training jobs?", "matching_document": "## Set up for SageMaker\n### Prerequisites\n1. **Setup SageMaker in your AWS account.** See the SageMaker Developer guide for more information.\n2. **Create an Amazon ECR repository** to store images you want to execute on Amazon SageMaker. See the Amazon ECR documentation for more information.\n3. **Create an Amazon S3 buckets** to store SageMaker inputs and outputs for your SageMaker training jobs. See the Amazon S3 documentation for more information. Make note of the S3 bucket URI and directory.\n4. **Create IAM execution role.** The role used in the SageMaker training job requires the following permissions to work. These permissions allow for logging events, pulling from ECR, and interacting with input and output buckets. (Note: if you already have this role for SageMaker training jobs, you do not need to create it again.)\nIAM role policy\n```\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"cloudwatch:PutMetricData\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:CreateLogGroup\",\n\"logs:DescribeLogStreams\",\n\"ecr:GetAuthorizationToken\"\n],\n\"Resource\": \"\\*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:ListBucket\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:GetObject\",\n\"s3:PutObject\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>/<object>\",\n\"arn:aws:s3:::<output-bucket>/<path>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:BatchGetImage\"\n],\n\"Resource\": \"arn:aws:ecr:<region>:<account-id>:repository/<repo>\"\n}\n]\n}\n\n```", "query_id": "5789c72d-9071-461b-9bb0-70ffbe32a6bd", "label": "other"}
{"query": "logging confusion matrix as chart in wandb", "matching_document": "## Log Plots\n#### Model Evaluation Charts\nYou can log this whenever your code has access to:  \n\n* a model's predicted scores (`predictions`) on a set of examples\n* the corresponding ground truth labels (`ground_truth`) for those examples\n* (optionally) a list of the labels/ class names (`labels=[\"cat\", \"dog\", \"bird\"...]` if label index 0 means cat, 1 = dog, 2 = bird, etc.)\n* (optionally) a subset (still in list format) of these labels to visualize on the plot  \n\nSee in the app \u2192  \n\nRun the code \u2192  \n\n`wandb.plot.confusion_matrix()`  \n\nCreate a multi-class confusion matrix in one line:  \n\n```\ncm = wandb.plot.confusion\\_matrix(\ny\\_true=ground\\_truth, preds=predictions, class\\_names=class\\_names\n)\n\nwandb.log({\"conf\\_mat\": cm})\n\n```  \n\nYou can log this wherever your code has access to:", "query_id": "09f90ac8-b788-4e79-9e78-1b778e328cff", "label": "visualisation"}
{"query": "Python code integration with W&B", "matching_document": "## Add W&B to your code\n#### Training script with W&B Python SDK\n1. Line 1: Import the Weights & Biases Python SDK.\n2. Line 6: Create a dictionary object where the key-value pairs define the sweep configuration. In the proceeding example, the batch size (`batch_size`), epochs (`epochs`), and the learning rate (`lr`) hyperparameters are varied during each sweep. For more information on how to create a sweep configuration, see Define sweep configuration.\n3. Line 19: Pass the sweep configuration dictionary to `wandb.sweep`. This initializes the sweep. This returns a sweep ID (`sweep_id`). For more information on how to initialize sweeps, see Initialize sweeps.\n4. Line 33: Use the `wandb.init()` API to generate a background process to sync and log data as a W&B Run.\n5. Line 37-39: (Optional) define values from `wandb.config` instead of defining hard coded values.\n6. Line 45: Log the metric we want to optimize with `wandb.log`. You must log the metric defined in your configuration. Within the configuration dictionary (`sweep_configuration` in this example) we defined the sweep to maximize the `val_acc` value).\n7. Line 54: Start the sweep with the `wandb.agent` API call. Provide the sweep ID (line 19), the name of the function the sweep will execute (`function=main`), and set the maximum number of runs to try to four (`count=4`). For more information on how to start W&B Sweep, see Start sweep agents.", "query_id": "b0c1be8a-e9c4-4726-bf32-2c6ef8ebe75c", "label": "integrations"}
{"query": "Securing AWS SageMaker training jobs with IAM roles", "matching_document": "## Set up for SageMaker\n### Prerequisites\n1. **Setup SageMaker in your AWS account.** See the SageMaker Developer guide for more information.\n2. **Create an Amazon ECR repository** to store images you want to execute on Amazon SageMaker. See the Amazon ECR documentation for more information.\n3. **Create an Amazon S3 buckets** to store SageMaker inputs and outputs for your SageMaker training jobs. See the Amazon S3 documentation for more information. Make note of the S3 bucket URI and directory.\n4. **Create IAM execution role.** The role used in the SageMaker training job requires the following permissions to work. These permissions allow for logging events, pulling from ECR, and interacting with input and output buckets. (Note: if you already have this role for SageMaker training jobs, you do not need to create it again.)\nIAM role policy\n```\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"cloudwatch:PutMetricData\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\",\n\"logs:CreateLogGroup\",\n\"logs:DescribeLogStreams\",\n\"ecr:GetAuthorizationToken\"\n],\n\"Resource\": \"\\*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:ListBucket\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:GetObject\",\n\"s3:PutObject\"\n],\n\"Resource\": [\n\"arn:aws:s3:::<input-bucket>/<object>\",\n\"arn:aws:s3:::<output-bucket>/<path>\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:BatchGetImage\"\n],\n\"Resource\": \"arn:aws:ecr:<region>:<account-id>:repository/<repo>\"\n}\n]\n}\n\n```", "query_id": "98d5212a-0136-4945-b96d-fff31946b9e6", "label": "other"}
{"query": "using tables in W&B", "matching_document": "---  \n\n## description: Explore how to use W&B Tables with this 5 minute Quickstart.  \n\n# Tables Quickstart  \n\nThe following Quickstart demonstrates how to log data tables, visualize data, and query data.  \n\nSelect the button below to try a PyTorch Quickstart example project on MNIST data.  \n\n## 1. Log a table  \n\nFollow the procedure outlined below to log a Table with W&B:\n1. Initialize a W&B Run with `wandb.init()`.\n2. Create a `wandb.Table()` object instance. Pass the name of the columns in your table along with the data for the `columns` and `data` parameters, respectively.  \n\n3. Log the table with `run.log()` as a key-value pair. Provide a name for your table for the key, and pass the object instance of `wandb.Table` as the value.  \n\n```\nrun = wandb.init(project=\"table-test\")\nmy\\_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]])\nrun.log({\"Table Name\": my\\_table})\n\n```  \n\n## 2. Visualize tables in the workspace  \n\n## 3. Compare across model versions", "query_id": "5822c07f-25fc-4365-b4cd-fa4c9bbb4a06", "label": "visualisation"}
{"query": "How to configure a sweep in Weights & Biases without a YAML file?", "matching_document": "## Sweep configuration structure\n### Basic structure\nBoth sweep configuration format options (YAML and Python dictionary) utilize key-value pairs and nested structures.  \n\nUse top-level keys within your sweep configuration to define qualities of your sweep search such as the name of the sweep (`name` key), the parameters to search through (`parameters` key), the methodology to search the parameter space (`method` key), and more.  \n\nFor example, the proceeding code snippets show the same sweep configuration defined within a YAML file and within a Python dictionary. Within the sweep configuration there are five top level keys specified: `program`, `name`, `method`, `metric` and `parameters`.  \n\n{label: 'CLI', value: 'cli'},\n{label: 'Python script or Jupyter notebook', value: 'script'},\n]}>  \n\nDefine a sweep in a Python dictionary data structure if you define training algorithm in a Python script or Jupyter notebook.  \n\nThe proceeding code snippet stores a sweep configuration in a variable named `sweep_configuration`:", "query_id": "4759b32f-bcc9-4062-8c3c-3ee18098a060", "label": "other"}
{"query": "How can LangChain be integrated with Weights & Biases?", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "ac1a6df3-71fc-43ea-855b-f6dca398a39b", "label": "integrations"}
{"query": "W&B report sharing", "matching_document": "---  \n\n## description: Collaborate and share W&B Reports with peers, co-workers, and your team.  \n\n# Collaborate on reports  \n\nCollaborate and Share W&B Reports  \n\nOnce you have saved a report, you can select the **Share** button to collaborate. A draft copy of the report is created when you select the **Edit** button. Draft reports auto-save. Select **Save to report** to publish your changes to the shared report.  \n\nA warning notification will appear if an edit conflict occurs. This can occur if you and another collaborator edit the same report at the same time. The warning notification will guide you to resolve potential edit conflicts.  \n\n### Comment on reports  \n\nClick the comment button on a panel in a report to add a comment directly to that panel.  \n\n### Who can edit and share reports?  \n\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.", "query_id": "8062c89c-e760-4a57-a02d-cbfd6f116b88", "label": "other"}
{"query": "What are common issues when logging distributed training with wandb?", "matching_document": "## Train model with DDP\nThe preceding image demonstrates the W&B App UI dashboard. On the sidebar we see two experiments. One labeled 'null' and a second (bound by a yellow box) called 'DPP'. If you expand the group (select the Group dropdown) you will see the W&B Runs that are associated to that experiment.  \n\n### Use W&B Service to avoid common distributed training issues.  \n\nThere are two common issues you might encounter when using W&B and distributed training:  \n\n1. **Hanging at the beginning of training** - A `wandb` process can hang if the `wandb` multiprocessing interferes with the multiprocessing from distributed training.\n2. **Hanging at the end of training** - A training job might hang if the `wandb` process does not know when it needs to exit. Call the `wandb.finish()` API at the end of your Python script to tell W&B that the Run finished. The wandb.finish() API will finish uploading data and will cause W&B to exit.  \n\n### Enable W&B Service  \n\n### Example use cases for multiprocessing", "query_id": "f9f1e8b8-1602-40b7-b275-d38ab134c5a7", "label": "other"}
{"query": "What are the key features of wandb?", "matching_document": "## Weights & Biases overview\nW&B is a platform that helps data scientists track their models, datasets, system information and more. With a few lines of code, you can start tracking everything about these features. It's free for personal use. Team use is normally a paid utility, but teams for academic purposes are free. You can use W&B with your favourite framework, like TensorFlow, Keras, PyTorch, Sklearn, fastai and many others.  \n\nAll tracking information is sent to a dedicated project page on the W&B UI, where you can open high quality visualizations, aggregate information and compare models or parameters. One of the advantages of remotely storing the experiment\u2019s information is that it is easy to collaborate on the same project and share the results with your teammates.  \n\nW&B provides 4 useful tools:  \n\n* Dashboard: Experiment tracking\n* Artifacts: Dataset versioning, model versioning\n* Sweeps: Hyperparameter optimization\n* Reports: Save and share reproducible findings", "query_id": "53de51ff-e1da-415e-b7f4-c69842a579e7", "label": "other"}
{"query": "What are the common issues when logging distributed training with wandb?", "matching_document": "## Train model with DDP\nThe preceding image demonstrates the W&B App UI dashboard. On the sidebar we see two experiments. One labeled 'null' and a second (bound by a yellow box) called 'DPP'. If you expand the group (select the Group dropdown) you will see the W&B Runs that are associated to that experiment.  \n\n### Use W&B Service to avoid common distributed training issues.  \n\nThere are two common issues you might encounter when using W&B and distributed training:  \n\n1. **Hanging at the beginning of training** - A `wandb` process can hang if the `wandb` multiprocessing interferes with the multiprocessing from distributed training.\n2. **Hanging at the end of training** - A training job might hang if the `wandb` process does not know when it needs to exit. Call the `wandb.finish()` API at the end of your Python script to tell W&B that the Run finished. The wandb.finish() API will finish uploading data and will cause W&B to exit.  \n\n### Enable W&B Service  \n\n### Example use cases for multiprocessing", "query_id": "973463b3-b7e5-448a-ad78-924503dee370", "label": "other"}
{"query": "best practices for tracking experiments in Weights & Biases", "matching_document": "## Create an Experiment\n### Best Practices\n\nThe following are some suggested guidelines to consider when you create experiments:  \n\n1. **Config**: Track hyperparameters, architecture, dataset, and anything else you'd like to use to reproduce your model. These will show up in columns\u2014 use config columns to group, sort, and filter runs dynamically in the app.\n2. **Project**: A project is a set of experiments you can compare together. Each project gets a dedicated dashboard page, and you can easily turn on and off different groups of runs to compare different model versions.\n3. **Notes**: A quick commit message to yourself. The note can be set from your script. You can edit notes at a later time on the Overview section of your project's dashboard on the W&B App.\n4. **Tags**: Identify baseline runs and favorite runs. You can filter runs using tags. You can edit tags at a later time on the Overview section of your project's dashboard on the W&B App.", "query_id": "ca5959c1-2b70-407c-a9c7-dda4c3306395", "label": "other"}
{"query": "Wandb init configuration", "matching_document": "## init\n| Arguments |  |\n| --- | --- |\n| `project` | (str, optional) The name of the project where you're sending the new run. If the project is not specified, the run is put in an \"Uncategorized\" project. |\n| `entity` | (str, optional) An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don't specify an entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under \"default location to create new projects\". |\n| `config` | (dict, argparse, absl.flags, str, optional) This sets `wandb.config`, a dictionary-like object for saving inputs to your job, like hyperparameters for a model or settings for a data preprocessing job. The config will show up in a table in the UI that you can use to group, filter, and sort runs. Keys should not contain `.` in their names, and values should be under 10 MB. If dict, argparse or absl.flags: will load the key value pairs into the `wandb.config` object. If str: will look for a yaml file by that name, and load config from that file into the `wandb.config` object. |\n| `save_code` | (bool, optional) Turn this on to save the main script or notebook to W&B. This is valuable for improving experiment reproducibility and to diff code across experiments in the UI. By default this is off, but you can flip the default behavior to on in your settings page. |\n| `group` | (str, optional) Specify a group to organize individual runs into a larger experiment. For example, you might be doing cross validation, or you might have multiple jobs that train and evaluate a model against different test sets. Group gives you a way to organize runs together into a larger whole, and you can toggle this on and off in the UI. For more details, see our guide to grouping runs. |", "query_id": "d7a79ed7-1cb9-4a59-8a9f-ce0b0b5ea428", "label": "other"}
{"query": "can you provide a bit more clarity on the difference between setting `resume` in `wandb.init` to `allow` vs. `auto`?\n\nI guess the difference has to do with whether the previous run crashed or not. I guess if the run didn't crash, `auto` may overwrite if there's matching `id`?", "matching_document": "| `resume` | (bool, str, optional) Sets the resuming behavior. Options: `\"allow\"`, `\"must\"`, `\"never\"`, `\"auto\"` or `None`. Defaults to `None`. Cases: - `None` (default): If the new run has the same ID as a previous run, this run overwrites that data. - `\"auto\"` (or `True`): if the previous run on this machine crashed, automatically resume it. Otherwise, start a new run. - `\"allow\"`: if id is set with `init(id=\"UNIQUE_ID\")` or `WANDB_RUN_ID=\"UNIQUE_ID\"` and it is identical to a previous run, wandb will automatically resume the run with that id. Otherwise, wandb will start a new run. - `\"never\"`: if id is set with `init(id=\"UNIQUE_ID\")` or `WANDB_RUN_ID=\"UNIQUE_ID\"` and it is identical to a previous run, wandb will crash. - `\"must\"`: if id is set with `init(id=\"UNIQUE_ID\")` or `WANDB_RUN_ID=\"UNIQUE_ID\"` and it is identical to a previous run, wandb will automatically resume the run with the id. Otherwise, wandb will crash. See our guide to resuming runs for more. |\n| `reinit` | (bool, optional) Allow multiple `wandb.init()` calls in the same process. (default: `False`) |\n| `magic` | (bool, dict, or str, optional) The bool controls whether we try to auto-instrument your script, capturing basic details of your run without you having to add more wandb code. (default: `False`) You can also pass a dict, json string, or yaml filename. |\n| `config_exclude_keys` | (list, optional) string keys to exclude from `wandb.config`. |\n| `config_include_keys` | (list, optional) string keys to include in `wandb.config`. |", "query_id": "ddf99f8f-9e1e-4e21-b4bd-943d24cce1ee", "label": "other"}
{"query": "W&B code artifacts", "matching_document": "---  \n\n## displayed\\_sidebar: default  \n\n# Code Saving  \n\nBy default, we only save the latest git commit hash. You can turn on more code features to compare the code between your experiments dynamically in the UI.  \n\nStarting with `wandb` version 0.8.28, we can save the code from your main training file where you call `wandb.init()`. This will get sync'd to the dashboard and show up in a tab on the run page, as well as the Code Comparer panel. Go to your settings page to enable code saving by default.  \n\n## Save Library Code  \n\nWhen code saving is enabled, wandb will save the code from the file that called `wandb.init()`. To save additional library code, you have two options:  \n\n* Call `wandb.run.log_code(\".\")` after calling `wandb.init()`\n* Pass a settings object to `wandb.init` with code\\_dir set: `wandb.init(settings=wandb.Settings(code_dir=\".\"))`  \n\n## Code Comparer  \n\n## Jupyter Session History  \n\n## Jupyter diffing", "query_id": "c90d0c2a-dc68-4bac-947d-aaa2b3b19bb1", "label": "artifact"}
{"query": "How to transfer runs between projects in wandb?", "matching_document": "---  \n\n## displayed\\_sidebar: default  \n\n# Manage Runs  \n\n### Move runs to a team  \n\nOn the project page:  \n\n1. Click the table tab to expand the runs table\n2. Click the checkbox to select all runs\n3. Click **Move**: the destination project can be in your personal account or any team that you're a member of.  \n\n### Send new runs to a team  \n\nIn your script, set the entity to your team. \"Entity\" just means your username or team name. Create an entity (personal account or team account) in the web app before sending runs there.  \n\n```\nwandb.init(entity=\"example-team\")\n\n```  \n\nYour **default entity** is updated when you join a team. This means that on your settings page, you'll see that the default location to create a new project is now the team you've just joined. Here's an example of what that settings page section looks like:", "query_id": "33945412-3dd7-47d0-bf7d-40b6c6a0d088", "label": "other"}
{"query": "Implementing Weights & Biases in a PyTorch project", "matching_document": "## \ud83d\udd25 = W&B \u2795 PyTorch\n\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.  \n\n## What this notebook covers:  \n\nWe show you how to integrate Weights & Biases with your PyTorch code to add experiment tracking to your pipeline.  \n\n## The resulting interactive W&B dashboard will look like:  \n\n## In pseudocode, what we'll do is:  \n\n```\n# import the library\nimport wandb\n\n# start a new experiment\nwandb.init(project=\"new-sota-model\")\n\n#\u2003capture a dictionary of hyperparameters with config\nwandb.config = {\"learning\\_rate\": 0.001, \"epochs\": 100, \"batch\\_size\": 128}\n\n# set up model and data\nmodel, dataloader = get\\_model(), get\\_data()\n\n# optional: track gradients\nwandb.watch(model)\n\nfor batch in dataloader:\nmetrics = model.training\\_step()\n#\u2003log metrics inside your training loop to visualize model performance\nwandb.log(metrics)\n\n# optional: save model at the end\nmodel.to\\_onnx()\nwandb.save(\"model.onnx\")\n\n```  \n\n## Follow along with a video tutorial!", "query_id": "e9cdca9f-dc3e-414c-ad96-e53ca57214b3", "label": "integrations"}
{"query": "confusion matrix visualization", "matching_document": "## Step 2: Create a custom chart for the confusion matrix\nW&B custom charts are written in [Vega](https://vega.github.io/vega/), a powerful and flexible visualization language. You can find many examples and walkthroughs online, and it can help to start with an existing preset that is most similar to your desired custom visualization. You can iterate from small changes in our [IDE](https://wandb.ai/wandb/posts/reports/The-W-B-Machine-Learning-Visualization-IDE--VmlldzoyOTQxNDY5/edit), which renders the plot as you change its definition.  \n\nHere is the full Vega spec for this multi-class confusion matrix:  \n\n* From your project workspace or report, click on \"Add a visualization\" and select \"Custom chart\"\n* Pick any existing preset and replace its definition with the Vega spec below\n* Click \"Save As\" to give this preset a name for easier reference (I recommend \"confusion_matrix\" :)", "query_id": "85487d87-02e8-4452-8d18-2a240d8a97fd", "label": "visualisation"}
{"query": "Weights & Biases features for LLM developers", "matching_document": "**Weights & Biases Prompts** is a suite of LLMOps tools built for the development of LLM-powered applications.  \n\nUse W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.  \n\n#### \ud83e\ude84 View Prompts In Action  \n\n**In this notebook we will demostrate W&B Prompts:**  \n\n* Using our 1-line LangChain integration\n* Using our Trace class when building your own LLM Pipelines  \n\nSee here for the full W&B Prompts documentation  \n\n## Installation  \n\n```\n!pip install \"wandb>=0.15.4\" -qqq\n!pip install \"langchain>=0.0.218\" openai -qqq\n\n```  \n\n```\nimport langchain\nassert langchain.__version__ >= \"0.0.218\", \"Please ensure you are using LangChain v0.0.188 or higher\"\n\n```  \n\n## Setup  \n\nThis demo requires that you have an OpenAI key  \n\n# W&B Prompts  \n\nW&B Prompts consists of three main components:  \n\n**Trace table**: Overview of the inputs and outputs of a chain.", "query_id": "7e58b85c-b91e-4d7e-8ee1-ee687a05138a", "label": "other"}
{"query": "tracking artifacts in wandb", "matching_document": "## Artifacts\n### How it works\nCreate an artifact with four lines of code:\n1. Create a W&B run.\n2. Create an artifact object with the `wandb.Artifact` API.\n3. Add one or more files, such as a model file or dataset, to your artifact object.\n4. Log your artifact to W&B.  \n\n`python showLineNumbers\nrun = wandb.init(project=\"artifacts-example\", job_type=\"add-dataset\")\nartifact = wandb.Artifact(name=\"my_data\", type=\"dataset\")\nartifact.add_dir(local_path=\"./dataset.h5\") # Add dataset directory to artifact\nrun.log_artifact(artifact) # Logs the artifact version \"my_data:v0\"`  \n\n:::tip\nThe preceding code snippet, and the colab linked on this page, show how to track files by uploading them to W&B. See the track external files page for information on how to add references to files or directories that are stored in external object storage (for example, in an Amazon S3 bucket).\n:::", "query_id": "04056895-371a-4a1a-b909-0cb8c39ec668", "label": "artifact"}
{"query": "Creating and tracking dataset versions using W&B", "matching_document": "## Dataset Versioning\n### Core Artifacts features\n1. **Upload**: Start tracking and versioning any data (files or directories) with `run.log_artifact()`. You can also track datasets in a remote filesystem (e.g. cloud storage in S3 or GCP) by reference, using a link or URI instead of the raw contents.\n2. **Version**: Define an artifact by giving it a type (`\"raw_data\"`, `\"preprocessed_data\"`, `\"balanced_data\"`) and a name (`\"imagenet_cats_10K\"`). When you log the same name again, W&B automatically creates a new version of the artifact with the latest contents.\n3. **Alias**: Set an alias like `\"best\"` or `\"production\"` to highlight the important versions in a lineage of artifacts.\n4. **Compare**: Select any two versions to browse the contents side-by-side. We're also working on a tool for dataset visualization, learn more here.\n5. **Download**: Obtain a local copy of the artifact or verify the contents by reference.  \n\nFor more detail on these features, check out Artifacts how it works.", "query_id": "603b12ed-f35f-44b1-81bc-d35561185fe2", "label": "artifact"}
{"query": "wandb.Table documentation", "matching_document": "## Table\n\nView source on GitHub  \n\nThe Table class used to display and analyze tabular data.  \n\n```\nTable(\ncolumns=None, data=None, rows=None, dataframe=None, dtype=None, optional=(True),\nallow\\_mixed\\_types=(False)\n)\n\n```  \n\nUnlike traditional spreadsheets, Tables support numerous types of data:\nscalar values, strings, numpy arrays, and most subclasses of `wandb.data_types.Media`.\nThis means you can embed `Images`, `Video`, `Audio`, and other sorts of rich, annotated media\ndirectly in Tables, alongside other traditional scalar values.  \n\nThis class is the primary class used to generate the Table Visualizer\nin the UI: <https://docs.wandb.ai/guides/data-vis/tables>.  \n\nTables can be constructed with initial data using the `data` or\n`dataframe` parameters:", "query_id": "24e0d7e2-6ef6-4b6f-a3f8-dd6d4d0f4eb1", "label": "visualisation"}
{"query": "Weights & Biases artifact storage", "matching_document": "## Storage\n\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.  \n\n## Manage storage consumption  \n\nW&B offers different methods of optimizing your storage consumption:  \n\n* Use\u00a0reference artifacts\u00a0to track files saved outside the W&B system, instead of uploading them to W&B storage.\n* Use an external cloud storage bucket for storage. *(Enterprise only)*  \n\n## Delete data  \n\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:  \n\n* Delete data interactively with the app UI.\n* Set a TTL policy on Artifacts so they are automatically deleted.", "query_id": "b37bc67b-b992-4445-b178-1a47956e7af2", "label": "artifact"}
{"query": "W&B sweep troubleshooting", "matching_document": "## Troubleshoot Sweeps\n\nTroubleshoot W&B Sweeps  \n\nTroubleshoot common error messages with the guidance suggested.  \n\n### `CommError, Run does not exist` and `ERROR Error uploading`  \n\nYour W&B Run ID might be defined if these two error messages are both returned. As an example, you might have a similar code snippet defined somewhere in your Jupyter Notebooks or Python script:  \n\n```\nwandb.init(id=\"some-string\")\n\n```  \n\nYou can not set a Run ID for W&B Sweeps because W&B automatically generates random, unique IDs for Runs created by W&B Sweeps.  \n\nW&B Run IDs need to be unique within a project.  \n\nWe recommend you pass a name to the name parameter when you initialized W&B, if you want to set a custom name that will appear on tables and graphs. For example:  \n\n```\nwandb.init(name=\"a helpful readable run name\")\n\n```  \n\n### `Cuda out of memory`  \n\n### `anaconda 400 error`  \n\nThe following error usually occurs when you do not log the metric that you are optimizing:", "query_id": "96b946ef-fc24-4e6e-8215-47cfb00f015e", "label": "other"}
{"query": "W&B team collaboration features", "matching_document": "## Teams\n\nUse W&B Teams as a central workspace for your ML team to build better models faster.  \n\n* **Track all the experiments** your team has tried so you never duplicate work.\n* **Save and reproduce** previously trained models.\n* **Share progress** and results with your boss and collaborators.\n* **Catch regressions** and immediately get alerted when performance drops.\n* **Benchmark model performance** and compare model versions.  \n\n## Create a collaborative team  \n\n1. **Sign up or log in** to your free W&B account.\n2. Click **Invite Team** in the navigation bar.\n3. Create your team and invite collaborators.  \n\n:::info\n**Note**: Only the admin of an organization can create a new team.\n:::  \n\n## Create a Team Profile  \n\nYou can customize your team's profile page to show an introduction and showcase reports and projects that are visible to the public or team members. Present reports, projects, and external links.  \n\n## Remove Team members  \n\n## Team Roles and Permissions  \n\n## Team Trials", "query_id": "78f710df-0a7e-46c1-b134-db27395d8737", "label": "other"}
{"query": "Weights & Biases artifact API", "matching_document": "Arguments:\n        name: A human-readable name for the artifact. Use the name to identify\n            a specific artifact in the W&B App UI or programmatically. You can\n            interactively reference an artifact with the `use_artifact` Public API.\n            A name can contain letters, numbers, underscores, hyphens, and dots.\n            The name must be unique across a project.\n        type: The artifact's type. Use the type of an artifact to both organize\n            and differentiate artifacts. You can use any string that contains letters,\n            numbers, underscores, hyphens, and dots. Common types include `dataset` or `model`.\n            Include `model` within your type string if you want to link the artifact\n            to the W&B Model Registry.\n        description: A description of the artifact. For Model or Dataset Artifacts,\n            add documentation for your standardized team model or dataset card. View\n            an artifact's description programmatically with the `Artifact.description`\n            attribute or programmatically with the W&B App UI. W&B renders the\n            description as markdown in the W&B App.\n        metadata: Additional information about an artifact. Specify metadata as a\n            dictionary of key-value pairs. You can specify no more than 100 total keys.\n\n    Returns:\n        An `Artifact` object.\n    \"\"\"", "query_id": "e6a928f6-fb9c-4a73-9b65-752ca0990382", "label": "artifact"}
{"query": "how to define W&B sweep in YAML", "matching_document": "## Add W&B to your code\n#### Training script with W&B Python SDK\nTo create a W&B Sweep, we first create a YAML configuration file. The configuration file contains he hyperparameters we want the sweep to explore. In the proceeding example, the batch size (`batch_size`), epochs (`epochs`), and the learning rate (`lr`) hyperparameters are varied during each sweep.  \n\n```\n# config.yaml\nprogram: train.py\nmethod: random\nname: sweep\nmetric:\ngoal: maximize\nname: val\\_acc\nparameters:\nbatch\\_size:\nvalues: [16,32,64]\nlr:\nmin: 0.0001\nmax: 0.1\nepochs:\nvalues: [5, 10, 15]\n\n```  \n\nFor more information on how to create a W&B Sweep configuration, see Define sweep configuration.  \n\nNote that you must provide the name of your Python script for the `program` key in your YAML file.  \n\nNext, we add the following to the code example:", "query_id": "35860a55-3f14-4de9-98a4-e735e7f092db", "label": "other"}
{"query": "Version control of datasets in machine learning projects", "matching_document": "## Version Control in Machine Learning\n### Data Version Control\n* Data preprocessing (data content changes) such as data cleaning, outlier handling, filling of missing values, etc.\n* Feature engineering (data becomes \"wider\") such as aggregation features, label encoding, scaling, etc.\n* Dataset splits (data is partitioned) typically mean dividing your data into training, validation, and testing data.\n* Dataset update (data becomes \"longer\") when new data points are available.", "query_id": "5dc2c71c-00c2-4be4-823a-05347d8154c0", "label": "other"}
{"query": "Weights & Biases model registry tutorial", "matching_document": "## Model registry\n### How it works\n1. **Log a model version**: In your training script, add a few lines of code to save the model files as an artifact to W&B.\n2. **Compare performance**: Check live charts to compare the metrics and sample predictions from model training and validation. Identify which model version performed the best.\n3. **Link to registry**: Bookmark the best model version by linking it to a registered model, either programmatically in Python or interactively in the W&B UI.  \n\nThe following code snippet demonstrates how to log and link a model to the Model Registry:  \n\n```python showLineNumbers\nimport wandb\nimport random\n\n# Start a new W&B run\n\nrun = wandb.init(project=\"models\\_quickstart\")\n\n# Simulate logging model metrics\n\nrun.log({\"acc\": random.random()})\n\n# Create a simulated model file\n\nwith open(\"my\\_model.h5\", \"w\") as f:\nf.write(\"Model: \" + str(random.random()))\n\n# Log and link the model to the Model Registry\n\nrun.link\\_model(path=\"./my\\_model.h5\", registered\\_model\\_name=\"MNIST\")\n\nrun.finish()\n```", "query_id": "54d7cff9-416c-4312-a458-a59908b4e306", "label": "other"}
{"query": "access artifact path W&B", "matching_document": "## Artifact\n### Methods\n#### `get_added_local_path_name`\n\nView source  \n\n```\nget\\_added\\_local\\_path\\_name(\nlocal\\_path: str\n) -> Optional[str]\n\n```  \n\nGet the artifact relative name of a file added by a local filesystem path.  \n\n| Arguments |  |\n| --- | --- |\n| `local_path` | The local path to resolve into an artifact relative name. |  \n\n| Returns |  |\n| --- | --- |\n| The artifact relative name. |  |  \n\n#### Examples:  \n\nBasic usage:  \n\n```\nartifact = wandb.Artifact(\"my\\_dataset\", type=\"dataset\")\nartifact.add\\_file(\"path/to/file.txt\", name=\"artifact/path/file.txt\")\n\n# Returns `artifact/path/file.txt`:\nname = artifact.get\\_added\\_local\\_path\\_name(\"path/to/file.txt\")\n\n```", "query_id": "29106b19-3e73-4fc7-9f03-f0182753bd45", "label": "artifact"}
{"query": "optimize W&B storage", "matching_document": "## Storage\n\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.  \n\n## Manage storage consumption  \n\nW&B offers different methods of optimizing your storage consumption:  \n\n* Use\u00a0reference artifacts\u00a0to track files saved outside the W&B system, instead of uploading them to W&B storage.\n* Use an external cloud storage bucket for storage. *(Enterprise only)*  \n\n## Delete data  \n\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:  \n\n* Delete data interactively with the app UI.\n* Set a TTL policy on Artifacts so they are automatically deleted.", "query_id": "7bbab76d-aa7a-4d9b-b88d-3103716d4d62", "label": "other"}
{"query": "What does setting the 'resume' parameter to 'allow' do in wandb.init?", "matching_document": "## Resume Runs\n#### Resume Guidance\n##### Automatic and controlled resuming\n\nAutomatic resuming only works if the process is restarted on top of the same filesystem as the failed process. If you can't share a filesystem, we allow you to set the `WANDB_RUN_ID`: a globally unique string (per project) corresponding to a single run of your script. It must be no longer than 64 characters. All non-word characters will be converted to dashes.  \n\n```\n# store this id to use it later when resuming\nid = wandb.util.generate\\_id()\nwandb.init(id=id, resume=\"allow\")\n# or via environment variables\nos.environ[\"WANDB\\_RESUME\"] = \"allow\"\nos.environ[\"WANDB\\_RUN\\_ID\"] = wandb.util.generate\\_id()\nwandb.init()\n\n```  \n\nIf you set `WANDB_RESUME` equal to `\"allow\"`, you can always set `WANDB_RUN_ID` to a unique string and restarts of the process will be handled automatically. If you set `WANDB_RESUME` equal to `\"must\"`, W&B will throw an error if the run to be resumed does not exist yet instead of auto-creating a new run.", "query_id": "5f9e6e76-bcb9-48b5-9337-8f4504395b95", "label": "other"}
{"query": "wandb.log confusion matrix chart", "matching_document": "'  \n\n# Confusion Matrix: Usage and Examples  \n\nDescription: In this article, we review the usage and examples for a multi-class confusion matrix using Weights & Biases.  \n\nBody:  \n\n# Method: wandb.plot.confusion_matrix()  \n\nLog a multi-class [confusion matrix](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) in one line:  \n\n```\nwandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\ny_true=ground_truth, preds=predictions,\nclass_names=class_names)})\n```  \n\nYou can log this wherever your code has access to:  \n\n# Basic Usage  \n\nIn this toy example, I finetune a CNN to predict one of 10 classes of living things in a photo (plants, animals, insects) while varying the number of epochs (E or pretrain_epochs) and the number of training examples (NT or num_train). I log a confusion matrix in each validation step, after each training epoch (so only the last confusion matrix at the end of training is visualized).  \n\n# Powerful Interactions  \n\n# Logging Details", "query_id": "9329ba15-8755-429b-8c82-d95805f59ace", "label": "visualisation"}
{"query": "using LangChain with Weights & Biases", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "8e3be407-2e9c-4f39-ad37-227906befc59", "label": "integrations"}
{"query": "WandB API examples for sweeps", "matching_document": "\"\"\"A set of runs associated with a sweep.\n\n    Examples:\n        Instantiate with:\n        ```\n        api = wandb.Api()\n        sweep = api.sweep(path/to/sweep)\n        ```\n\n    Attributes:\n        runs: (`Runs`) list of runs\n        id: (str) sweep id\n        project: (str) name of project\n        config: (str) dictionary of sweep configuration\n        state: (str) the state of the sweep\n        expected_run_count: (int) number of expected runs for the sweep\n    \"\"\"", "query_id": "ff93dae5-ead7-4d27-a3ab-a4a376663963", "label": "other"}
{"query": "What are the steps to connect LangChain with Weights & Biases?", "matching_document": "## Prompts Quickstart\n### Use W&B Trace with LangChain\n#### 1. Set the LANGCHAIN\\_WANDB\\_TRACING environment variable\nFirst, set the LANGCHAIN\\_WANDB\\_TRACING environment variable to true. This will turn on automated Weights & Biases logging with LangChain:  \n\n```\nimport os\n\n# turn on wandb logging for langchain\nos.environ[\"LANGCHAIN\\_WANDB\\_TRACING\"] = \"true\"\n\n```  \n\nThats it! Now any call to a LangChain LLM, Chain, Tool or Agent will be logged to Weights & Biases.", "query_id": "e961a5f9-2447-4925-ae7e-8f34e8418a02", "label": "integrations"}
{"query": "How to version models using Weights & Biases model registry?", "matching_document": "## Link a model version\n### Interactively link a model\n1. Navigate to your project's artifact browser on the W&B App at: `https://wandb.ai/<entity>/<project>/artifacts`\n2. Select the Artifacts icon on the left sidebar.\n3. Click on the model version you want to link to your registry.\n4. Within the **Version overview** section, click the **Link to registry** button.\n5. From the modal that appears on the right of the screen, select a registered model from the **Select a register model** menu dropdown.\n6. Click **Next step**.\n7. (Optional) Select an alias from the **Aliases** dropdown.\n8. Click **Link to registry**.", "query_id": "1a123420-2fcc-4671-aacc-7db790f9968b", "label": "artifact"}
{"query": "How to invite team members to a shared project in Weights & Biases?", "matching_document": "## Teams\n### Invite team members\nInvite new members to your team.\n1. Ensure the team member already has a W&B Account.\n2. Navigate to <https://wandb.ai/subscriptions>.\n3. Select **Manage members**.\n4. A model will appear. Provide the username or email for the **Email or Username** field, select a team for them to join from the **Team** dropdown menu, and select a role type from the **Organizational Role** dropdown menu.  \n\n1. Select the **Add** button.  \n\n:::info\n\\* If you have an Enterprise account, please contact your Account Executive to invite new members to your team.\n:::", "query_id": "bde88efe-6718-4faf-870c-b2139de5daf5", "label": "other"}
{"query": "Weights & Biases reports", "matching_document": "## Create a report\n\nCreate a W&B Report  \n\nCreate a report interactively with the W&B App UI or programmatically with the W&B Python SDK.  \n\n:::info\nCreating Reports programmatically with the Python SDK is in Beta and in active development. See this Google Colab for an example.\n:::  \n\n1. Navigate to your project workspace in the W&B App.\n2. Click **Create report** in the upper right corner of your workspace.  \n\n1. A modal will appear. Select the charts you would like to start with. You can add or delete charts later from the report interface.  \n\n1. Select the **Filter run sets** option to prevent new runs from being added to your report. You can toggle this option on or off. Once you click **Create report,** a draft report will be available in the report tab to continue working on.  \n\n1. Navigate to your project workspace in the W&B App.\n2. Select to the **Reports** tab (clipboard image) in your project.\n3. Select the **Create Report** button on the report page.", "query_id": "54330a7f-91ce-489b-aee6-df46f172655d", "label": "visualisation"} 
{"query": "wandb setup", "matching_document": "## \ud83d\ude80 Setup\n\nStart out by installing the experiment tracking library and setting up your free W&B account:  \n\n1. Install with `!pip install`\n2. `import` the library into Python\n3. `.login()` so you can log metrics to your projects  \n\nIf you've never used Weights & Biases before,\nthe call to `login` will give you a link to sign up for an account.\nW&B is free to use for personal and academic projects!  \n\n```\n!pip install wandb -Uq\n\n```  \n\n```\nimport wandb\n\n```  \n\n```\nwandb.login()\n\n```", "query_id": "d5fec68e-cdc4-44ed-89a7-b7a9d1487f25", "label": "other"}

# Terminal Chatbot Example

> [!TIP]
> For Chinese version, please refer to [README_ZH.md](README_ZH.md).

This example demonstrates how to create interactive chatbot interfaces with MCPChatbot:

- Regular mode (`chatbot_terminal.py`): Interactive terminal chat with complete responses
- Streaming mode (`chatbot_terminal_stream.py`): Interactive terminal chat with streaming responses

## Usage

### Regular Mode

```bash
python example/chatbot_terminal/chatbot_terminal.py
```

Options:

- `--llm [openai|ollama]`: Choose LLM provider (default: openai)
- `--no-workflow`: Hide workflow trace display

This script will:

1. Initialize the MCP servers
2. Start an interactive chat session
3. Process your prompts and display full responses
4. Show the workflow trace after each response (unless disabled)

### Streaming Mode

```bash
python example/chatbot_terminal/chatbot_terminal_stream.py
```

Options:

- `--llm [openai|ollama]`: Choose LLM provider (default: openai)
- `--no-workflow`: Hide workflow trace display

This script will:

1. Initialize the MCP servers
2. Start an interactive chat session
3. Display responses in real-time, including:
   - Status updates
   - Tool processing steps
   - Incremental response chunks
4. Show the workflow trace after completion (unless disabled)

## Example Output

<details>
<summary>Click to expand example output</summary>

```text
â•°â”€ python example/chatbot_terminal/chatbot_terminal.py --llm openai                  â”€â•¯
[04/11/25 00:32:41] INFO     Processing request of type ListToolsRequest   server.py:432
                    INFO     Processing request of type ListToolsRequest   server.py:432
You: ä½ å¥½ï¼Œèƒ½å¸®æˆ‘æ€»ç»“ä¸‹ /Users/{{USER_NAME}}/OpenSource/mcp_chatbot/README.md è¿™ä¸ª readme ä¸­æ˜¯å¦æœ‰æŒ‡å¯¼æˆ‘å¦‚ä½•é…ç½®é¡¹ç›®çš„tutorialï¼Ÿ 
2025-04-11 00:33:26,088 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-11 00:33:26,094 - INFO - 
[Debug] LLM Response: {
    "tool": "read_markdown_file",
    "arguments": {
        "directory_path": "/Users/{{USER_NAME}}/OpenSource/mcp_chatbot"
    }
}
2025-04-11 00:33:26,095 - INFO - Executing read_markdown_file...
[04/11/25 00:33:26] INFO     Processing request of type CallToolRequest    server.py:432
2025-04-11 00:33:26,102 - INFO - 
[Debug] Tool Results: Tool execution results:

Tool Call 1:
Tool Name: read_markdown_file
- Arguments: {
  "directory_path": "/Users/{{USER_NAME}}/OpenSource/mcp_chatbot"
}
- Tool call result: meta=None content=[TextContent(type='text', text='# MCPChatbot Example\n\nThis project demonstrates how to integrate the Model Context Protocol (MCP) with customized LLM (e.g. Qwen), creating a powerf

2025-04-11 00:33:33,314 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-11 00:33:33,316 - INFO - 
[Debug] LLM Response: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“**ï¼šä½¿ç”¨`git clone`å‘½ä»¤å°†é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ã€‚
2. **è®¾ç½®è™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼šåˆ›å»ºå¹¶æ¿€æ´»ä¸€ä¸ªPython 3.10+çš„è™šæ‹Ÿç¯å¢ƒã€‚
3. **å®‰è£…ä¾èµ–**ï¼šé€šè¿‡`pip install -r requirements.txt`æˆ–ä½¿ç”¨`uv pip install -r requirements.txt`æ¥å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ã€‚
4. **é…ç½®ç¯å¢ƒ**ï¼š
   - å¤åˆ¶`.env.example`æ–‡ä»¶ä¸º`.env`ã€‚
   - ç¼–è¾‘`.env`æ–‡ä»¶ä»¥æ·»åŠ ä½ çš„LLM APIå¯†é’¥å’Œç›¸å…³è·¯å¾„ç­‰ä¿¡æ¯ã€‚
5. **é‡è¦é…ç½®è¯´æ˜**ï¼š
   - ä¿®æ”¹`mcp_servers/servers_config.json`ä»¥åŒ¹é…æ‚¨çš„æœ¬åœ°MCPæœåŠ¡å™¨è®¾ç½®ã€‚
   - ç¡®ä¿åœ¨`.env`æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®äº†Markdownæ–‡ä»¶å¤¹è·¯å¾„å’Œå…¶ä»–å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚

æ­¤å¤–ï¼Œæ–‡æ¡£è¿˜æä¾›äº†è¿è¡Œå‰æ£€æŸ¥é…ç½®çš„è„šæœ¬ä»¥åŠå¦‚ä½•å¯åŠ¨åŸºæœ¬èŠå¤©ç•Œé¢å’Œç¤ºä¾‹è„šæœ¬çš„æ–¹æ³•ã€‚å¦‚æœä½ æŒ‰ç…§è¿™äº›æ­¥éª¤æ“ä½œï¼Œåº”è¯¥èƒ½å¤ŸæˆåŠŸé…ç½®å¹¶è¿è¡Œè¿™ä¸ªé¡¹ç›®ã€‚
2025-04-11 00:33:33,317 - INFO - WorkflowTrace
â”œâ”€â”€ ğŸ” USER_QUERY: ä½ å¥½ï¼Œèƒ½å¸®æˆ‘æ€»ç»“ä¸‹ /Users/{{USER_NAME}}/OpenSource/mcp_chatbot/RE
â”œâ”€â”€ ğŸ’­ LLM_THINKING: LLM is processing your query...
â”œâ”€â”€ ğŸ¤– LLM_RESPONSE: { "tool": "read_markdown_file", "arguments
â”œâ”€â”€ ğŸ”§ TOOL_CALL: Call 1: read_markdown_file
â”‚   â””â”€â”€ Tool: read_markdown_file, Args: {"directory_path": "/Users/{{USER_NAME}}/OpenSource/m...
â”œâ”€â”€ âš¡ï¸ TOOL_EXECUTION: Executing read_markdown_file...
â”œâ”€â”€ ğŸ“Š TOOL_RESULT: Success
â”‚   â””â”€â”€ Status: Success
â”‚      â””â”€â”€ Result: meta=None content=[TextContent(type='text', tex...
â”œâ”€â”€ ğŸ’­ LLM_THINKING: LLM processing tool results (iteration 1)...
â”œâ”€â”€ ğŸ¤– LLM_RESPONSE: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“
â””â”€â”€ âœ… FINAL_RESPONSE: Final response after 1 tool iterations
Assistant: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“**ï¼šä½¿ç”¨`git clone`å‘½ä»¤å°†é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ã€‚
2. **è®¾ç½®è™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼šåˆ›å»ºå¹¶æ¿€æ´»ä¸€ä¸ªPython 3.10+çš„è™šæ‹Ÿç¯å¢ƒã€‚
3. **å®‰è£…ä¾èµ–**ï¼šé€šè¿‡`pip install -r requirements.txt`æˆ–ä½¿ç”¨`uv pip install -r requirements.txt`æ¥å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ã€‚
4. **é…ç½®ç¯å¢ƒ**ï¼š
   - å¤åˆ¶`.env.example`æ–‡ä»¶ä¸º`.env`ã€‚
   - ç¼–è¾‘`.env`æ–‡ä»¶ä»¥æ·»åŠ ä½ çš„LLM APIå¯†é’¥å’Œç›¸å…³è·¯å¾„ç­‰ä¿¡æ¯ã€‚
5. **é‡è¦é…ç½®è¯´æ˜**ï¼š
   - ä¿®æ”¹`mcp_servers/servers_config.json`ä»¥åŒ¹é…æ‚¨çš„æœ¬åœ°MCPæœåŠ¡å™¨è®¾ç½®ã€‚
   - ç¡®ä¿åœ¨`.env`æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®äº†Markdownæ–‡ä»¶å¤¹è·¯å¾„å’Œå…¶ä»–å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚

æ­¤å¤–ï¼Œæ–‡æ¡£è¿˜æä¾›äº†è¿è¡Œå‰æ£€æŸ¥é…ç½®çš„è„šæœ¬ä»¥åŠå¦‚ä½•å¯åŠ¨åŸºæœ¬èŠå¤©ç•Œé¢å’Œç¤ºä¾‹è„šæœ¬çš„æ–¹æ³•ã€‚å¦‚æœä½ æŒ‰ç…§è¿™äº›æ­¥éª¤æ“ä½œï¼Œåº”è¯¥èƒ½å¤ŸæˆåŠŸé…ç½®å¹¶è¿è¡Œè¿™ä¸ªé¡¹ç›®ã€‚
```

</details>

## Configuration

> [!TIP]
> Before running the example:
>
> 1. Make sure you've configured the `.env` file with proper API keys
> 2. Set up the MCP servers as described in the main README
> 3. Run `bash scripts/check.sh` to verify your environment setup

## Features

- ğŸŒˆ **Colorful Terminal Output**: Uses colorama for better readability
- ğŸ”€ **Workflow Visualization**: Optionally displays the workflow trace after each response
- ğŸ”„ **Interactive Session**: Maintains context between messages in a conversation
- ğŸ“Š **Tool Integration**: Shows real-time tool calls and results
- ğŸšª **Easy Exit**: Type "exit" or "quit" to end the session

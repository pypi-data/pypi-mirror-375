# ç»ˆç«¯èŠå¤©æœºå™¨äººç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ MCPChatbot åˆ›å»ºäº¤äº’å¼èŠå¤©æœºå™¨äººç•Œé¢ï¼š

- å¸¸è§„æ¨¡å¼ï¼ˆ`chatbot_terminal.py`ï¼‰ï¼šå¸¦æœ‰å®Œæ•´å“åº”çš„äº¤äº’å¼ç»ˆç«¯èŠå¤©
- æµå¼æ¨¡å¼ï¼ˆ`chatbot_terminal_stream.py`ï¼‰ï¼šå¸¦æœ‰æµå¼å“åº”çš„äº¤äº’å¼ç»ˆç«¯èŠå¤©

## ä½¿ç”¨æ–¹æ³•

### å¸¸è§„æ¨¡å¼

```bash
python example/chatbot_terminal/chatbot_terminal.py
```

é€‰é¡¹ï¼š

- `--llm [openai|ollama]`ï¼šé€‰æ‹© LLM æä¾›è€…ï¼ˆé»˜è®¤ï¼šopenaiï¼‰
- `--no-workflow`ï¼šéšè—å·¥ä½œæµç¨‹è·Ÿè¸ªæ˜¾ç¤º

è¯¥è„šæœ¬å°†ï¼š

1. åˆå§‹åŒ– MCP æœåŠ¡å™¨
2. å¯åŠ¨äº¤äº’å¼èŠå¤©ä¼šè¯
3. å¤„ç†æ‚¨çš„æç¤ºå¹¶æ˜¾ç¤ºå®Œæ•´å“åº”
4. åœ¨æ¯ä¸ªå“åº”åæ˜¾ç¤ºå·¥ä½œæµç¨‹è·Ÿè¸ªï¼ˆé™¤éç¦ç”¨ï¼‰

### æµå¼æ¨¡å¼

```bash
python example/chatbot_terminal/chatbot_terminal_stream.py
```

é€‰é¡¹ï¼š

- `--llm [openai|ollama]`ï¼šé€‰æ‹© LLM æä¾›è€…ï¼ˆé»˜è®¤ï¼šopenaiï¼‰
- `--no-workflow`ï¼šéšè—å·¥ä½œæµç¨‹è·Ÿè¸ªæ˜¾ç¤º

è¯¥è„šæœ¬å°†ï¼š

1. åˆå§‹åŒ– MCP æœåŠ¡å™¨
2. å¯åŠ¨äº¤äº’å¼èŠå¤©ä¼šè¯
3. å®æ—¶æ˜¾ç¤ºå“åº”ï¼ŒåŒ…æ‹¬ï¼š
   - çŠ¶æ€æ›´æ–°
   - å·¥å…·å¤„ç†æ­¥éª¤
   - å¢é‡å“åº”å—
4. å®Œæˆåæ˜¾ç¤ºå·¥ä½œæµç¨‹è·Ÿè¸ªï¼ˆé™¤éç¦ç”¨ï¼‰

## ç¤ºä¾‹è¾“å‡º

<details>
<summary>ç‚¹å‡»å±•å¼€ç¤ºä¾‹è¾“å‡º</summary>

```text
â•°â”€ python example/chatbot_terminal/chatbot_terminal.py --llm openai                  â”€â•¯
[04/11/25 00:32:41] INFO     Processing request of type ListToolsRequest   server.py:432
                    INFO     Processing request of type ListToolsRequest   server.py:432
You: ä½ å¥½ï¼Œèƒ½å¸®æˆ‘æ€»ç»“ä¸‹ /Users/{{USER_NAME}}/OpenSource/mcp_chatbot/README.md è¿™ä¸ª readme ä¸­æ˜¯å¦æœ‰æŒ‡å¯¼æˆ‘å¦‚ä½•é…ç½®é¡¹ç›®çš„tutorialï¼Ÿ 
2025-04-11 00:33:26,088 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-11 00:33:26,094 - INFO - 
[Debug] LLM Response: {
    "tool": "read_markdown_file",
    "arguments": {
        "directory_path": "/Users/{{USER_NAME}}/OpenSource/mcp_chatbot"
    }
}
2025-04-11 00:33:26,095 - INFO - Executing read_markdown_file...
[04/11/25 00:33:26] INFO     Processing request of type CallToolRequest    server.py:432
2025-04-11 00:33:26,102 - INFO - 
[Debug] Tool Results: Tool execution results:

Tool Call 1:
Tool Name: read_markdown_file
- Arguments: {
  "directory_path": "/Users/{{USER_NAME}}/OpenSource/mcp_chatbot"
}
- Tool call result: meta=None content=[TextContent(type='text', text='# MCPChatbot Example\n\nThis project demonstrates how to integrate the Model Context Protocol (MCP) with customized LLM (e.g. Qwen), creating a powerf

2025-04-11 00:33:33,314 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions "HTTP/1.1 200 OK"
2025-04-11 00:33:33,316 - INFO - 
[Debug] LLM Response: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“**ï¼šä½¿ç”¨`git clone`å‘½ä»¤å°†é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ã€‚
2. **è®¾ç½®è™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼šåˆ›å»ºå¹¶æ¿€æ´»ä¸€ä¸ªPython 3.10+çš„è™šæ‹Ÿç¯å¢ƒã€‚
3. **å®‰è£…ä¾èµ–**ï¼šé€šè¿‡`pip install -r requirements.txt`æˆ–ä½¿ç”¨`uv pip install -r requirements.txt`æ¥å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ã€‚
4. **é…ç½®ç¯å¢ƒ**ï¼š
   - å¤åˆ¶`.env.example`æ–‡ä»¶ä¸º`.env`ã€‚
   - ç¼–è¾‘`.env`æ–‡ä»¶ä»¥æ·»åŠ ä½ çš„LLM APIå¯†é’¥å’Œç›¸å…³è·¯å¾„ç­‰ä¿¡æ¯ã€‚
5. **é‡è¦é…ç½®è¯´æ˜**ï¼š
   - ä¿®æ”¹`mcp_servers/servers_config.json`ä»¥åŒ¹é…æ‚¨çš„æœ¬åœ°MCPæœåŠ¡å™¨è®¾ç½®ã€‚
   - ç¡®ä¿åœ¨`.env`æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®äº†Markdownæ–‡ä»¶å¤¹è·¯å¾„å’Œå…¶ä»–å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚

æ­¤å¤–ï¼Œæ–‡æ¡£è¿˜æä¾›äº†è¿è¡Œå‰æ£€æŸ¥é…ç½®çš„è„šæœ¬ä»¥åŠå¦‚ä½•å¯åŠ¨åŸºæœ¬èŠå¤©ç•Œé¢å’Œç¤ºä¾‹è„šæœ¬çš„æ–¹æ³•ã€‚å¦‚æœä½ æŒ‰ç…§è¿™äº›æ­¥éª¤æ“ä½œï¼Œåº”è¯¥èƒ½å¤ŸæˆåŠŸé…ç½®å¹¶è¿è¡Œè¿™ä¸ªé¡¹ç›®ã€‚
2025-04-11 00:33:33,317 - INFO - WorkflowTrace
â”œâ”€â”€ ğŸ” USER_QUERY: ä½ å¥½ï¼Œèƒ½å¸®æˆ‘æ€»ç»“ä¸‹ /Users/{{USER_NAME}}/OpenSource/mcp_chatbot/RE
â”œâ”€â”€ ğŸ’­ LLM_THINKING: LLM is processing your query...
â”œâ”€â”€ ğŸ¤– LLM_RESPONSE: { "tool": "read_markdown_file", "arguments
â”œâ”€â”€ ğŸ”§ TOOL_CALL: Call 1: read_markdown_file
â”‚   â””â”€â”€ Tool: read_markdown_file, Args: {"directory_path": "/Users/{{USER_NAME}}/OpenSource/m...
â”œâ”€â”€ âš¡ï¸ TOOL_EXECUTION: Executing read_markdown_file...
â”œâ”€â”€ ğŸ“Š TOOL_RESULT: Success
â”‚   â””â”€â”€ Status: Success
â”‚      â””â”€â”€ Result: meta=None content=[TextContent(type='text', tex...
â”œâ”€â”€ ğŸ’­ LLM_THINKING: LLM processing tool results (iteration 1)...
â”œâ”€â”€ ğŸ¤– LLM_RESPONSE: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“
â””â”€â”€ âœ… FINAL_RESPONSE: Final response after 1 tool iterations
Assistant: åœ¨README.mdæ–‡ä»¶ä¸­ï¼Œç¡®å®æœ‰æŒ‡å¯¼å¦‚ä½•é…ç½®é¡¹ç›®çš„æ•™ç¨‹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®æ­¥éª¤ï¼š

1. **å…‹éš†ä»“åº“**ï¼šä½¿ç”¨`git clone`å‘½ä»¤å°†é¡¹ç›®å…‹éš†åˆ°æœ¬åœ°ã€‚
2. **è®¾ç½®è™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼šåˆ›å»ºå¹¶æ¿€æ´»ä¸€ä¸ªPython 3.10+çš„è™šæ‹Ÿç¯å¢ƒã€‚
3. **å®‰è£…ä¾èµ–**ï¼šé€šè¿‡`pip install -r requirements.txt`æˆ–ä½¿ç”¨`uv pip install -r requirements.txt`æ¥å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ã€‚
4. **é…ç½®ç¯å¢ƒ**ï¼š
   - å¤åˆ¶`.env.example`æ–‡ä»¶ä¸º`.env`ã€‚
   - ç¼–è¾‘`.env`æ–‡ä»¶ä»¥æ·»åŠ ä½ çš„LLM APIå¯†é’¥å’Œç›¸å…³è·¯å¾„ç­‰ä¿¡æ¯ã€‚
5. **é‡è¦é…ç½®è¯´æ˜**ï¼š
   - ä¿®æ”¹`mcp_servers/servers_config.json`ä»¥åŒ¹é…æ‚¨çš„æœ¬åœ°MCPæœåŠ¡å™¨è®¾ç½®ã€‚
   - ç¡®ä¿åœ¨`.env`æ–‡ä»¶ä¸­æ­£ç¡®è®¾ç½®äº†Markdownæ–‡ä»¶å¤¹è·¯å¾„å’Œå…¶ä»–å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚

æ­¤å¤–ï¼Œæ–‡æ¡£è¿˜æä¾›äº†è¿è¡Œå‰æ£€æŸ¥é…ç½®çš„è„šæœ¬ä»¥åŠå¦‚ä½•å¯åŠ¨åŸºæœ¬èŠå¤©ç•Œé¢å’Œç¤ºä¾‹è„šæœ¬çš„æ–¹æ³•ã€‚å¦‚æœä½ æŒ‰ç…§è¿™äº›æ­¥éª¤æ“ä½œï¼Œåº”è¯¥èƒ½å¤ŸæˆåŠŸé…ç½®å¹¶è¿è¡Œè¿™ä¸ªé¡¹ç›®ã€‚
```

</details>

## é…ç½®

> [!TIP]
> åœ¨è¿è¡Œç¤ºä¾‹ä¹‹å‰ï¼š
>
> 1. ç¡®ä¿æ‚¨å·²åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®äº†æ­£ç¡®çš„ API å¯†é’¥
> 2. æŒ‰ç…§ä¸» README ä¸­çš„è¯´æ˜è®¾ç½® MCP æœåŠ¡å™¨
> 3. è¿è¡Œ `bash scripts/check.sh` ä»¥éªŒè¯æ‚¨çš„ç¯å¢ƒè®¾ç½®

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸŒˆ **å½©è‰²ç»ˆç«¯è¾“å‡º**ï¼šä½¿ç”¨ colorama æé«˜å¯è¯»æ€§
- ğŸ”€ **å·¥ä½œæµå¯è§†åŒ–**ï¼šå¯é€‰æ‹©æ€§åœ°åœ¨æ¯ä¸ªå“åº”åæ˜¾ç¤ºå·¥ä½œæµè·Ÿè¸ª
- ğŸ”„ **äº¤äº’å¼ä¼šè¯**ï¼šåœ¨å¯¹è¯ä¸­ç»´æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- ğŸ“Š **å·¥å…·é›†æˆ**ï¼šå®æ—¶æ˜¾ç¤ºå·¥å…·è°ƒç”¨å’Œç»“æœ
- ğŸšª **ä¾¿æ·é€€å‡º**ï¼šè¾“å…¥ "exit" æˆ– "quit" ç»“æŸä¼šè¯
